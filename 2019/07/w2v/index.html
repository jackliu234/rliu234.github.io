<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>Notes on Word2Vec, Stanford Open Course · Rongjia Liu</title><meta name="description" content="Notes on Word2Vec, Stanford Open Course - Rongjia Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/arctic.css"><link rel="search" type="application/opensearchdescription+xml" href="http://jackliu234.com/atom.xml" title="Rongjia Liu"><script src="js/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/archive/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/search/" target="_self" class="nav-list-link">SEARCH</a></li><!-- li.nav-list-item--><!--    a.nav-list-link(class="search" href=url_for("search") target="_self") <i class="fa fa-search" aria-hidden="true"></i>--></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Notes on Word2Vec, Stanford Open Course</h1><div class="post-info">Jul 29, 2019</div><div class="post-content"><a id="more"></a>
<p>The <code>Word2Vec</code> is an iteration based <code>Natural Language Proceessing</code> (NLP) framework for learning word vectors, developed by Thomas Mikolov. For each word position <script type="math/tex">t</script> in a large corpus of text <script type="math/tex">T</script> we define a fixed window size <script type="math/tex">m</script> and the likelihood function <script type="math/tex">L(\theta)</script>:</p>
<script type="math/tex; mode=display">L(\theta) = \prod_{t=1}^T\prod_{j\in[-m, m] \neq 0} p(w_{t+j}|w_t;\theta)</script><p>We define the <code>objective function</code> <script type="math/tex">J(\theta)</script> as the average negative log-likelihood:</p>
<script type="math/tex; mode=display">J(\theta) = -\dfrac{1}{T}\log L(\theta) = -\dfrac{1}{T}\sum_{t=1}^T\sum_{j\in [-m, m]\neq 0} \log p(w_{t+j}|w_t;\theta)</script><p>Say a word <script type="math/tex">w_1</script> is more likely to appear together with <script type="math/tex">w_2</script> than <script type="math/tex">w_3</script>, we want to define a probability measure <script type="math/tex">p</script> such that:</p>
<script type="math/tex; mode=display">p(w_2|w_1) > p(w_3|w_1)</script><p>We achieve this by creating two word vectors <script type="math/tex">\textbf{v}_w</script> (center) and <script type="math/tex">\textbf{u}_w</script> (outside) for each word <script type="math/tex">w</script> and using the dot product to represent the similarity of two words. The we use the <code>softmax</code> function to transform the dot products into a probability measure. Note that a softmax function amplifies probabilities for the largest values while still assign some probabilities to smaller values.</p>
<script type="math/tex; mode=display">p(w_o|w_c) = \text{softmax}(\textbf{u}_{o}^T\textbf{v}_{c}) = \dfrac{\exp(\textbf{u}_{o}^T\textbf{v}_{c})}{\sum_{w\in\mathcal{W}}  \exp(\textbf{u}_{w}^T\textbf{v}_{c})}</script><h1 id="Gradient-Calculation"><a href="#Gradient-Calculation" class="headerlink" title="Gradient Calculation"></a>Gradient Calculation</h1><p>We can now derive the gradient of the objective function. First look at <script type="math/tex">\partial J(\theta)/\partial \textbf{v}_c</script>:</p>
<script type="math/tex; mode=display">\begin{align}
\dfrac{\partial J(\theta)}{\partial \textbf{v}_c} &= -\dfrac{1}{T}\sum_{t=1}^T\sum_{j\in [-m, m]\neq 0} \dfrac{\partial}{\partial \textbf{v}_c}\log \dfrac{\exp(\textbf{u}_{o}^T\textbf{v}_{c})}{\sum_{w\in\mathcal{W}}  \exp(\textbf{u}_{w}^T\textbf{v}_{c})}
\end{align}</script><p>Looking at the derivative inside the double summation and use the fact that:</p>
<script type="math/tex; mode=display">\dfrac{\partial \textbf{x}^T\textbf{a}}{\partial \textbf{x}} = \dfrac{\partial \textbf{a}^T\textbf{x}}{\partial \textbf{x}} = a</script><p>Therefore,</p>
<script type="math/tex; mode=display">\begin{align}
\dfrac{\partial J(t, j, \theta)}{\partial \textbf{v}_c} &= \dfrac{\partial}{\partial \textbf{v}_c}\log \dfrac{\exp(\textbf{u}_{o}^T\textbf{v}_{c})}{\sum_{w\in\mathcal{W}}  \exp(\textbf{u}_{w}^T\textbf{v}_{c})} \\
&= \textbf{u}_o - \sum_{w\in\mathcal{W}} p(w|w_c)\textbf{u}_w \\
\end{align}</script><p>Computing the other derivative w.r.t. <script type="math/tex">\textbf{u}_o</script></p>
<script type="math/tex; mode=display">\begin{align}
\dfrac{\partial J(t, j, \theta)}{\partial \textbf{u}_o} &= \dfrac{\partial}{\partial \textbf{u}_o}\log \dfrac{\exp(\textbf{u}_{o}^T\textbf{v}_{c})}{\sum_{w\in\mathcal{W}}  \exp(\textbf{u}_{w}^T\textbf{v}_{c})} \\
&= \textbf{v}_c [1-  \dfrac{1}{\sum_{w\in\mathcal{W}}  \exp(\textbf{u}_{w}^T\textbf{v}_{c})}] \\
\end{align}</script><p>We used two vector for each word such that the optimization is easier. Eventually we average the two vectors for a given word. We can now use <code>gradient descent</code> to update the model parameters <script type="math/tex">\theta</script>.</p>
<script type="math/tex; mode=display">\theta_{new} = \theta_{old} + \alpha \nabla_\theta J(\theta)</script><p>Since <script type="math/tex">\nabla_\theta J(\theta)</script> can be expensive to calculate for the entire corpus of text, we use <code>stochastic gradient descent</code> which updates <script type="math/tex">\theta</script> with randomly selected samples of text.  </p>
<h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><p><strong><em>requests</em></strong> Requests is a library for making HTTP requests in Python. <code>HTTP</code> functions as a <code>request–response protocol</code> in the client–server computing model. A web browser, for example, may be the client and an application running on a computer hosting a website may be the server.</p>
<ul>
<li>The client submits an HTTP request message to the server.</li>
<li>The server, which provides resources such as HTML files and other content, or performs other functions on behalf of the client, returns a response message to the client. The response contains completion status information about the request and may also contain requested content in its message body.</li>
</ul>
<p>A <code>Uniform Resource Locator</code> (URL) is a reference to a web resource, typically includes a protocol (http), a hostname (www.website.com), and a file name (index.html).</p>
<p><strong><em>bs4</em></strong> Beautiful Soup is a Python library for pulling data out of HTML and XML files, and it can work with a <code>parser</code> to provide functionality of navigating, searching, and modifying the parse tree. Both HTML and XML are markup languages designed for storing and transporting data.</p>
<p><strong><em>re</em></strong> Provides regular expression matching operations.</p>
<p><strong><em>nltk</em></strong> The Natural Language Toolkit is a Python platform for working with human language data. It provides interfaces to lexical resources as well as a suite of text processing libraries.</p>
<p><strong><em>gensim</em></strong> Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"></span><br><span class="line">url      = <span class="string">'https://en.wikipedia.org/wiki/Python_(programming_language)'</span></span><br><span class="line">response = requests.get(url).content</span><br><span class="line">soup     = bs4.BeautifulSoup(response, <span class="string">"lxml"</span>).find_all(<span class="string">'p'</span>)</span><br><span class="line">raw_data = <span class="string">""</span>.join([s.text <span class="keyword">for</span> s <span class="keyword">in</span> soup])</span><br></pre></td></tr></table></figure>
<p>Perform data cleaning and retain only key words from the parsed data.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data  = raw_data.lower()</span><br><span class="line">data  = re.sub(<span class="string">'[^a-zA-Z]'</span>, <span class="string">' '</span>, data)</span><br><span class="line">data  = re.sub(<span class="string">r'\s+'</span>, <span class="string">' '</span>, data)</span><br><span class="line">words = [nltk.word_tokenize(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> nltk.sent_tokenize(data)]</span><br><span class="line">words = [[w <span class="keyword">for</span> w <span class="keyword">in</span> words[i] <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords.words(<span class="string">'english'</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words))]</span><br></pre></td></tr></table></figure></p>
<p>Train Word2Vec model.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = Word2Vec(words, min_count=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<p>View the word vector for the word ‘python’:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.wv[<span class="string">'python'</span>]</span><br></pre></td></tr></table></figure></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">0.00272543</span>,  <span class="number">0.00044483</span>, <span class="number">-0.002916</span>  , <span class="number">-0.00091269</span>, <span class="number">-0.0028646</span> ,</span><br><span class="line">        <span class="number">0.00312438</span>, <span class="number">-0.00202833</span>,  <span class="number">0.00418179</span>,  <span class="number">0.00512605</span>,  <span class="number">0.00398485</span>,</span><br><span class="line">        <span class="number">0.00383619</span>, <span class="number">-0.00527396</span>,  <span class="number">0.00136241</span>,  <span class="number">0.00048289</span>,  <span class="number">0.00393969</span>,</span><br><span class="line">        <span class="number">0.00363039</span>,  <span class="number">0.00516506</span>,  <span class="number">0.00065952</span>,  <span class="number">0.00279527</span>, <span class="number">-0.00289068</span>,</span><br><span class="line">       <span class="number">-0.0002257</span> , <span class="number">-0.00423192</span>,  <span class="number">0.00389712</span>,  <span class="number">0.00435556</span>, <span class="number">-0.00169954</span>,</span><br><span class="line">        <span class="number">0.00372895</span>, <span class="number">-0.00204547</span>,  <span class="number">0.00031502</span>, <span class="number">-0.000558</span>  ,  <span class="number">0.00302919</span>,</span><br><span class="line">       <span class="number">-0.00329071</span>, <span class="number">-0.00445723</span>, <span class="number">-0.00090491</span>, <span class="number">-0.00073062</span>,  <span class="number">0.0055988</span> ,</span><br><span class="line">       <span class="number">-0.00180256</span>, <span class="number">-0.00032414</span>,  <span class="number">0.00129002</span>, <span class="number">-0.00077443</span>, <span class="number">-0.00511642</span>,</span><br><span class="line">        <span class="number">0.00082947</span>,  <span class="number">0.00207873</span>,  <span class="number">0.00064075</span>,  <span class="number">0.00320432</span>,  <span class="number">0.00252466</span>,</span><br><span class="line">        <span class="number">0.00165025</span>,  <span class="number">0.00274325</span>,  <span class="number">0.00557919</span>,  <span class="number">0.00422611</span>, <span class="number">-0.00348751</span>,</span><br><span class="line">        <span class="number">0.00488516</span>,  <span class="number">0.00238723</span>, <span class="number">-0.0034958</span> ,  <span class="number">0.00119023</span>, <span class="number">-0.0009317</span> ,</span><br><span class="line">       <span class="number">-0.00051728</span>, <span class="number">-0.00448227</span>, <span class="number">-0.00145251</span>,  <span class="number">0.00098566</span>, <span class="number">-0.00352017</span>,</span><br><span class="line">       <span class="number">-0.00017685</span>,  <span class="number">0.00388307</span>,  <span class="number">0.00305843</span>, <span class="number">-0.00614224</span>,  <span class="number">0.00319819</span>,</span><br><span class="line">       <span class="number">-0.0038121</span> ,  <span class="number">0.00025529</span>, <span class="number">-0.00525783</span>, <span class="number">-0.00364403</span>,  <span class="number">0.00531866</span>,</span><br><span class="line">       <span class="number">-0.00040134</span>,  <span class="number">0.00509736</span>, <span class="number">-0.00279795</span>, <span class="number">-0.00520586</span>,  <span class="number">0.00088609</span>,</span><br><span class="line">       <span class="number">-0.00209225</span>,  <span class="number">0.00341286</span>,  <span class="number">0.00403736</span>, <span class="number">-0.00360165</span>, <span class="number">-0.0025662</span> ,</span><br><span class="line">       <span class="number">-0.00442059</span>, <span class="number">-0.00286324</span>, <span class="number">-0.00441705</span>, <span class="number">-0.00248354</span>, <span class="number">-0.00311305</span>,</span><br><span class="line">       <span class="number">-0.0017566</span> , <span class="number">-0.00094437</span>,  <span class="number">0.00274204</span>, <span class="number">-0.00165205</span>, <span class="number">-0.00576063</span>,</span><br><span class="line">       <span class="number">-0.00485601</span>,  <span class="number">0.00124242</span>, <span class="number">-0.00536304</span>, <span class="number">-0.00042135</span>, <span class="number">-0.00091981</span>,</span><br><span class="line">        <span class="number">0.00247618</span>, <span class="number">-0.0041995</span> , <span class="number">-0.00123214</span>, <span class="number">-0.00465355</span>,  <span class="number">0.00184935</span>],</span><br><span class="line">      dtype=float32)</span><br></pre></td></tr></table></figure>
<p>View words that are most similar to ‘python’:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.wv.most_similar(<span class="string">'python'</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">'revision'</span>, <span class="number">0.3588007390499115</span>),</span><br><span class="line"> (<span class="string">'system'</span>, <span class="number">0.35286271572113037</span>),</span><br><span class="line"> (<span class="string">'feature'</span>, <span class="number">0.33182504773139954</span>),</span><br><span class="line"> (<span class="string">'integer'</span>, <span class="number">0.3286311626434326</span>),</span><br><span class="line"> (<span class="string">'systems'</span>, <span class="number">0.3157789707183838</span>),</span><br><span class="line"> (<span class="string">'x'</span>, <span class="number">0.3120567798614502</span>),</span><br><span class="line"> (<span class="string">'numbers'</span>, <span class="number">0.30488497018814087</span>),</span><br><span class="line"> (<span class="string">'supports'</span>, <span class="number">0.29797300696372986</span>),</span><br><span class="line"> (<span class="string">'language'</span>, <span class="number">0.28043025732040405</span>),</span><br><span class="line"> (<span class="string">'major'</span>, <span class="number">0.2673966884613037</span>)]</span><br></pre></td></tr></table></figure>
<p>Some other resources: <a href="https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/" target="_blank" rel="noopener">stackabuse</a>, <a href="https://skymind.ai/wiki/word2vec#foreign" target="_blank" rel="noopener">skymind</a> and <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">google code</a></p>
<p><br><br><br></p>
<p>Reference:</p>
<ul>
<li><a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">Deep Learning for Natural Language Processing</a>, Standford University</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2019/08/gradient-descent/" class="prev">上一篇</a><a href="/2019/07/high-freq-data-model/" class="next">下一篇</a></div><div id="container"></div><!-- link(rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css")--><link rel="stylesheet" href="/css/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
    clientID: '4ec287ddd4ac34ff5087',
    clientSecret: 'ae45426765f12ac3e2f903662b8938dc4881f703',
    repo: 'jackliu234.github.io',
    owner: 'jackliu234',
    admin: ['jackliu234'],
    perPage: 100,
    id: 'Mon Jul 29 2019 00:00:00 GMT+0200 GMT'.split('GMT')[0].replace(/\s/g, '-'),
    distractionFreeMode: false,
    pagerDirection: 'first'
})

gitalk.render('container')</script><!-- block copyright--></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-133275176-1",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>