<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>PCA: A Deeper Dive · Rongjia Liu</title><meta name="description" content="PCA: A Deeper Dive - Rongjia Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/arctic.css"><link rel="search" type="application/opensearchdescription+xml" href="http://jackliu234.com/atom.xml" title="Rongjia Liu"><script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/archive/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/search/" target="_self" class="nav-list-link">SEARCH</a></li><!-- li.nav-list-item--><!--    a.nav-list-link(class="search" href=url_for("search") target="_self") <i class="fa fa-search" aria-hidden="true"></i>--></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PCA: A Deeper Dive</h1><div class="post-info">Apr 27, 2019</div><div class="post-content"><a id="more"></a>


<p>PCA finds low dimensional representation of a dataset that contains as much as possible of the variation. As each of the $n$ observations lives on a $p$-dimensional space, and not all dimensions are equally interesting.</p>
<h1 id="Linear-Algebra-Review"><a href="#Linear-Algebra-Review" class="headerlink" title="Linear Algebra Review"></a>Linear Algebra Review</h1><p>Let $A$ be a $n\times n$ matrix. With $n=2, \ 3$, the <code>determinant</code> of $A$ can be calculated as follow.
$$ det(\begin{bmatrix}
    a &amp; b \
    c &amp; d \
\end{bmatrix} ) = ad - bc $$</p>
<p>$$\begin{align} det ( \begin{bmatrix}
    a &amp; b &amp; c \
    d &amp; e &amp; f \
    g &amp; h &amp; i \
\end{bmatrix} ) &amp;= aei + bfg + cdh - cdg - bdi - afh \
&amp;= a \times det(
    \begin{bmatrix}
        e &amp; f \
        h &amp; i \
    \end{bmatrix}
    ) + b \times det(
        \begin{bmatrix}
            d &amp; f \
            g &amp; i \
        \end{bmatrix}
        ) + c \times det(
            \begin{bmatrix}
                d &amp; e \
                g &amp; h \
            \end{bmatrix}
            ) \end{align}$$</p>
<p>Properties of determinant:
$$\begin{align} det(A^T) &amp;= det(A) \
det(A^{-1}) &amp;= det(A)^{-1} \
det(AB) &amp;= det(A)det(B) \end{align}$$</p>
<p>A real number $\lambda$ is an <code>eigenvalue</code> of $A$ if there exists a non-zero vector $x$ (<code>eigenvector</code>) in $\mathbb{R}^n$ such that:</p>
<p>$$Ax = \lambda x$$</p>
<p>The determinant of matrix $A - \lambda I$ is called the <code>characteristic polynomial</code> of $A$. The equation $det(A - \lambda I)$ is called the <code>characteristic equation</code> of $A$, where the eigenvalues $\lambda$ are the real roots of the equation. It can be shown that:</p>
<p>$$ \prod_{i=1}^n \lambda_i = det(A) \
\sum_{i=1}^n \lambda_i = \sum_{i=1}^n a_{i, \ i} = trace(A) $$</p>
<p>Matrix $A$ is <code>invertible</code> if there exists a $n\times n$ matrix $B$ such that $AB = BA = I$. A square matrix is invertible if and only if its determinant is non-zero. A non-square matrix do not have an inverse.</p>
<p>Matrix $A$ is called <code>diagonalizable</code> if and only if it has linearly independent eigenvectors. Let $\textbf{U}$ denote the eigen vectors of A and $\textbf{D}$ denote the diagonal $\lambda$ vector. Then:</p>
<p>$$A = \textbf{UDU}^{-1} \rightarrow A^x = \textbf{UD}^x\textbf{U}^{-1} $$</p>
<p>If matrix $A$ is <code>symmetric</code>, then:</p>
<ul>
<li>all eigenvalues of $A$ are real numbers</li>
<li>all eigenvectors of $A$ from distinct eigenvalues are orthogonal</li>
</ul>
<p>Matrix $A$ is <code>positive semi-definite</code> if and only if any of the following:</p>
<ul>
<li>for any $n\times 1$ matrix $x$, $x^TAx \geq 0$</li>
<li>all eigenvalues of $A$ are non-negative</li>
<li>all the upper left submatrices $A_K$ have non-negative determinants.</li>
</ul>
<p>Matrix $A$ is <code>positive definite</code> if and only if any of the following:</p>
<ul>
<li>for any $n\times 1$ matrix $x$, $x^TAx &gt; 0$</li>
<li>all eigenvalues of $A$ are positive</li>
<li>all the upper left submatrices $A_K$ have positive determinants.</li>
</ul>
<p>All <code>covariance</code>, <code>correlation</code> matrices must be <code>symmetric</code> and <code>positive semi-definite</code>. If there is no perfect linear dependence between random variables, then it must be <code>positive definite</code>.</p>
<p>Let $A$ be an invertible matrix, the <code>LU decomposition</code> breaks down $A$ as the product of a lower triangle matrix $L$ and upper triangle matrix $U$. Some applications are:</p>
<ul>
<li>solve $Ax=b$: $LUx=b \rightarrow Ly=b \text{ ; } Ux=y$</li>
<li>solve $det(A)$: $det(A) = det(L)\ det(U)=\prod L_{i, \ i}\prod U_{j, \ j}$</li>
</ul>
<p>Let $A$ be a symmetric positive definite matrix, the <code>Cholesky decomponsition</code> expand on the <code>LU decomposition</code> and breaks down $A=U^TU$, where $U$ is a <code>unique</code> upper triangular matrix with positive diagonal entries. Cholesky decomposition can be used to generate correltaed random variables in Monte Carlo simulation</p>
<h1 id="Matrix-Interpretation"><a href="#Matrix-Interpretation" class="headerlink" title="Matrix Interpretation"></a>Matrix Interpretation</h1><p>Consider a $n\times p$ matrix:</p>
<p>$$\begin{bmatrix}
    x_{11}       &amp; x_{12} &amp; x_{13} &amp; \dots &amp; x_{1p} \
    x_{21}       &amp; x_{22} &amp; x_{23} &amp; \dots &amp; x_{2p} \
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
    x_{n1}       &amp; x_{n2} &amp; x_{n3} &amp; \dots &amp; x_{np}
\end{bmatrix}$$</p>
<p>To find the first principal component $F^1$, we define it as the normalized linear combination of $X$ that has the largest variance, where its <code>loading</code> $\phi^1_j$ are normalized: $\sum^p_{j=1} (\phi^1_j)^2 = 1$</p>
<p>$$ F^1 = \phi^1_1X_1 + \phi^1_2X_2 + \dots + \phi^1_pX_p $$</p>
<p>Or equivalently, for each <code>score</code>: $F^1_i = \sum_{j=1}^{p} \phi^1_jx_{ij}$</p>
<p>In matrix form:
$$\begin{bmatrix}
    x_{11}       &amp; x_{12} &amp; x_{13} &amp; \dots &amp; x_{1p} \
    x_{21}       &amp; x_{22} &amp; x_{23} &amp; \dots &amp; x_{2p} \
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
    x_{n1}       &amp; x_{n2} &amp; x_{n3} &amp; \dots &amp; x_{np}
\end{bmatrix}\times
\begin{bmatrix}
    \phi^1_1 \
    \phi^1_2 \
    \vdots \
    \phi^1_p \
\end{bmatrix}=
\begin{bmatrix}
    f^1_1 \
    f^1_2 \
    \vdots \
    f^1_n \
\end{bmatrix}$$</p>
<p>Finally, the first principal component loading vector $\phi^1$ solves the optimization problem that maximize the sample variance of the scores $f^1$. An objective function can be formulated as follow and solved via an <code>eigen decomposition</code>:</p>
<p>$$ \text{maximize }{\ \dfrac{1}{n}\sum_{i=1}^n(f^1_i)^2\ } \text{ subject to } \sum^p_{j=1} (\phi^1_j)^2 = 1$$</p>
<p>To find the second principal component loading $\phi^2$, use the same objective function with $\phi^2$ replacement and include an additional constraint that $\phi^2$ is orthogonal to $\phi^1$.</p>
<h1 id="Geometric-Interpretation"><a href="#Geometric-Interpretation" class="headerlink" title="Geometric Interpretation"></a>Geometric Interpretation</h1><p>The $p\times k$ <code>loading</code> matrix $L = [\phi^1 \dots \phi^k]$ defines a linear transformation that projects the data from the feature space $\mathbb{R}^p$ into a subspace $\mathbb{R}^k$, in which the data has the most variance. The result of the projection is the <code>factor</code> matrix $F = [F^1 \dots F^k]$, also known as the <code>principal components</code>.</p>
<p>$$
\underbrace{
    \begin{bmatrix}
        x_{11}       &amp; x_{12} &amp; x_{13} &amp; \dots &amp; x_{1p} \
        x_{21}       &amp; x_{22} &amp; x_{23} &amp; \dots &amp; x_{2p} \
        \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
        x_{n1}       &amp; x_{n2} &amp; x_{n3} &amp; \dots &amp; x_{np}
    \end{bmatrix}
}<em>{data} \times
\underbrace{
    \begin{bmatrix}
        \phi^1_1 &amp; \dots &amp; \phi^k_1\
        \phi^1_2 &amp; \dots &amp; \phi^k_2\
        \vdots &amp; \ddots &amp; \vdots \
        \phi^1_p &amp; \dots &amp; \phi^k_p\
    \end{bmatrix}
}</em>{loadings}=
\underbrace{
    \begin{bmatrix}
        f^1_1 &amp; \dots &amp; f^k_1 \
        f^1_2 &amp; \dots &amp; f^k_2 \
        \vdots &amp; \ddots &amp; \vdots \
        f^1_n &amp; \dots &amp; f^k_n \
    \end{bmatrix}
}_{\text{principal components} \
\text{or, factor scores}}$$</p>
<p>In other words, the principal components vectors $F^1 … F^k$ forms a low-dimensional linear subspace that are the closest (shortest average squared Euclidean distance) to the observations.</p>
<h1 id="Eigen-Decomposition"><a href="#Eigen-Decomposition" class="headerlink" title="Eigen Decomposition"></a>Eigen Decomposition</h1><p>Given $n\times p$ data matrix $X$, the objective of <code>PCA</code> is to find a lower dimension representation factor matrix $F$, from which a $n\times p$ matrix $\tilde{X}$ can be constructed where distance between the covariance matrices $cov(X)$ and $cov(\tilde{X})$ are minimized.</p>
<p>The covariance matrix of $X$ is a $p\times p$ symmetric positive semi-definite matrix, therefore we have the following decomposition where $\textbf{u}$’s’ are $p\times 1$ eigenvectors of $cov(X)$ and $\lambda$’s are the eigenvalues. Note that $\textbf{u}$ can be a zero vector if the columns of $cov(X)$ are linearly dependent.</p>
<p>$$\begin{align}
cov(X) &amp;= \dfrac{1}{n-1}X^TX \
&amp;=\dfrac{1}{n-1}
\begin{bmatrix}
    \textbf{u}<em>1 &amp; \dots &amp; \textbf{u}_p
\end{bmatrix}
\begin{bmatrix}
    \lambda_1 &amp; \dots &amp; 0 \
    \vdots &amp; \ddots &amp; \vdots \
    0 &amp; \dots &amp; \lambda_p
\end{bmatrix}
\begin{bmatrix}
    \textbf{u}_1 &amp; \dots &amp; \textbf{u}_p
\end{bmatrix}^T \
&amp;= \dfrac{1}{n-1}\sum</em>{i=1}^p \lambda_i\textbf{u}_i\textbf{u}_i^T \end{align}$$</p>
<p>If we ignore the constant $1/(n-1)$, and define the $p\times p$ <code>loading</code> matrix $L_0=[\textbf{u}_1, \dots, \textbf{u}_p]$ and $n\times p$ <code>factor</code> matrix $F_0$ where $F_0^TF_0=\Lambda$. Then:</p>
<p>$$X = F_0L_0^T$$</p>
<p>Now comes the <code>PCA</code> idea: Let’s rank the $\lambda_i$’s in descending order, pick $k &lt; p$ such that:</p>
<p>$$ \dfrac{1}{n-1}\sum_{i=1}^{k} \lambda_i\textbf{u}<em>i\textbf{u}_i^T \approx \dfrac{1}{n-1}\sum</em>{i=1}^p \lambda_i\textbf{u}<em>i\textbf{u}_i^T = cov(X) \
\text{and we denote it } cov(\tilde{X}) \text{, i.e. } cov(\tilde{X}) = \dfrac{1}{n-1}\sum</em>{i=1}^{k} \lambda_i\textbf{u}_i\textbf{u}_i^T$$</p>
<p>Now we observe that the matrix $cov(\tilde{X})$ is also a $p\times p$ positive semi-definite matrix. Following similar decomposition, we obtain a $p\times k$ matrix $L$ and $n\times k$ matrix $F$, where:</p>
<p>$$ \tilde{X} = FL^T $$</p>
<p>Here we have it, a dimension-reduced $n\times k$ factor matrix $F$, where its projection back to $n\times p$ space, $\tilde{X}$, has similar covariance as the original $n\times p$ dataset $X$.</p>
<h1 id="Practical-Considerations"><a href="#Practical-Considerations" class="headerlink" title="Practical Considerations"></a>Practical Considerations</h1><p>PCA excels at identifying <code>latent</code> variables from the <code>measurable</code> variables. PCA can only be applied to <code>numeric</code> data, while categorical variables need to be binarized beforehand.</p>
<ul>
<li><p><code>Centering</code>: yes.</p>
</li>
<li><p><code>Scaling</code>:</p>
<ul>
<li>if the range and scale of the variables are different, <code>correlation matrix</code> is typically used to perform PCA, i.e. each variables are scaled to have standard deviation of $1$</li>
<li>otherwise if the variables are in the same units of measure, using the <code>covariance matrix</code> (not standardizing) the variables could reveal interesting properties of the data</li>
</ul>
</li>
<li><p><code>Uniqueness</code>: each loading vector $\phi^1$ is unique up to a sign flip, as the it can take on opposite direction in the same subspace. Same applies to the score vector $Z^1$, as $var(Z^1) = var(-Z^1)$</p>
</li>
<li><p><code>Propotional of Variance Explained</code>: we can compute the total variance in a data set in the first formula below. The variance explained by the $m$-th principal component is: $\dfrac{1}{n} \sum_{i=1}^n (z^m_i)^2$. Therefore, the second formula can be computed for the $PVE$:</p>
</li>
</ul>
<p>$$ \sum_{j=1}^p Var(X_j) = \sum_{j=1}^p [ \dfrac{1}{n} \sum_{i=1}^n x^2_{ij} ] \
PVE^m = \dfrac{\sum_{i=1}^n (z^m_i)^2}{\sum_{j=1}^p\sum_{i=1}^n x^2_{ij}} $$</p>
<p><br><br><br></p>
<p>Reference:</p>
<ul>
<li>An Introduction to Statistical Learning with Applications in R, James, Witten, Hastie and Tibshirani</li>
<li>FINM 33601 Lecture Note, Y. Balasanov, University of Chicago</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2019/05/apollo-add-tags/" class="prev">PREV</a><a href="/2019/04/islr/" class="next">NEXT</a></div><div id="container"></div><!-- link(rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css")--><link rel="stylesheet" href="/css/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
    clientID: '4ec287ddd4ac34ff5087',
    clientSecret: 'ae45426765f12ac3e2f903662b8938dc4881f703',
    repo: 'jackliu234.github.io',
    owner: 'jackliu234',
    admin: ['jackliu234'],
    perPage: 100,
    id: 'Sat Apr 27 2019 00:00:00 GMT-0500 GMT'.split('GMT')[0].replace(/\s/g, '-'),
    distractionFreeMode: false,
    pagerDirection: 'first'
})

gitalk.render('container')</script><!-- block copyright--></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-133275176-1",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>