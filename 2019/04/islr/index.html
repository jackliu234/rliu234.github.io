<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>Notes on ISLR · Rongjia Liu</title><meta name="description" content="Notes on ISLR - Rongjia Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/arctic.css"><link rel="search" type="application/opensearchdescription+xml" href="http://jackliu234.com/atom.xml" title="Rongjia Liu"><script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/archive/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/search/" target="_self" class="nav-list-link">SEARCH</a></li><!-- li.nav-list-item--><!--    a.nav-list-link(class="search" href=url_for("search") target="_self") <i class="fa fa-search" aria-hidden="true"></i>--></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Notes on ISLR</h1><div class="post-info">Apr 13, 2019</div><div class="post-content"><p>This is a study note on the book <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/" target="_blank" rel="noopener">An Introduction to Statistical Learning with Applications in R</a>, with my own experimental R-code for each topic.</p>
<blockquote>
<p><strong><em>Update</em></strong>: Please do not hesitate to let me know if you spot any error on this page via comment or <a href="jackliu234@gmail.com">email</a>. Cheers!
<img src="islr-images.jpeg" alt="images.jpeg"></p>
</blockquote>
<h1 id="Navigation"><a href="#Navigation" class="headerlink" title="Navigation"></a><span id="nav">Navigation</span></h1><p><a href="#00">00. Introduction</a>
<a href="#01">01. Linear Model</a>
<a href="#02">02. Tree-Based Method</a>
<a href="#03">03. Unsupervised Learning</a></p>
<h1 id="Introduction-8634"><a href="#Introduction-8634" class="headerlink" title="Introduction &#8634;"></a><span id="00">Introduction</span> <sup><a href="#nav">&#8634;</a></sup></h1><p>Suppose we observe a quantitative response $Y$ and $p$ different predicting variables $X = (X_1, X_2…X_p)$. We assume that there is a underlying relationship $f$ between $Y$ and $X$:</p>
<p>$$Y=f(X) + \epsilon$$</p>
<p>We want to estimate $f$ mainly for two purpose:</p>
<ul>
<li><code>prediction</code>: in the case where $Y$ is not easily obtained, we want to estimate $f$ with $\hat{f}$, and use $\hat{f}$ to predict Y with $\hat{Y}$. Here $\hat{f}$ can be a black box, such as highly non-linear approaches which offers accuracy over interpretability.</li>
</ul>
<p>$$ \hat{Y} = \hat{f}(X) $$</p>
<ul>
<li><code>inference</code>: in the case where we are more interested in how $Y$ is affected by the change in each $X_n$. We need to know the exact form of $\hat{f}$. For example, linear model is often used which offer interpretable inference but sometimes inaccurate.</li>
</ul>
<h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><p>There are important and subtle differences between a <code>feature</code> and a <code>variable</code>.</p>
<ul>
<li><code>variable</code>: raw data</li>
<li><code>feature</code>: data that is transformed, derived from raw data. A feature can be more predictive and have a direct relationship with the target variable, but it isn’t immediately represented by the raw variable.</li>
</ul>
<p>The need for feature engineering arises from limitations of modeling algorithms:</p>
<ol>
<li>the curse of dimensionality (leading to statistical insiginificance)</li>
<li>the need to represent the signal in a meaningful and interpretable way</li>
<li>the need to capture complex signals in the data accurately</li>
<li>computational feasibility when the number of features gets large.</li>
</ol>
<h3 id="Feature-Transformation"><a href="#Feature-Transformation" class="headerlink" title="Feature Transformation"></a>Feature Transformation</h3><p>The <a href="https://en.wikipedia.org/wiki/Occam's_razor" target="_blank" rel="noopener">Occam’s Razor</a> principle states that a simpler solutions are more likely to be corret than complex ones.</p>
<p>Consider the following example of modeling a exponentially distributed response. After applying a log transformation, we can view the relation from a different viewpoint provided by the new feature space, in which a simpler model may achieve more predictive power than a complex model in the original input space.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x1 &lt;- runif(<span class="number">100</span>, <span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">x2 &lt;- exp(x1)</span><br><span class="line">df &lt;- data.frame(x1 = x1, x2 = x2)</span><br><span class="line">df$logx2 &lt;- log(df$x2)</span><br><span class="line">ss</span><br><span class="line">options(repr.plot.width=<span class="number">6</span>, repr.plot.height=<span class="number">3</span>)</span><br><span class="line">p1 &lt;- ggplot(data = df, aes(x = x1, y = x2)) + geom_point(size=<span class="number">0.3</span>)</span><br><span class="line">p2 &lt;- ggplot(data = df, aes(x = x1, y = logx2)) + geom_point(size=<span class="number">0.3</span>)</span><br><span class="line">grid.arrange(p1, p2, nrow=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>![Screen Shot 2019-05-16 at 11.05.16 AM.png](statistical-learning-Screen Shot 2019-05-16 at 11.05.16 AM.png)</p>
<br>

<p>Consider another classification problem, in which we want to identify the boundary between the two classes. A complex model would draw a circle as the divider. A simpler approach would be to create a new feature with distances of each point from the origin. The divider becomes a much simpler straight line.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x1 &lt;- runif(<span class="number">1000</span>,-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">x2 &lt;- runif(<span class="number">1000</span>,-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">class &lt;- ifelse(sqrt(x1^<span class="number">2</span> +x2^<span class="number">2</span>) &lt; <span class="number">0.5</span>, <span class="string">"A"</span>, <span class="string">"B"</span>)</span><br><span class="line"></span><br><span class="line">df &lt;- data.frame(x1 = x1, x2 = x2, class = class)</span><br><span class="line">p1 &lt;- ggplot(data = df, aes(x = x1, y = x2, color = class)) +</span><br><span class="line">      geom_point(size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">df$dist_from_0 &lt;- sqrt(df$x1^<span class="number">2</span> + df$x2^<span class="number">2</span>)</span><br><span class="line">p2 &lt;- ggplot(data = df, aes(x = <span class="number">0</span>, y = dist_from_0, color = class)) +</span><br><span class="line">      geom_point(position = <span class="string">"jitter"</span>, size=<span class="number">1</span>) +</span><br><span class="line">      theme(axis.title.x=element_blank(),</span><br><span class="line">            axis.text.x=element_blank(),</span><br><span class="line">            axis.ticks.x=element_blank()) +</span><br><span class="line">      annotate(<span class="string">"segment"</span>, x = -<span class="number">0.5</span>, xend = <span class="number">0.5</span>, y = <span class="number">0.5</span>, yend = <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">options(repr.plot.width=<span class="number">8</span>, repr.plot.height=<span class="number">3</span>)</span><br><span class="line">grid.arrange(p1, p2, nrow=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>![Screen Shot 2019-05-16 at 11.41.39 AM.png](statistical-learning-Screen Shot 2019-05-16 at 11.41.39 AM.png)</p>
<h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><p>For $p$ predictive variables, there are a total of $2^p$ models. We use feature selection to choose a smaller subset of the variable to model.</p>
<ul>
<li><code>Forward Selection</code> We begin with the null model with no variables and an intercept. We then fit $n$ simple linear regression to choose our first variable with the lowest $RSS$. Same way to choose the next variable to be added until some stoppoing rule.</li>
<li><code>Backward Selection</code> We begin with the full model and remove the variable with the largest coefficient $p$-value. Re-fit and remove the next.</li>
<li><code>Mixed Selection</code> We begin with the null model and the forward selection technique. Whenever the p-value for a variable exceeds a threshold we remove it.</li>
</ul>
<h2 id="Regression-Problems"><a href="#Regression-Problems" class="headerlink" title="Regression Problems"></a>Regression Problems</h2><p>In regression problems the variables are quantitative, we use <code>mean squared error</code>, or $MSE$, to measure the quality of estimator $\hat{f}$:</p>
<p>$$ MSE = \dfrac{\sum_{i=1}^n [y_i - \hat{f}(x_i)]^2}{n}$$</p>
<p>A fundamental property of statistical learning that holds regardless of the particular dataset and statistical method is that as model flexibility increases, we can observe a monotone decrease in training MSE and an <code>U-shape</code> in test $MSE$.</p>
<h3 id="Bias-vs-Variance"><a href="#Bias-vs-Variance" class="headerlink" title="Bias vs Variance"></a>Bias vs Variance</h3><p>The <code>bias-variance</code> trade off decompose the expected test MSE of a single data point $x_0$:</p>
<p>$$\begin{align}
\mathbb{E}MSE^{test}(x_0) &amp;= \mathbb{E}[y_0 - \hat{f}(x_0)]^2 \
&amp;= Var[\hat{f}(x_0)] + Bias[\hat{f}(x_0)]^2 + Var[\epsilon]
\end{align}$$</p>
<p>where:</p>
<ul>
<li><code>variance</code> refers to the variance of the estimator  among different datasets. A highly flexible $\hat{f}$ lead to a high variance, as even small variance in data induce change in the $\hat{f}$’s form.</li>
<li><code>bias</code> refers to the error due to estimator $\hat{f}$ inflexibility. For example, using linear $\hat{f}$ to estimate non-linear relationship leads to high bias.</li>
</ul>
<h2 id="Classification-Problems"><a href="#Classification-Problems" class="headerlink" title="Classification Problems"></a>Classification Problems</h2><p>In regression problems the variables are qualitative, we use <code>error rate</code> to measure the quality of estimator $\hat{f}$:</p>
<p>$$ \text{error rate} = \dfrac{\sum_{i=1}^n \textbf{1}{y_i \neq \hat{f}(x_i)}}{n}$$</p>
<h3 id="The-Bayes-Classifier"><a href="#The-Bayes-Classifier" class="headerlink" title="The Bayes Classifier"></a>The Bayes Classifier</h3><p>The <code>Bayes classifier</code> predict the classification based on the combination of the prior probability and its likelihood given predictor values. With categories $C_1, C_2, C_3…$, and predictor values $\textbf{x} = (x_1, x_2, x_3…)$, $\hat{y}$ is assigned to category $C_k$ which has the maximum posterior probability:</p>
<p>$$\hat{y} = \hat{f}^{Bayes}(\textbf{x}) = C_k, ;where; k = argmax_k ;p(C_k | \textbf{x})$$</p>
<p>Where:</p>
<p>$$ p(C_k | \textbf{x}) = \dfrac{p(x_1, x_2, x_3…|C_k)p(C_k)}{p(x_1, x_2, x_3…)}$$</p>
<p>The <code>naive Bayes classifier</code> assumes independence between the predictor $X_i$’s, and the formula becomes:</p>
<p>$$\hat{y} = \hat{f}^{naiveBayes}(\textbf{x}) = C_k, where; k = argmax_k ;p(C_k) \times \prod p(x_i | C_k)$$</p>
<p>When $x_i$ is continuous, the <code>Guassian naive Bayes classifier</code> assumes that $ p(x_i | C_k) \sim \mathcal{N}(\mu_{i, C_k}, \sigma^2_{i, C_k})$</p>
<p>In R, we use the naiveBayes function from the e1071 package to predict Survival from the Titanic dataset.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(e1071)</span><br><span class="line"><span class="keyword">library</span>(caret)</span><br><span class="line">set.seed(<span class="number">9999</span>)</span><br><span class="line">df=as.data.frame(Titanic)</span><br><span class="line">df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq),]</span><br><span class="line">df$Freq &lt;- <span class="literal">NULL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create a 75/25 train/test split</span></span><br><span class="line">partition &lt;- createDataPartition(df$Survived, list=<span class="literal">FALSE</span>, p=<span class="number">0.75</span>)</span><br><span class="line">train &lt;- df[partition, ]</span><br><span class="line">test &lt;- df[-partition, ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit naive bayes classifier</span></span><br><span class="line">bayes &lt;- naiveBayes(Survived ~ ., data=train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 10-fold validation</span></span><br><span class="line">bayes.cv &lt;- train(Survived ~ ., data=train,</span><br><span class="line">                  method = <span class="string">"nb"</span>,</span><br><span class="line">                  trControl = trainControl(method = <span class="string">"cv"</span>,</span><br><span class="line">                                           number = <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p>Viewing the model results, the prior probabilities $p(C_k)$ are shown in the “A-priori probabilities” section.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; bayes</span><br><span class="line"></span><br><span class="line">Naive Bayes Classifier <span class="keyword">for</span> Discrete Predictors</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">naiveBayes.default(x = X, y = Y, laplace = laplace)</span><br><span class="line"></span><br><span class="line">A-priori probabilities:</span><br><span class="line">Y</span><br><span class="line">       No       Yes</span><br><span class="line"><span class="number">0.6767554</span> <span class="number">0.3232446</span></span><br></pre></td></tr></table></figure>

<p>The likelihood $p(x_i | C_k)$ are shown in the “Conditional probabilities” section</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Conditional probabilities:</span><br><span class="line">     Class</span><br><span class="line">Y            1st        2nd        3rd       Crew</span><br><span class="line">  No  <span class="number">0.09481216</span> <span class="number">0.10554562</span> <span class="number">0.34615385</span> <span class="number">0.45348837</span></span><br><span class="line">  Yes <span class="number">0.28838951</span> <span class="number">0.15730337</span> <span class="number">0.24344569</span> <span class="number">0.31086142</span></span><br><span class="line"></span><br><span class="line">     Sex</span><br><span class="line">Y           Male     Female</span><br><span class="line">  No  <span class="number">0.91323792</span> <span class="number">0.08676208</span></span><br><span class="line">  Yes <span class="number">0.53183521</span> <span class="number">0.46816479</span></span><br><span class="line"></span><br><span class="line">     Age</span><br><span class="line">Y          Child      Adult</span><br><span class="line">  No  <span class="number">0.03130590</span> <span class="number">0.96869410</span></span><br><span class="line">  Yes <span class="number">0.06928839</span> <span class="number">0.93071161</span></span><br></pre></td></tr></table></figure>

<br>

<p>The confusionMatrix function from the caret package returns a test Accuracy of 0.7978, which corresponds to an <code>Bayes error rate</code> of <code>0.2023</code>.</p>
<p>$$ \text{error rate}^{Bayes} = 1 - \mathbb{E}max_k;p[Y=C_k|X=(x_1, x_2, x_3…)]$$</p>
<p>Theoretically, the Bayes classifier produces the lowest error rate if we know the true conditional probability $p(C_k | \textbf{x})$, which is not the case with real data. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(predict(bayes, test), test$Survived)</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction  No Yes</span><br><span class="line">       No  <span class="number">343</span>  <span class="number">82</span></span><br><span class="line">       Yes  <span class="number">29</span>  <span class="number">95</span></span><br><span class="line"></span><br><span class="line">               Accuracy : <span class="number">0.7978</span></span><br></pre></td></tr></table></figure>

<p>With 10-fold cross validation, the test error rate is <code>0.2095</code>.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(predict(bayes.cv, test), test$Survived)</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction  No Yes</span><br><span class="line">       No  <span class="number">332</span>  <span class="number">75</span></span><br><span class="line">       Yes  <span class="number">40</span> <span class="number">102</span></span><br><span class="line"></span><br><span class="line">               Accuracy : <span class="number">0.7905</span></span><br></pre></td></tr></table></figure>

<h3 id="K-Nearest-Neighbors"><a href="#K-Nearest-Neighbors" class="headerlink" title="K-Nearest Neighbors"></a>K-Nearest Neighbors</h3><p>The <code>KNN classifier</code> estimate the conditional probability $p(C_k | \textbf{x})$ based on the set of K predictors in the training data that are the most similar to $\textbf{x}$, representing by $\mathcal{N}$.</p>
<p>$$\hat{y} = \hat{f}^{KNN}(\textbf{x}) = C_k, ;where; k = argmax_k ;p(C_k | \textbf{x}) \
with; p(C_k | \textbf{x}) = \dfrac{\sum_{i\in\mathcal{N}}\textbf{1}_{y_i = C_k}}{K}$$</p>
<p>Note that the $p(C_k | \textbf{x})$ is one minus the $\mathcal{N}$-local error rate of a $C_k$ estimate , and therefore with KNN we are picking the $C_k$ that minimizes the $\mathcal{N}$-local error rate given $\textbf{x}$.</p>
<p>When $K=1$, the estimator $\hat{f}^{KNN}$ produces a training error rate of $0$, but the test error rate might be quite high due to overfitting. The method is therefore very flexible with low bias and high variance. As $K\rightarrow\infty$, the estimator becomes more linear.  </p>
<p>In R, we use the knn function (with K=1) in the class library to predict Survival from the Titanic dataset.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(e1071)</span><br><span class="line"><span class="keyword">library</span>(class)</span><br><span class="line">set.seed(<span class="number">9999</span>)</span><br><span class="line">df &lt;- as.data.frame(Titanic)</span><br><span class="line">df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq),]</span><br><span class="line">df$Freq &lt;- <span class="literal">NULL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The "knn" function in the "class" library only works with numeric data</span></span><br><span class="line">df$iClass &lt;- as.integer(df$Class)</span><br><span class="line">df$iSex &lt;- as.integer(df$Sex)</span><br><span class="line">df$iAge &lt;- as.integer(df$Age)</span><br><span class="line">df$iSurvived &lt;- as.integer(df$Survived)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random 75/25 train/test split</span></span><br><span class="line">train_ind &lt;- sample(seq_len(nrow(df)), size = floor(<span class="number">0.75</span> * nrow(df)))</span><br><span class="line">df_train &lt;- df[train_ind, c(<span class="number">5</span>:<span class="number">7</span>)]</span><br><span class="line">df_test &lt;- df[-train_ind, c(<span class="number">5</span>:<span class="number">7</span>)]</span><br><span class="line">df_train_cl &lt;- df[train_ind, <span class="number">4</span>]</span><br><span class="line">df_test_cl &lt;- df[-train_ind, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit KNN</span></span><br><span class="line">knn &lt;- knn(train = df_train,</span><br><span class="line">           test = df_test,</span><br><span class="line">           cl = df_train_cl,</span><br><span class="line">           k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 10-fold CV</span></span><br><span class="line">knn.cv &lt;- tune.knn(x = df[, c(<span class="number">5</span>:<span class="number">7</span>)],</span><br><span class="line">                  y = df[, <span class="number">4</span>],</span><br><span class="line">                  k = <span class="number">1</span>:<span class="number">20</span>,</span><br><span class="line">                  tunecontrol=tune.control(sampling = <span class="string">"cross"</span>),</span><br><span class="line">                  cross=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>The confusion matrix shows an error rate of <code>0.1942</code>.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(knn, df_test_cl)</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction  No Yes</span><br><span class="line">       No  <span class="number">364</span> <span class="number">101</span></span><br><span class="line">       Yes   <span class="number">6</span>  <span class="number">80</span></span><br><span class="line"></span><br><span class="line">               Accuracy : <span class="number">0.8058</span></span><br></pre></td></tr></table></figure>

<br>

<p>Now run KNN with the tune wrapper to perform 10-fold cross validation. The result recommends KNN with K=1, which turns out to be the same as what we originally tested.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; knn.cv</span><br><span class="line"></span><br><span class="line">Parameter tuning of ‘knn.wrapper’:</span><br><span class="line"></span><br><span class="line">- sampling method: <span class="number">10</span>-fold cross validation</span><br><span class="line"></span><br><span class="line">- best parameters:</span><br><span class="line"> k</span><br><span class="line"> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">- best performance: <span class="number">0.2157576</span></span><br></pre></td></tr></table></figure>

<p>Summarizing the test error rate for naiveBayes and KNN.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">naiveBayes (cv10):          <span class="number">0.2095</span></span><br><span class="line">KNN (cv10, K=<span class="number">1</span>):            <span class="number">0.1942</span></span><br></pre></td></tr></table></figure>

<h1 id="Linear-Model-8634"><a href="#Linear-Model-8634" class="headerlink" title="Linear Model &#8634;"></a><span id="01">Linear Model</span> <sup><a href="#nav">&#8634;</a></sup></h1><h2 id="Simple-Linear-Regression"><a href="#Simple-Linear-Regression" class="headerlink" title="Simple Linear Regression"></a>Simple Linear Regression</h2><p>A simple linear regression assumes that:</p>
<p>$$ Y \sim \beta_0 + \beta_1X $$</p>
<h3 id="Coefficient-Estimate"><a href="#Coefficient-Estimate" class="headerlink" title="Coefficient Estimate"></a>Coefficient Estimate</h3><p>Given data points $(x_i, y_i)$, let $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i$. We define the <code>residual sum of squares</code>, or $RSS$, as:</p>
<p>$$ RSS = \sum (y_i -  \hat{y_i})^2 $$</p>
<p>Minimizing $RSS$ as an objective function, we can solve for $\hat{\beta_0}, \hat{\beta_1}$:</p>
<p>$$\begin{align}
\hat{\beta_1} &amp;= \dfrac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i-\bar{x})^2} \
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta_1}\bar{x}
\end{align}$$</p>
<h3 id="Coefficient-Estimate-Gaussian-Residual"><a href="#Coefficient-Estimate-Gaussian-Residual" class="headerlink" title="Coefficient Estimate - Gaussian Residual"></a>Coefficient Estimate - Gaussian Residual</h3><p>Furthermore, if we assume the <code>individual</code> error terms are i.i.d Gaussian, i.e.:</p>
<p>$$ Y = \beta_0 + \beta_1x + \epsilon \
\text{where, } \epsilon \sim \mathcal{N}(0, \sigma^2)$$</p>
<p>We now have a conditional pdf of $Y$ given $x$:</p>
<p>$$ p(y_i|x_i; \beta_0, \beta_1, \sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\dfrac{[y_i - (\beta_0 + \beta_1x_i)]^2}{2\sigma^2}}$$</p>
<p>The maximum log-likelihood estimator $\hat{l}$for the paramter estimate $b_0$, $b_1$, and $s^2$ can be computed as follow:</p>
<p>$$\begin{align}
\hat{l}(b_0, b_1, s^2| x_i, y_i) &amp;= log \prod p(y_i|x_i; b_0, b_1, s^2) \
&amp;= -\dfrac{n}{2}log{2\pi} - nlogs - \dfrac{1}{2s^2}\sum [y_i - (b_0+b_1x)]^2
\end{align}$$</p>
<p>Setting the partial-derivative of the estimator with respect to each of the parameter to zero, we can obtain the maximum likelihood parameters:</p>
<p>$$\begin{align}
\hat{\beta_1} &amp;= \dfrac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i-\bar{x})^2} \
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta_1}\bar{x} \
\hat{\sigma^2} &amp;= \dfrac{1}{n}\sum [y_i - (b_0+b_1x_i)]^2
\end{align}$$</p>
<p>MLE, or maximum likelihood estimation, is a frequentist approach for estimating model parameters based on the assumed underlying model form and error distribution, by maximizing the probability of seeing what we saw in the data. MLE is a near-optimal method of estimation, and is optimal in many cases.</p>
<p>If we assume a Gaussian error distribution and a linear model, then the conclusion above states that maximizing the MLE objective function is the <code>SAME</code> as minimizing the RSS objective function.</p>
<p>More on <code>frequentist</code> vs <code>Bayesian</code> in <a href="https://cdn-files.soa.org/e-learning/Predictive_Analytics_ASA/Module_6/job_aid_MLE.pdf" target="_blank" rel="noopener">this SOA work paper</a>. Also see <a href="https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf" target="_blank" rel="noopener">this CMU lecture note</a> and  for more detail regarding the derivation.</p>
<h3 id="Model-Fit"><a href="#Model-Fit" class="headerlink" title="Model Fit"></a>Model Fit</h3><p>Recall that we assume there is a underlying relationship $f$ between $Y$ and $X$:</p>
<p>$$Y=f(X) + \epsilon$$</p>
<p>The <code>residual standard error</code>, or $RSE$, estimates the standard deviation of $\epsilon$. Note that RSE is an absolute measure of the <em>lack of fit</em> of the model and depends on units of $Y$.</p>
<p>$$ RSE = \sqrt{RSS/(n-2)} $$</p>
<p>The $R^2$ measures the proportion of variance explained by the regression. $TSS$ is the <code>total sum of squares</code> which measures the total variance in $Y$</p>
<p>$$ R^2 = 1 - \dfrac{RSS}{TSS} $$</p>
<p>In a simple regression setting, $R^2 = Corr(X, Y)^2$</p>
<h3 id="Residual-Plot"><a href="#Residual-Plot" class="headerlink" title="Residual Plot"></a>Residual Plot</h3><p>Here is a good <a href="http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/#y-unbalanced-header" target="_blank" rel="noopener">article</a> on how to interpret your residual plot.</p>
<p>Summarizing the approaches for different residual issues:</p>
<ol>
<li><code>Y-axis Unbalanced</code>: transform target</li>
<li><code>X-axis Unbalanced</code>: transform predictor</li>
<li><code>Heteroscedasticity</code>: transform target/predictor</li>
<li><code>Non-Linearity</code>: transform predictor; create non-linear model</li>
<li><code>Outlier</code>: transform target/predictor; remove/assess the outlier;</li>
</ol>
<h2 id="Multiple-Linear-Regression"><a href="#Multiple-Linear-Regression" class="headerlink" title="Multiple Linear Regression"></a>Multiple Linear Regression</h2><p>The multiple linear regression takes the form:</p>
<p>$$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p + \epsilon $$</p>
<h3 id="F-statistic"><a href="#F-statistic" class="headerlink" title="F-statistic"></a>F-statistic</h3><p>We use the F-statistic to test the null hypothesis that there are no relationships between the predictors and target variable.</p>
<p>$$ H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0 \
H_a: \text{ at least one } \beta \text{ is non-zero} $$</p>
<p>We calculate the F-statistic as follow:</p>
<p>$$ F = \dfrac{(TSS - RSS)/p}{RSS/(n-p-1)} $$</p>
<p>We expect $F=1$ if $H_0$ is true.</p>
<h3 id="Potential-Problems"><a href="#Potential-Problems" class="headerlink" title="Potential Problems"></a>Potential Problems</h3><p><code>Non-Linearity</code> Residual plots are useful to detect whether the underlying relationship $f$ is non-linear. One of the solutions to this problem is to fit transformations of the predictors such as $log{X}$, $\sqrt{X}$, and $X^2$.  </p>
<p><code>Collinearity</code> This is where two or more predictors are closely correlated with each other, or “collinear”. Collinearity reduces the accuracy of the coefficient estimates by increasing its standard deviation and p-value. One way to detect collinearity is from the correlation matrix or the “pairs” plot in R.</p>
<p>There are two solutions: dropping one of the collinear predictor, or combine the collinear predictors into a single predictor.</p>
<p><code>Multi-collinearity</code> This is where collinearity exist between three or more predictors when none of the pairwise correlations are high. The best way to assess multi-collinearity if through <code>variance inflation factor</code>, or VIF.</p>
<p>$$ VIF(\beta_j) = \dfrac{var(\beta_j) \text{ in full model}}{var(\beta_j) \text{ in single } \beta_j \text{ model}} $$</p>
<p>A VIF of $1$ indicates no collinearity. A VIF of $10+$ indicates high collinearity.</p>
<p><code>Outliers</code> Use residual plot to detect and potentially remove outliers.</p>
<h1 id="Linear-Model-Selection-and-Regularization"><a href="#Linear-Model-Selection-and-Regularization" class="headerlink" title="Linear Model Selection and Regularization"></a>Linear Model Selection and Regularization</h1><p>Linear model can be improved by using alternative fitting procedures, which produce better prediction accuracy and model interpretability.</p>
<ul>
<li><p><code>Subset Selection</code> Select a subset of the original predictors and then fit the model.</p>
</li>
<li><p><code>Shrinkage/Regularization</code> Fit the model by shrinking coefficient estimates towards zero, therefore reducing variances of the coefficient estimates.</p>
</li>
<li><p><code>Dimension Reduction</code> Project the predictors onto a M-dimensional subspace, then use the M projections as predictors to fit a linear model with least square.</p>
</li>
</ul>
<h2 id="Subset-Selection"><a href="#Subset-Selection" class="headerlink" title="Subset Selection"></a>Subset Selection</h2><p>The <code>beset subset selection</code> fits a separate least square regression for each combination from the $p$ predictors, creating $2^p$ models to compare.</p>
<p>The <code>forward stepwise selection</code> and <code>backward stepwise selection</code> fits a total of $1+p(p+1)/2$ models. Specifically, at each step a predictor is added/removed to the model only if it gives the greatest additional improvement (lowest RSS or highest adjusted $R^2$) among all the predictors.</p>
<p>After the selection process, we need to determine the optimal model that gives the lowest potential test error, either through:</p>
<ul>
<li><p>Cross-validation, or</p>
</li>
<li><p>Adjusted train error</p>
<ul>
<li><p>Example 1: $\boldsymbol{C_p} := \dfrac{1}{n}(RSS + 2d\hat{\sigma}^2)$, where $d$ is the number of predictors in the subset, and $\hat{\sigma}^2$ is the variance of error estimated using the full models containing all $p$  predictors. Essentially, a penalty term $2d\hat{\sigma}^2$ is added to the train RSS to adjust for the fact that the training error tends to <em>underestimate</em> the test error.</p>
</li>
<li><p>Example 2: $\boldsymbol{AIC} = \dfrac{1}{n\hat{\sigma}^2}(RSS + 2d\hat{\sigma}^2)$. The AIC, or Akaike information criterion, uses the maximum likelihood function to assess the relative quality of statistical models give a set of data. In linear models with Gaussian error terms, the maximum likelihood function is equivalent to $RSS$, and therefore $C_p$ and $AIC$ are proportional to each other.</p>
</li>
<li><p>Example 3: $\boldsymbol{BIC} = \dfrac{1}{n\hat{\sigma}^2}(RSS + log(n)d\hat{\sigma}^2) $</p>
</li>
<li><p>Example 4: $\boldsymbol{Adjusted}; \boldsymbol{R^2} = 1 - \dfrac{RSS/(n-d-1)}{TSS/(n-1)}$. While $RSS$ always decreases in the stepwise selection as the number of predictors increases, $RSS/(n-d-1)$ may or may not decrease.</p>
</li>
</ul>
</li>
</ul>
<h2 id="Shrinkage"><a href="#Shrinkage" class="headerlink" title="Shrinkage"></a>Shrinkage</h2><h3 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h3><p>Recall that in least square regression, the coefficient $\beta$ are estimated by minimizing the objective function RSS.</p>
<p>$$ Objective\ Function = RSS = \sum (y_i -  \hat{y_i})^2 $$</p>
<p>In <code>ridge regression</code>, the objective function include an additional <em>shrinkage penalty</em>, where $\lambda&gt;0$ is a <em>tuning parameter</em>. Note that we do not want to shrink the intercept $\beta_0$, which is simply a measure of the mean of the responses:</p>
<p>$$ Objective\ Function = RSS + \lambda\sum_{i&gt;0}\beta_i^2 $$</p>
<ul>
<li>As $\lambda$ increases, the variance decreases and the bias increases. The model fit usually is improved initially as the variance decreases, but worsen at some point when bias starts to increases rapidly. Cross-validation is often used to select the optimal $\lambda$.</li>
<li>As $\lambda\rightarrow\infty$, the coefficient approaches $0$.</li>
</ul>
<p>The ridge regression works when the linear model has low bias and high variance, e.g. when the underlying relationship is close to linear. The ridge regression trades off a small increase in bias for large decrease in variance.</p>
<p>Additionally, it is important to standardize all features when applying regularization. Imagining a feature in dollar and in thousand dollar: the model with the dollar feature will have much higher coefficient compared to the thousand dollar one, leading to larger regularization effect for the dollar feature.</p>
<h3 id="Lasso-Regression"><a href="#Lasso-Regression" class="headerlink" title="Lasso Regression"></a>Lasso Regression</h3><p>Although the ridge regression shrinks the coefficients, it does not eliminiate excess predictors. Model interpretation might be an issue for the ridge regression where the number of predictors are large. The <code>lasso regression</code> overcomes this issue and force some coefficients to be exactly $0$.</p>
<p>$$ Objective\ Function = RSS + \lambda\sum_{i&gt;0}|\beta_i| $$</p>
<p>However, there are <code>limitations</code> of feature selections using regularization techniques such as lasso, such as model interpretability. In addition, the feature we selected are optimized in linear models, and may not necessarily translate to other model forms.</p>
<p>Note the difference between $L2$ (ridge) and $L1$ (lasso) penalty:</p>
<ul>
<li>when the coefficients (absolute value) are <code>greater</code> than 1 (when the parameters are large), the $L2$ penalty is greater than the $L1$, and <code>ridge</code> provides more shrinkage.</li>
<li>when the coefficients (absolute value) are <code>smaller</code> than 1 (when the parameters are small), the $L1$ penalty is greater than the $L2$, and <code>lasso</code> provides more shrinkage.</li>
</ul>
<p>In R, we use the <code>glmnet</code> package to compute ridge and lasso regressions to predict mpg from the mtcars built-in data set.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(glmnet)</span><br><span class="line"><span class="keyword">library</span>(caret)</span><br><span class="line">set.seed(<span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># data</span></span><br><span class="line">df &lt;- as.data.frame(mtcars)</span><br><span class="line">x &lt;- model.matrix(mpg~., df)[, -<span class="number">1</span>]</span><br><span class="line">y &lt;- df$mpg</span><br><span class="line"></span><br><span class="line"><span class="comment"># 75/25 train/test split</span></span><br><span class="line">partition &lt;- createDataPartition(df$mpg, list = <span class="literal">FALSE</span>, p = <span class="number">.75</span>)</span><br><span class="line">df_train &lt;- df[partition, ]</span><br><span class="line">df_test &lt;- df[-partition, ]</span><br><span class="line">x_train &lt;- x[partition, ]</span><br><span class="line">x_test &lt;- x[-partition, ]</span><br><span class="line">y_train &lt;- y[partition]</span><br><span class="line">y_test &lt;- y[-partition]</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit regression</span></span><br><span class="line">m1 &lt;- lm(mpg ~ ., df_train)</span><br><span class="line">m1.pred &lt;- predict(m1, df_test)</span><br><span class="line">m1.mse &lt;- round(mean((y_test - m1.pred)^<span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit ridge regression</span></span><br><span class="line">m2 &lt;- cv.glmnet(x_train, y_train, alpha=<span class="number">0</span>, nfolds=<span class="number">6</span>)</span><br><span class="line">m2.bestlambda &lt;- m2$lambda.min</span><br><span class="line">m2.pred &lt;- predict(m2, s=m2.bestlambda, x_test)</span><br><span class="line">m2.mse &lt;- round(mean((y_test - m2.pred)^<span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit lasso regression</span></span><br><span class="line">m3 &lt;- cv.glmnet(x_train, y_train, alpha=<span class="number">1</span>, nfolds=<span class="number">6</span>)</span><br><span class="line">m3.bestlambda &lt;- m3$lambda.min</span><br><span class="line">m3.pred &lt;- predict(m3, s=m3.bestlambda, x_test)</span><br><span class="line">m3.mse &lt;- round(mean((y_test - m3.pred)^<span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get coefficients</span></span><br><span class="line">m2.best &lt;- glmnet(x_train, y_train, alpha=<span class="number">0</span>, lambda=m2.bestlambda)</span><br><span class="line">m3.best &lt;- glmnet(x_train, y_train, alpha=<span class="number">1</span>, lambda=m3.bestlambda)</span><br><span class="line">comp &lt;- cbind(coef(m1), coef(m2.best), coef(m3.best))</span><br><span class="line">colnames(comp) &lt;- c(<span class="string">"original"</span>, <span class="string">"ridge"</span>, <span class="string">"lasso"</span>)</span><br></pre></td></tr></table></figure>

<p>The test MSE are as follow. Note that both ridge and lasso regression perform better than the original regression.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; m1.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">16.64</span></span><br><span class="line">&gt; m2.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">3.66</span></span><br><span class="line">&gt; m3.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">6.61</span></span><br></pre></td></tr></table></figure>

<p>We also made a comparison of the coefficients, based on the normal regression and the regularized regression with cv-optimal $lambda$.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt; comp</span><br><span class="line"><span class="number">11</span> x <span class="number">3</span> sparse Matrix of class <span class="string">"dgCMatrix"</span></span><br><span class="line">                original        ridge        lasso</span><br><span class="line">(Intercept) -<span class="number">19.52071389</span> <span class="number">19.420775180</span> <span class="number">14.002192375</span></span><br><span class="line">cyl           <span class="number">1.69431225</span> -<span class="number">0.275408324</span>  .          </span><br><span class="line">disp          <span class="number">0.01185998</span> -<span class="number">0.004705579</span>  .          </span><br><span class="line">hp           -<span class="number">0.01449594</span> -<span class="number">0.011129305</span> -<span class="number">0.002545135</span></span><br><span class="line">drat          <span class="number">3.08676192</span>  <span class="number">1.272808791</span>  <span class="number">1.334526777</span></span><br><span class="line">wt           -<span class="number">3.19280650</span> -<span class="number">1.137507747</span> -<span class="number">2.254408320</span></span><br><span class="line">qsec          <span class="number">1.02473436</span>  <span class="number">0.117671597</span>  <span class="number">0.331182874</span></span><br><span class="line">vs            <span class="number">0.97127211</span>  <span class="number">0.677494227</span>  .          </span><br><span class="line">am            <span class="number">2.63740010</span>  <span class="number">1.633418877</span>  <span class="number">1.703501439</span></span><br><span class="line">gear          <span class="number">3.36943552</span>  <span class="number">0.794847062</span>  <span class="number">1.571031414</span></span><br><span class="line">carb         -<span class="number">1.45443855</span> -<span class="number">0.588427657</span> -<span class="number">1.162118484</span></span><br></pre></td></tr></table></figure>

<h2 id="Resampling-Method"><a href="#Resampling-Method" class="headerlink" title="Resampling Method"></a>Resampling Method</h2><p>Resampling methods involve repeatedly drawing samples from a training set to re-fit the model.</p>
<h3 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross-Validation"></a>Cross-Validation</h3><p>We often use the <code>test error rate</code> to determine and compare how well a statistical learning model perform. However, in the absence of a large designated test set, the test error rate can be difficult to estimate. The train error rate is often quite different from the test error rate. Therefore, cross-validation can be used to estimate the test error rates by creating validation sets off the train data.</p>
<p>A <code>k-fold cross validation</code> involves randomly dividing the observations into $k$-groups. The process is repeated $k$-times where each group is treated as a validation set while fitting the remaining $k-1$ groups. The $k$-fold CV test MSE is the average of all the MSE from each validation set:</p>
<p>$$ MSE_{CV} = \dfrac{1}{k}\sum_{i=1}^{k} MSE_i$$</p>
<p>The <code>leave-one-out cross validation</code>, or LOOCV, is a special case of $k$-fold CV with $k=1$. Since LOOCV requires fitting the model $n$ times, with $n$ being equal to number of train data points. The $k$-fold CV with $k=5\ or \ 10$ are more feasible computationally as it only needs to fit the model $5\ or\ 10$ times.</p>
<h3 id="Bootstrap"><a href="#Bootstrap" class="headerlink" title="Bootstrap"></a>Bootstrap</h3><p>Bootstrap provides a <code>measure of accruacy</code> of either a parameter estimate or a given statistical learning method. It can be used to estimate variance of a parameter by repeatedly re-sampling the same data set with replacement (i.e. duplicate data entries allowed) while re-calculating the parameter based on each re-sample.</p>
<h2 id="Hyper-Parameter-Tuning"><a href="#Hyper-Parameter-Tuning" class="headerlink" title="Hyper-Parameter Tuning"></a>Hyper-Parameter Tuning</h2><p>We specify numerious constants called <code>hyperparameter</code> during our modeling process, e.g. $\lambda$, $\alpha$, etc. To set these constant such that our model can predict accruately while avoiding over-complexity and overfitting, we <code>tune</code> our hyperparameter with cross validation. For more details see the <a href="https://jackliu234.com/pa-review-notebook/auto-claim/">auto claim notebook</a>.</p>
<h1 id="Generalized-Linear-Model"><a href="#Generalized-Linear-Model" class="headerlink" title="Generalized Linear Model"></a>Generalized Linear Model</h1><p><code>Generalized linear models</code> were developed by Nelder and Wedderburn in a <a href="https://docs.ufpr.br/~taconeli/CE225/Artigo.pdf" target="_blank" rel="noopener">paper</a> published in 1972 to provide a flexible framework which introduces a link function that transform the linear combinations of predictors.</p>
<p>An <code>Ordinary Linear Model</code> has many limitations:</p>
<ol>
<li>As ordinary linear model produces a numeric response, it requires the assumptions of orderings to predict qualitative responses.</li>
<li>Negative values may be predicted when not allowed.</li>
<li>When the variance of the target variable depends on the mean, the homoscedasticity assumption is violated, and therefore the least square estimator is no longer the MLE estimator and various statistical test would not hold.</li>
<li>Sensitive to outliers.</li>
<li>Does not perform well with non-linear relationships.</li>
</ol>
<br>

<p>The <code>Generalized Linear Models</code> relaxes the assumptions of OLM. First, GLM relaxes the normal residual assumption of OLM, and allow the target variable $Y$ to follow any distribution within the exponential distribution family:</p>
<p>$$ f(y_i|x_i) \sim \text{exponential distribution family} $$</p>
<p>With regard to this distribution, there exists a <code>canonical</code> link function associated with it that simplifies the mathematics of solving GLM analytically.</p>
<ul>
<li>Normal =&gt; Identity: $\phi(a) = a$</li>
<li>Exponential/Gamma =&gt; Negative Inverse: $\phi(a) = - a^{-1}$</li>
<li>Inverse Gaussian =&gt; Inverse Square: $\phi(a) = a^{-2}$</li>
<li>Poisson =&gt; Log: $\phi(a) = ln(a)$</li>
<li>Bernoulli/Binomial/Multinomial =&gt; Logit: $\phi(a) = ln[a/(1-a)]$</li>
</ul>
<p>We can either choose the canonical link function or pick another one (which may not lead to a converged GLM solution, however). With this link function, GLM assumes that the expectation of the target is the inverse linked linear combination of predictors:</p>
<p>$$ \mathbb{E}(y_i|x_i) = \phi^{-1}(\beta x_i) $$</p>
<p>With all above assumptions satisfy, the coefficient $\beta$ of a GLM model can then be solved:</p>
<p>$$ \phi(Y) \sim \beta X $$</p>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>The <code>logistic regression</code> model is popular for classification problems. With two response classes, we can calculate the probability of assigning the response in each class and predict the response by choosing the class with the higher probability. or more than two response classes, multiple-class logistic regression is available but the <code>discrimentant analysis</code> is more popular.</p>
<p>We define $p(X) = \mathbb{P}[Y = 1|X]$ as our new response variable and the <code>link</code> function: <code>logit</code> function, short for logistic function, as such:</p>
<p>$$ \phi(p(X)) = logit(p(X)) = log[\dfrac{p(X)}{1-p(X)}]$$</p>
<p>Since we assume a linear relationship between our predictor $X$ and the linked reponse $logit(p(X))$, we have:</p>
<p>$$ log[\dfrac{p(X)}{1-p(X)}] = \beta_0+\beta_1X_1+\beta_2X_2\dots $$</p>
<p>Therefore,</p>
<p>$$ p(X) =  \dfrac{e^{\beta_0+\beta_1X_1+\beta_2X_2\dots}}{1+e^{\beta_0+\beta_1X_1+\beta_2X_2\dots}} $$</p>
<p>Now we have a nice property of $p(X) \in (0, 1)$, which is exactly what we wanted to model probability responses. The quantity $p(X)/(1-p(X))$ is called the <code>odds</code>.</p>
<p>In R, we use the glm function (with family=binomial) in the predict Survival from the Titanic dataset.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(e1071)</span><br><span class="line"><span class="keyword">library</span>(caret)</span><br><span class="line">set.seed(<span class="number">9999</span>)</span><br><span class="line">df &lt;- as.data.frame(Titanic)</span><br><span class="line">df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]</span><br><span class="line">df &lt;- subset(df, select = -c(Freq))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Binarize class, sex and age</span></span><br><span class="line">df.new &lt;- predict(dummyVars(<span class="string">"~Class+Sex+Age"</span>,</span><br><span class="line">                            df,</span><br><span class="line">                            sep=<span class="string">"_"</span>,</span><br><span class="line">                            fullRank=<span class="literal">TRUE</span>), df)</span><br><span class="line">df.new &lt;- as.data.frame(df.new)</span><br><span class="line">df.new[<span class="string">"Survived"</span>] &lt;- df$Survived</span><br><span class="line">df &lt;- df.new</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random 80/20 train/test split</span></span><br><span class="line">train_ind &lt;- sample(seq_len(nrow(df)), size = floor(<span class="number">0.80</span> * nrow(df)))</span><br><span class="line">df_train &lt;- df[train_ind, ]</span><br><span class="line">df_test &lt;- df[-train_ind, ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit Logistic Regression</span></span><br><span class="line">model &lt;- glm(Survived~.,</span><br><span class="line">             df_train,</span><br><span class="line">             family=binomial)</span><br><span class="line">summary(model)</span><br><span class="line">contrasts(df$Survived)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict In-Sample</span></span><br><span class="line">prob_train &lt;- predict(model, df_train, type=<span class="string">"response"</span>)</span><br><span class="line">pred_train &lt;- rep(<span class="string">"No"</span>, nrow(df_train))</span><br><span class="line">pred_train[prob_train &gt; <span class="number">.5</span>] &lt;- <span class="string">"Yes"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict Out-Of-Sample</span></span><br><span class="line">prob_test &lt;- predict(model, df_test, type=<span class="string">"response"</span>)</span><br><span class="line">pred_test &lt;- rep(<span class="string">"No"</span>, nrow(df_test))</span><br><span class="line">pred_test[prob_test &gt; <span class="number">.5</span>] &lt;- <span class="string">"Yes"</span></span><br></pre></td></tr></table></figure>

<p>From <code>summary(model)</code>, note that most coeefficients are significant.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; summary(model)</span><br><span class="line">Coefficients:</span><br><span class="line">            Estimate Std. Error z value Pr(&gt;|z|)    </span><br><span class="line">(Intercept)   <span class="number">0.3094</span>     <span class="number">0.3132</span>   <span class="number">0.988</span>    <span class="number">0.323</span>    </span><br><span class="line">Class_2nd    -<span class="number">1.1163</span>     <span class="number">0.2187</span>  -<span class="number">5.105</span> <span class="number">3.31e-07</span> `</span><br><span class="line">Class_3rd    -1.7041     0.1918  -8.883  &lt; 2e-16 `</span><br><span class="line">Class_Crew   -<span class="number">0.8848</span>     <span class="number">0.1773</span>  -<span class="number">4.991</span> <span class="number">6.01e-07</span> `</span><br><span class="line">Sex_Female    2.3691     0.1555  15.235  &lt; 2e-16 `</span><br><span class="line">Age_Adult    -<span class="number">0.6289</span>     <span class="number">0.2767</span>  -<span class="number">2.273</span>    <span class="number">0.023</span> *</span><br></pre></td></tr></table></figure>

<p>Note that probability of 1 correspond to “Yes” in the Survived variable.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; contrasts(df$Survived)</span><br><span class="line">    Yes</span><br><span class="line">No    <span class="number">0</span></span><br><span class="line">Yes   <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>From in-sample confusion matrix.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(as.factor(pred_train), df_train$Survived)</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction   No  Yes</span><br><span class="line">       No  <span class="number">1087</span>  <span class="number">296</span></span><br><span class="line">       Yes  <span class="number">102</span>  <span class="number">275</span></span><br><span class="line"></span><br><span class="line">               Accuracy : <span class="number">0.7739</span></span><br></pre></td></tr></table></figure>

<p>From out-of-sample confusion matrix.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(as.factor(pred_test), df_test$Survived)</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction  No Yes</span><br><span class="line">       No  <span class="number">277</span>  <span class="number">66</span></span><br><span class="line">       Yes  <span class="number">24</span>  <span class="number">74</span></span><br><span class="line"></span><br><span class="line">               Accuracy : <span class="number">0.7959</span></span><br></pre></td></tr></table></figure>

<p>Comparing the test error rate between naiveBayes, KNN and logistic regression.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">naiveBayes (cv10):          <span class="number">0.2095</span></span><br><span class="line">KNN (cv10, K=<span class="number">1</span>):            <span class="number">0.1942</span></span><br><span class="line">logistic regression:        <span class="number">0.2041</span></span><br></pre></td></tr></table></figure>

<h2 id="Poisson-Regression"><a href="#Poisson-Regression" class="headerlink" title="Poisson Regression"></a>Poisson Regression</h2><p>The <code>Poisson distribution</code> expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event.</p>
<p>$$ \mathbb{P}(\text{k events in interval}) = e^{\lambda}\dfrac{\lambda^k}{k!} $$</p>
<p>We can fit <code>Posisson regression</code> if we observe that the frequencies of response variable $Y$ exhibits a Poisson shape. We will create a new response vector $\theta$ and assumes that $log(\theta)$ has an underlying linear relationship in $X$.</p>
<p>$$ Y \sim poisson(\boldsymbol {\theta}) \
\text{and, } log(\boldsymbol {\theta}) = \beta X$$</p>
<p>That is, we assume that for each $X_i$, $Y_i \sim poisson(e^{\beta X_i})$. The log link function ensures that $\boldsymbol{\theta}$ is strictly positive.</p>
<p>Note that we had made a strong assumption that for each $X_i$, the mean and variance of $Y_i$ are the same, as dictated by the Poisson distribution. However, if the data shows larger variance than expected, or <code>overdispersion</code>, we can then use the <code>quasi-Poisson regression</code>, which is essenstially the negative binomial distribution with looser assumptions than Poisson.</p>
<p>In the <code>diamonds</code> dataset from the <code>ggplot2</code> package, we plotted the histogram of the price data from 50,000 observations.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line">df &lt;- as.data.frame(diamonds)</span><br><span class="line">ggplot(df, aes(x=price)) + geom_histogram(binwidth=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>The price data shows resemblence to a Poisson distribution.</p>
<p><img src="statistical-learning-Rplot1.png" alt="Rplot1.png"></p>
<p>We fitted four different models: linear, ridge, lasso, and Poisson regressions.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(e1071)</span><br><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line"><span class="keyword">library</span>(caret)</span><br><span class="line"><span class="keyword">library</span>(glmnet)</span><br><span class="line">set.seed(<span class="number">9999</span>)</span><br><span class="line">df &lt;- as.data.frame(diamonds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># caret::dummyVars does not work well with ordered factor. change to unordered.</span></span><br><span class="line">df[<span class="string">"cut_1"</span>] &lt;- factor(df$cut, order=<span class="literal">FALSE</span>)</span><br><span class="line">df[<span class="string">"color_1"</span>] &lt;- factor(df$color, order=<span class="literal">FALSE</span>)</span><br><span class="line">df[<span class="string">"clarity_1"</span>] &lt;- factor(df$clarity, order=<span class="literal">FALSE</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Binarize Category Variable</span></span><br><span class="line">dummy &lt;- dummyVars(~ cut_1 + color_1 + clarity_1, df, sep=<span class="string">"_"</span>, fullRank=<span class="literal">TRUE</span>)</span><br><span class="line">df.new &lt;- as.data.frame(predict(dummy, newdata = df))</span><br><span class="line">df.new[<span class="string">"carat"</span>] &lt;- df$carat</span><br><span class="line">df.new[<span class="string">"price"</span>] &lt;- df$price</span><br><span class="line">df &lt;- df.new</span><br><span class="line">x &lt;- model.matrix(price~., df)[, -<span class="number">1</span>]</span><br><span class="line">y &lt;- df$price</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random 80/20 train/test split</span></span><br><span class="line">partition &lt;- sample(seq_len(nrow(df)), size = floor(<span class="number">0.80</span> * nrow(df)))</span><br><span class="line">df_train &lt;- df[partition, ]</span><br><span class="line">df_test &lt;- df[-partition, ]</span><br><span class="line">x_train &lt;- x[partition, ]</span><br><span class="line">x_test &lt;- x[-partition, ]</span><br><span class="line">y_train &lt;- y[partition]</span><br><span class="line">y_test &lt;- y[-partition]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit Linear Regression</span></span><br><span class="line">m1 &lt;- lm(price~., df_train)</span><br><span class="line">m1.pred &lt;- predict(m1, df_test)</span><br><span class="line">m1.mse &lt;- round(mean((y_test - m1.pred)^<span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit Ridge Regression</span></span><br><span class="line">m2 &lt;- cv.glmnet(x_train, y_train, alpha=<span class="number">0</span>, nfolds=<span class="number">10</span>)</span><br><span class="line">m2.bestlambda &lt;- m2$lambda.min</span><br><span class="line">m2.pred &lt;- predict(m2, s=m2.bestlambda, x_test)</span><br><span class="line">m2.mse &lt;- round(mean((y_test - m2.pred)^<span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit Lasso Regression</span></span><br><span class="line">m3 &lt;- cv.glmnet(x_train, y_train, alpha=<span class="number">1</span>, nfolds=<span class="number">10</span>)</span><br><span class="line">m3.bestlambda &lt;- m3$lambda.min</span><br><span class="line">m3.pred &lt;- predict(m3, s=m3.bestlambda, x_test)</span><br><span class="line">m3.mse &lt;- round(mean((y_test - m3.pred)^<span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit Poisson Regression</span></span><br><span class="line">m4 &lt;- glm(price~., df_train, family=poisson(link=<span class="string">"log"</span>))</span><br><span class="line">m4.pred &lt;- predict(m4, df_test)</span><br><span class="line">m4.mse &lt;- round(mean((y_test - exp(m4.pred))^<span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit quasiPoisson Regression</span></span><br><span class="line">m5 &lt;- glm(price~., df_train, family=quasipoisson(link=<span class="string">"log"</span>))</span><br><span class="line">m5.pred &lt;- predict(m5, df_test)</span><br><span class="line">m5.mse &lt;- round(mean((y_test - exp(m5.pred))^<span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compare Coefficient</span></span><br><span class="line">m2.best &lt;- glmnet(x_train, y_train, alpha=<span class="number">0</span>, lambda=m2.bestlambda)</span><br><span class="line">m3.best &lt;- glmnet(x_train, y_train, alpha=<span class="number">1</span>, lambda=m3.bestlambda)</span><br><span class="line">comp &lt;- cbind(coef(m1), coef(m2.best), coef(m3.best), coef(m4), coef(m5))</span><br><span class="line">colnames(comp) &lt;- c(<span class="string">"original"</span>, <span class="string">"ridge"</span>, <span class="string">"lasso"</span>, <span class="string">"Poisson"</span>, <span class="string">"quasiPoi"</span>)</span><br></pre></td></tr></table></figure>

<p>Showing the results. We can see that lasso regression improved upon ridge. However, the Poisson regression show very high MSE, and not improved by using quasi-Poisson to deal with overdispersion.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt; m1.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">1296082</span></span><br><span class="line"></span><br><span class="line">&gt; m2.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">1662769</span></span><br><span class="line"></span><br><span class="line">&gt; m3.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">1296773</span></span><br><span class="line"></span><br><span class="line">&gt; m4.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">5237564</span></span><br><span class="line"></span><br><span class="line">&gt; m5.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">5237564</span></span><br></pre></td></tr></table></figure>

<p>Comparing coefficients:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt; comp</span><br><span class="line"><span class="number">19</span> x <span class="number">5</span> sparse Matrix of class <span class="string">"dgCMatrix"</span></span><br><span class="line">                    original        ridge      lasso     Poisson    quasiPoi</span><br><span class="line">(Intercept)       -<span class="number">7369.2859</span> -<span class="number">2751.693983</span> -<span class="number">7083.4777</span>  <span class="number">5.17185599</span>  <span class="number">5.17185599</span></span><br><span class="line">cut_1_Good          <span class="number">686.6877</span>   <span class="number">112.163397</span>   <span class="number">657.1100</span>  <span class="number">0.17515789</span>  <span class="number">0.17515789</span></span><br><span class="line">`cut_1_Very Good`   <span class="number">864.3160</span>   <span class="number">319.828814</span>   <span class="number">838.9901</span>  <span class="number">0.20528993</span>  <span class="number">0.20528993</span></span><br><span class="line">cut_1_Premium       <span class="number">893.6342</span>   <span class="number">367.364011</span>   <span class="number">866.6581</span>  <span class="number">0.18009057</span>  <span class="number">0.18009057</span></span><br><span class="line">cut_1_Ideal        <span class="number">1025.5579</span>   <span class="number">421.231162</span>   <span class="number">999.8031</span>  <span class="number">0.20406528</span>  <span class="number">0.20406528</span></span><br><span class="line">color_1_E          -<span class="number">225.3985</span>   -<span class="number">23.486004</span>  -<span class="number">205.5637</span> -<span class="number">0.05660526</span> -<span class="number">0.05660526</span></span><br><span class="line">color_1_F          -<span class="number">294.1807</span>     <span class="number">2.717515</span>  -<span class="number">274.2126</span> -<span class="number">0.03079075</span> -<span class="number">0.03079075</span></span><br><span class="line">color_1_G          -<span class="number">522.9599</span>  -<span class="number">112.556133</span>  -<span class="number">501.1661</span> -<span class="number">0.10597258</span> -<span class="number">0.10597258</span></span><br><span class="line">color_1_H          -<span class="number">997.6266</span>  -<span class="number">490.885083</span>  -<span class="number">975.9837</span> -<span class="number">0.25411004</span> -<span class="number">0.25411004</span></span><br><span class="line">color_1_I         -<span class="number">1452.1455</span>  -<span class="number">749.323400</span> -<span class="number">1426.9279</span> -<span class="number">0.42200863</span> -<span class="number">0.42200863</span></span><br><span class="line">color_1_J         -<span class="number">2343.2864</span> -<span class="number">1450.247841</span> -<span class="number">2315.1429</span> -<span class="number">0.63819087</span> -<span class="number">0.63819087</span></span><br><span class="line">clarity_1_SI2      <span class="number">2612.5463</span>  -<span class="number">472.362109</span>  <span class="number">2345.9392</span>  <span class="number">1.14176901</span>  <span class="number">1.14176901</span></span><br><span class="line">clarity_1_SI1      <span class="number">3555.6013</span>   <span class="number">149.192295</span>  <span class="number">3287.0687</span>  <span class="number">1.38540593</span>  <span class="number">1.38540593</span></span><br><span class="line">clarity_1_VS2      <span class="number">4210.3193</span>   <span class="number">671.522828</span>  <span class="number">3940.7103</span>  <span class="number">1.50843614</span>  <span class="number">1.50843614</span></span><br><span class="line">clarity_1_VS1      <span class="number">4507.7144</span>   <span class="number">861.334888</span>  <span class="number">4235.9431</span>  <span class="number">1.58850692</span>  <span class="number">1.58850692</span></span><br><span class="line">clarity_1_VVS2     <span class="number">4965.2210</span>  <span class="number">1192.175019</span>  <span class="number">4691.8855</span>  <span class="number">1.66443836</span>  <span class="number">1.66443836</span></span><br><span class="line">clarity_1_VVS1     <span class="number">5072.6388</span>  <span class="number">1162.725817</span>  <span class="number">4796.5710</span>  <span class="number">1.61060572</span>  <span class="number">1.61060572</span></span><br><span class="line">clarity_1_IF       <span class="number">5436.6567</span>  <span class="number">1476.377453</span>  <span class="number">5158.0224</span>  <span class="number">1.72336616</span>  <span class="number">1.72336616</span></span><br><span class="line">carat              <span class="number">8899.8818</span>  <span class="number">7672.223943</span>  <span class="number">8883.8976</span>  <span class="number">1.65798106</span>  <span class="number">1.65798106</span></span><br></pre></td></tr></table></figure>

<p>We are curious as to why the Poisson regression perform much worse than a simple linear regression, when the reponse variable clearly shows Poisson patterns.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">p1 &lt;- ggplot() +</span><br><span class="line">  geom_point(data=df_test, aes(x=as.numeric(row.names(df_test)), y=price),</span><br><span class="line">             shape=<span class="string">"."</span>, color=<span class="string">"black"</span>) +</span><br><span class="line">  geom_point(aes(x=as.numeric(row.names(df_test)), y=m1.pred),</span><br><span class="line">             shape=<span class="string">"."</span>, color=<span class="string">"red"</span>)</span><br><span class="line"></span><br><span class="line">p2 &lt;- ggplot() +</span><br><span class="line">  geom_point(data=df_test, aes(x=as.numeric(row.names(df_test)), y=price),</span><br><span class="line">             shape=<span class="string">"."</span>, color=<span class="string">"black"</span>) +</span><br><span class="line">  geom_point(aes(x=as.numeric(row.names(df_test)), y=exp(m4.pred)),</span><br><span class="line">           shape=<span class="string">"."</span>, color=<span class="string">"red"</span>)</span><br></pre></td></tr></table></figure>

<p>We first look at the linear regression fit, where the black dots are the original data and the red dots are the fitted data:</p>
<p><img src="statistical-learning-Rplotp1.png" alt="Rplotp1.png"></p>
<p>We then look at the Poisson regression fit. Unfortunately the Poisson regression create ultra-high predictions for some values, which skew the MSE matrix. This is we forget to log the numerical variable (carat) when we use log link function, which results in a exponential shape for the prediction.</p>
<p><img src="statistical-learning-Rplotp2.png" alt="Rplotp2.png"></p>
<p>We change the code as follow:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fit Poisson Regression</span></span><br><span class="line">m4 &lt;- glm(price~-carat+log(carat), df_train, family=poisson(link=<span class="string">"log"</span>))</span><br><span class="line">m4.pred &lt;- predict(m4, df_test)</span><br><span class="line">m4.mse &lt;- round(mean((y_test - exp(m4.pred))^<span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit quasiPoisson Regression</span></span><br><span class="line">m5 &lt;- glm(price~-carat+log(carat)., df_train, family=quasipoisson(link=<span class="string">"log"</span>))</span><br><span class="line">m5.pred &lt;- predict(m5, df_test)</span><br><span class="line">m5.mse &lt;- round(mean((y_test - exp(m5.pred))^<span class="number">2</span>), <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>The mse are now better:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; m4.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">2544181</span></span><br><span class="line">&gt; m5.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">2544181</span></span><br></pre></td></tr></table></figure>

<p>Plotting the prediction. We can see that other than one prediction outlier, the overall predictions are better than when we did not log the carat.</p>
<p><img src="statistical-learning-Rplot.new.png" alt="Rplot.new.png"></p>
<h2 id="Goodness-of-Fit"><a href="#Goodness-of-Fit" class="headerlink" title="Goodness of Fit"></a>Goodness of Fit</h2><p><code>Deviance</code> is a measure of the goodness of fit of a generalized linear model, similar to a $RSS$ in the simple linear model. . The default value is called <code>null deviance</code> and is the deviance calculated when the response variable is predicted using its sample mean.</p>
<p><code>Adding</code> additional feature to the model would generally <code>decrease</code> the deviance and <code>decrease</code> the degree of freedoms.</p>
<h1 id="Tree-Based-Method-8634"><a href="#Tree-Based-Method-8634" class="headerlink" title="Tree-Based Method &#8634;"></a><span id="02">Tree-Based Method</span> <sup><a href="#nav">&#8634;</a></sup></h1><p>The <code>tree-based</code> method involve segmenting the predictor space into several regions to make a prediction for a given observation. There are several advantages to the tree-based methods:</p>
<ul>
<li>Easy to explain</li>
<li>Intuitive to human reasoning</li>
<li>Graphic and interpretable</li>
<li>No need to dummy variables for qualitative data</li>
</ul>
<p>On the other hand, disadvantages:</p>
<ul>
<li>Lower predictive accuracy</li>
<li>Sensitive to change in data</li>
</ul>
<p>Several techniques can make significant improvement to compensate the disadvantages, namely bagging, random forest and boosting.  </p>
<h2 id="Regression-Tree"><a href="#Regression-Tree" class="headerlink" title="Regression Tree"></a>Regression Tree</h2><p>In a linear regression model, the underlying relationship is assumed to be:</p>
<p>$$ f(X) = \beta_0 + \sum\beta_iX_i $$</p>
<p>Whereas in a <code>regression tree</code> model, the underlying is assumed to be:</p>
<p>$$ f(X) = \sum c_i\textbf{1}_{X_i\in R_i} $$</p>
<p>Where each $R_i$ represent a partition of the feature space. The goal is to solve for the partition set ${R_i}_{i\in I}$ which minimize the objective function:</p>
<p>$$ Objective; Function = RSS = \sum_{i\in I}\sum_{X_j\in R_i} (y_j-\bar{y}_{R_i})^2 $$</p>
<p>To find ${R_i}_{i\in I}$ efficiently, we introduce <code>recursive binary splitting</code>, which is a top-down and greedy approach. It is greedy because it is short-sighted in that it always chooses the current best split, instead of the optimal split overall.</p>
<p>Due to the greedy nature, it is preferred that we first grow a complex tree and then <code>prune</code> it back, so that all potential large reductions in RSS are captured.</p>
<h3 id="Cost-Complexity-Pruning"><a href="#Cost-Complexity-Pruning" class="headerlink" title="Cost Complexity Pruning"></a>Cost Complexity Pruning</h3><p>The <code>cost complexity pruning</code> approach aim to minimize the objective function, give each value of $\alpha$:</p>
<p>$$ Objective; Function = \sum_{i\in I_T}\sum_{X_j\in R_i} (y_j-\bar{y}_{R_i})^2 + \alpha|T|$$</p>
<p>Where $|T|$ is the number of terminal nodes of subtree $T\subset T_0$ where $T_0$ is the original un-prune tree. The tuning parameter $\alpha$ controls the complexity of the subtree $T$, penalizing any increase in nodes. The goal is to prune the tree with various $\alpha$ and then use cross-validation to select the best $\alpha$.</p>
<p>This is similar to the lasso equation, which also introduce a tuning parameter $\lambda$ to control the complexity of a linear model.</p>
<p>$$ Objective; Function = RSS + \lambda\sum_{i&gt;0}|\beta_i| $$</p>
<p>In R, we use the <code>rpart</code> library, which stands for recursive partitioning and regression trees, to fit a regression tree to predict <code>mpg</code> in our <code>mtcars</code> dataset.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(caret)</span><br><span class="line"><span class="keyword">library</span>(rpart)</span><br><span class="line"><span class="keyword">library</span>(rpart.plot)</span><br><span class="line">set.seed(<span class="number">9999</span>)</span><br><span class="line">df &lt;- as.data.frame(mtcars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a 75/25 train/test split</span></span><br><span class="line">partition &lt;- createDataPartition(df$mpg, list=<span class="literal">FALSE</span>, p=<span class="number">0.75</span>)</span><br><span class="line">train &lt;- df[partition, ]</span><br><span class="line">test &lt;- df[-partition, ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit regression tree</span></span><br><span class="line">t1 &lt;- rpart(mpg ~ ., train)</span><br><span class="line">t1.predict &lt;- predict(t1, test)</span><br><span class="line">t1.mse &lt;- round(mean((test$mpg - t1.predict)^<span class="number">2</span>), <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>The test MSE, as compared to the previous linear models.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># regression tree</span></span><br><span class="line">&gt; t1.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">11.59</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># linear regression</span></span><br><span class="line">&gt; m1.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">16.64</span></span><br><span class="line">&gt; m2.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">3.66</span></span><br><span class="line">&gt; m3.mse</span><br><span class="line">[<span class="number">1</span>] <span class="number">6.61</span></span><br></pre></td></tr></table></figure>

<p>Plotting the tree with <code>rpart.plot(t1)</code> command.
<img src="statistical-learning-Rplot.png" alt="Rplot.png"></p>
<h2 id="Classification-Tree"><a href="#Classification-Tree" class="headerlink" title="Classification Tree"></a>Classification Tree</h2><p>Classification tree is very similar to regression trees expect we need an alternative method to RSS when deciding a split. There are three common approaches.</p>
<ul>
<li><code>Classification error rate</code>: $ E = 1 - \underset{k}{max}(\hat{p}_{mk}) $</li>
</ul>
<p>Where $\hat{p}_{mk}$ represents the porportion of train observations from the $m$-th parent node that are from the $k$-th child node. However, the this approach is not sufficiently sensitive to node impurity for tree-growing, compared to the next two.</p>
<ul>
<li><code>Gini index</code>: $G = \sum_k \hat{p}<em>{mk}(1-\hat{p}</em>{mk}) $</li>
</ul>
<p>Note that the Gini index decreases as all $\hat{p}_{mk}$ get closer to $0$ or $1$. Therefore it is a measure of the <em>node purity</em>.  </p>
<ul>
<li><code>Entropy</code>: $ D = -\sum_k \hat{p}<em>{mk} log_2(\hat{p}</em>{mk}) $</li>
</ul>
<p>The Entropy is also a measure of the <em>node purity</em> and similar to the Gini index numerically.</p>
<p>For a two-class decision tree, the impurity measures calculated from different methods for a given $\hat{p}_{mk}$ are simulated below with python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.001</span>)</span><br><span class="line">y1 = x*np.log(x)/np.log(<span class="number">2</span>)*<span class="number">-1</span> + (<span class="number">1</span>-x)*np.log(<span class="number">1</span>-x)/np.log(<span class="number">2</span>)*<span class="number">-1</span></span><br><span class="line">y2 = x*(<span class="number">1</span>-x) + (<span class="number">1</span>-x)*(<span class="number">1</span>-(<span class="number">1</span>-x))</span><br><span class="line">y3 = <span class="number">1</span>-np.maximum(x, <span class="number">1</span>-x)</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(x, y1)</span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.plot(x, y3)</span><br><span class="line">plt.legend([<span class="string">'entropy'</span>, <span class="string">'gini'</span>, <span class="string">'class error'</span>], loc=<span class="string">'upper right'</span>)</span><br></pre></td></tr></table></figure>

<p><img src="statistical-learning-2-Unknown.png" alt="Unknown.png"></p>
<p>We can see that all three method are similar and consistent with each other. Entropy and the Gini are more sensitive to changes in the node probabilities, therefore preferrable when <code>growing</code> the trees. The classification error is more often used during <code>complexity pruning</code>.</p>
<h3 id="Information-Gain"><a href="#Information-Gain" class="headerlink" title="Information Gain"></a>Information Gain</h3><p>When the measure of node purity is calculated, we want to maximize the <code>information gain</code> after each split. We use $P$ and $C$ to denote the parent and child node, with entropy as the measure. $N$ is the number of observations under the parent node:</p>
<p>$$ IG = Entropy(P) - \sum_{k=1}^{K}\dfrac{N_k}{N}Entropy(C_k) $$</p>
<p>In R, we use the <code>rpart</code> library to create a classification tree to predict <code>Survived</code> in the <code>Titanic</code> data set.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(caret)</span><br><span class="line"><span class="keyword">library</span>(rpart)</span><br><span class="line"><span class="keyword">library</span>(rpart.plot)</span><br><span class="line">set.seed(<span class="number">9999</span>)</span><br><span class="line">df &lt;- as.data.frame(Titanic)</span><br><span class="line">df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]</span><br><span class="line">df &lt;- subset(df, select = -c(Freq))</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a 75/25 train/test split</span></span><br><span class="line">partition &lt;- createDataPartition(df$Survived, list=<span class="literal">FALSE</span>, p=<span class="number">0.75</span>)</span><br><span class="line">train &lt;- df[partition, ]</span><br><span class="line">test &lt;- df[-partition, ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit classification tree</span></span><br><span class="line">t2 &lt;- rpart(Survived ~ ., train)</span><br><span class="line">t2.prob &lt;- predict(t2, test)</span><br><span class="line">t2.pred &lt;- rep(<span class="string">"No"</span>, nrow(t2.prob))</span><br><span class="line">t2.pred[t2.prob[, <span class="number">2</span>] &gt; <span class="number">.5</span>] &lt;- <span class="string">"Yes"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pruning</span></span><br><span class="line">t2.prune &lt;- prune(t2, cp = <span class="number">0.05</span>)</span><br><span class="line">rpart.plot(t2.prune)</span><br></pre></td></tr></table></figure>

<p>The confusion matrix shows:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(as.factor(t2.pred), test$Survived)</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction  No Yes</span><br><span class="line">       No  <span class="number">368</span> <span class="number">105</span></span><br><span class="line">       Yes   <span class="number">4</span>  <span class="number">72</span></span><br><span class="line"></span><br><span class="line">               Accuracy : <span class="number">0.8015</span></span><br></pre></td></tr></table></figure>

<p>Plotting the tree with <code>rpart.plot(t2)</code>.
<img src="statistical-learning-Rplot2.png" alt="Rplot2.png"></p>
<p>Plotting the complexity parameter against the cross-validation relative error with <code>plotcp(t2)</code>. Although the relative error is at the lowest at $5$ nodes, comparable level of relative error was achieved at $2$ nodes. Because decision tree is prone to overfitting, here we manually prune the tree back to 2 nodes.</p>
<p><img src="statistical-learning-Rplot3.png" alt="Rplot3.png"></p>
<p>Plotting the tree with <code>rpart.plot(t2.prune)</code>.</p>
<p><img src="statistical-learning-Rplot4.png" alt="Rplot4.png"></p>
<p>The confusion matrix after pruning. We lose a small bit of out-of-sample accuracy due to manual pruning.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(as.factor(t2.prune.pred), test$Survived)</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction  No Yes</span><br><span class="line">       No  <span class="number">343</span>  <span class="number">83</span></span><br><span class="line">       Yes  <span class="number">29</span>  <span class="number">94</span></span><br><span class="line"></span><br><span class="line">               Accuracy : <span class="number">0.796</span></span><br></pre></td></tr></table></figure>

<p>Comparing the test error rate with naiveBayes, KNN and logistic regression.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">naiveBayes (cv10):           <span class="number">0.2095</span></span><br><span class="line">KNN (cv10, K=<span class="number">1</span>):             <span class="number">0.1942</span></span><br><span class="line">classification tree:         <span class="number">0.1985</span></span><br><span class="line">classification tree (prune): <span class="number">0.2040</span></span><br></pre></td></tr></table></figure>

<h3 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h3><p>The <code>confusion matrix</code> is a convenient summary of the model prediction. In our previous example with the un-pruned tree:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(as.factor(t2.pred), test$Survived)</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction     No    Yes</span><br><span class="line">       No     <span class="number">368</span>    <span class="number">105</span></span><br><span class="line">       Yes      <span class="number">4</span>     <span class="number">72</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mapping the number to definition</span></span><br><span class="line">          Reference</span><br><span class="line">Prediction     No    Yes</span><br><span class="line">       No      TN     FN</span><br><span class="line">       Yes     FP     TP</span><br></pre></td></tr></table></figure>

<p>There are four types of prediction:</p>
<ul>
<li>True Positive (TP): 72</li>
<li>True Negative (TN): 368</li>
<li>False Positive (FP), or Type I Error: 4</li>
<li>False Negative (FN), or Type II Error: 105</li>
</ul>
<p>Several metrics can be computed:</p>
<ul>
<li>Accuracy: (TP + TN) / N = (72 + 368) / 549 = 0.8015</li>
<li>Error Rate: (FP + FN) / N = 1 - Accuracy = 0.1985</li>
<li>Precision: TP / (Predicted Positive) = 72 / (72 + 4) = 0.9474</li>
<li>Sensitivity: TP / (Actually Positive) = 72 / (72 + 105) = 0.4068</li>
</ul>
<h3 id="Receiver-Operator-Characteristic-Curve"><a href="#Receiver-Operator-Characteristic-Curve" class="headerlink" title="Receiver Operator Characteristic Curve"></a>Receiver Operator Characteristic Curve</h3><p>The <code>ROC</code> curve can be used to evaluate the performacne of our model. The ROC curve plots the <code>TPR</code> (true positive rate) against <code>FPR</code> (false positive rate) over a range of <code>cutoff</code> values:</p>
<ul>
<li>TPR = TP / (Actually Positive) = 0.4068</li>
<li>FPR = FP / (Actually Negative) = 0.0107</li>
</ul>
<p>In python, we can create a ROC curve from our previous prediction TPR and FPR:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.0001</span>)</span><br><span class="line">plt.plot(x, x)</span><br><span class="line">plt.xlabel(<span class="string">'FPR'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'TPR'</span>)</span><br><span class="line"></span><br><span class="line">x2=<span class="number">0.0107</span></span><br><span class="line">y2=<span class="number">0.4680</span></span><br><span class="line">plt.scatter(x2, y2, s=<span class="number">20</span>, color=<span class="string">'red'</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, x2], [<span class="number">0</span>, y2], <span class="string">'r-'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">plt.plot([x2, <span class="number">1</span>], [y2, <span class="number">1</span>], <span class="string">'r-'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">plt.title(<span class="string">'ROC Plot'</span>)</span><br></pre></td></tr></table></figure>

<p><img src="statistical-learning-2-Unknown-1.png" alt="Unknown-1.png"></p>
<p>The baseline line refers to a model/cutoff where all observations in the <code>testing</code> set are predicted to be positive (point x=1, y=1), or negative (point x=0, y=0). The area under the red lines and above the x-axis is an <code>estimate of the model fit</code> and is called <code>AUC</code>, or area under the ROC curve.</p>
<p>An AUC of 1 means that the model has a TPR of 1 and FPR of 0.</p>
<h2 id="Ensemble-Methods"><a href="#Ensemble-Methods" class="headerlink" title="Ensemble Methods"></a>Ensemble Methods</h2><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>The decision tree method in general suffer from high model variance, compared to linear regression which shows low variance. <code>Bootstrap aggregation</code>, or <code>bagging</code> is a general-purpose procedure for <code>reducing variance</code> of a statistical model without affecting the bias. It is particular useful in the decision tree model context.</p>
<ul>
<li><p>For regression trees, construct $B$ regression trees using $B$ bootstrapped (repeatedly sampled) training sets. These trees grow deep and are not pruned, therefore having high variance. At the end, average the trees to reduce the variance.</p>
</li>
<li><p>For classification trees, construct $B$ classification trees. When predicting a test observation, take the majority classification resulted from the $B$ trees.</p>
</li>
</ul>
<p>Note that the bagging results are more accruate but less visual. We can obtain the variable importances by computing the total RSS/Gini decreases by splits over each predictors, hence providing better interpretations of the results.</p>
<h3 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h3><p>Random forest improves upon bagged trees by de-correlating the trees. At each split, only $m\approx\sqrt{p}$ predictors are considered, isolating effects on single feature with large influences.</p>
<p>In R, use the <code>randomForest</code> package:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(caret)</span><br><span class="line"><span class="keyword">library</span>(randomForest)</span><br><span class="line">set.seed(<span class="number">9999</span>)</span><br><span class="line">df &lt;- as.data.frame(Titanic)</span><br><span class="line">df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]</span><br><span class="line">df &lt;- subset(df, select = -c(Freq))</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a 75/25 train/test split</span></span><br><span class="line">partition &lt;- createDataPartition(df$Survived, list=<span class="literal">FALSE</span>, p=<span class="number">0.75</span>)</span><br><span class="line">train &lt;- df[partition, ]</span><br><span class="line">test &lt;- df[-partition, ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># fit random forest</span></span><br><span class="line">rf &lt;- randomForest(formula = Survived ~ .,</span><br><span class="line">                      data = train,</span><br><span class="line">                      ntree = <span class="number">100</span>,</span><br><span class="line">                      importance = <span class="literal">TRUE</span>)</span><br></pre></td></tr></table></figure>

<p>In-sample confusion matrix:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; rf</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line"> randomForest(formula = Survived ~ ., data = train, ntree = <span class="number">100</span>,      importance = <span class="literal">TRUE</span>)</span><br><span class="line">               Type of random forest: classification</span><br><span class="line">                     Number of trees: <span class="number">100</span></span><br><span class="line">No. of variables tried at each split: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        OOB estimate of  error rate: <span class="number">23.43</span>%</span><br><span class="line">Confusion matrix:</span><br><span class="line">      No Yes class.error</span><br><span class="line">No  <span class="number">1063</span>  <span class="number">55</span>  <span class="number">0.04919499</span></span><br><span class="line">Yes  <span class="number">332</span> <span class="number">202</span>  <span class="number">0.62172285</span></span><br></pre></td></tr></table></figure>

<p>Out-of-sample confusino matrix:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(as.factor(predict(rf, test)), test$Survived)</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction  No Yes</span><br><span class="line">       No  <span class="number">368</span> <span class="number">108</span></span><br><span class="line">       Yes   <span class="number">4</span>  <span class="number">69</span></span><br><span class="line"></span><br><span class="line">               Accuracy : <span class="number">0.796</span></span><br></pre></td></tr></table></figure>

<p>Comparing the test error rate with naiveBayes, KNN and logistic regression.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">naiveBayes (cv10):           <span class="number">0.2095</span></span><br><span class="line">KNN (cv10, K=<span class="number">1</span>):             <span class="number">0.1942</span></span><br><span class="line">classification tree:         <span class="number">0.1985</span></span><br><span class="line">classification tree (prune): <span class="number">0.2040</span></span><br><span class="line">random forest:               <span class="number">0.2040</span></span><br></pre></td></tr></table></figure>

<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p><code>Boosting</code> is also a general-purpose procedure for improving accuracy of the statistical model. In boosting, we repeatedly fit new trees to the residuals from the previous tree, and add the new trees to the main tree such that a <code>loss function</code> is minimized (subject to shrinkage parameter $\lambda$, typical 0.01, to control overfitting). CV is often used to determine the total number of trees to be fitted.</p>
<p>A <code>gradient boosting machine</code> is an algorithm that calculates the gradient of the loss function and update the paramters such that the model moves in the direction of the negative gradient, thus closer to a minimum point of the loss function.</p>
<ul>
<li><code>XGBoost</code> is an open-source software library that provides a <code>gradient boosting</code> framework for R. XGBoost initially started as a research project by <a href="https://homes.cs.washington.edu/~tqchen/" target="_blank" rel="noopener">Tianqi Chen</a> as part of the Distributed (Deep) Machine Learning Community (DMLC) group</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">llibrary(caret)</span><br><span class="line"><span class="keyword">library</span>(xgboost)</span><br><span class="line"><span class="keyword">library</span>(pROC)</span><br><span class="line">set.seed(<span class="number">9999</span>)</span><br><span class="line">df &lt;- as.data.frame(Titanic)</span><br><span class="line">df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]</span><br><span class="line">df &lt;- subset(df, select = -c(Freq))</span><br><span class="line"></span><br><span class="line"><span class="comment"># turn survival into 0 and 1</span></span><br><span class="line">Survived_Ind &lt;- rep(<span class="number">0</span>, nrow(df))</span><br><span class="line">Survived_Ind[df$Survived == <span class="string">"Yes"</span>] &lt;- <span class="number">1</span></span><br><span class="line">df$Survived_Ind &lt;- Survived_Ind</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># create a 75/25 train/test split</span></span><br><span class="line">partition &lt;- createDataPartition(df$Survived_Ind, list=<span class="literal">FALSE</span>, p=<span class="number">0.75</span>)</span><br><span class="line">train &lt;- df[partition, ]</span><br><span class="line">test &lt;- df[-partition, ]</span><br><span class="line">test_2 &lt;- test</span><br><span class="line">train &lt;- subset(train, select = -c(Survived))</span><br><span class="line">test &lt;- subset(test, select = -c(Survived))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># create model frame for xgboost input</span></span><br><span class="line">train.mf  &lt;- model.frame(as.formula(<span class="string">"Survived_Ind ~."</span>), data = train)</span><br><span class="line">test.mf  &lt;- model.frame(as.formula(<span class="string">"Survived_Ind ~."</span>), data = test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a model matrix only contains numerical values.</span></span><br><span class="line">train.mm  &lt;- model.matrix(attr(train.mf, <span class="string">"terms"</span>), data = train)</span><br><span class="line">test.mm  &lt;- model.matrix(attr(test.mf, <span class="string">"terms"</span>), data = test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [optional] create A XGB dense matrix contains an R matrix and metadata</span></span><br><span class="line">train.dm  &lt;- xgb.DMatrix(train.mm, label = train$Survived, missing = -<span class="number">1</span>)</span><br><span class="line">test.dm  &lt;- xgb.DMatrix(test.mm, label = test$Survived, missing = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># create xgboost parameter list</span></span><br><span class="line">params  &lt;-  list(<span class="string">"booster"</span> = <span class="string">"gbtree"</span>, <span class="comment"># "gblinear" for glm</span></span><br><span class="line">              <span class="string">"objective"</span> = <span class="string">"binary:logistic"</span>, <span class="comment"># the output here is a probability</span></span><br><span class="line">              <span class="string">"eval_metric"</span> = <span class="string">"auc"</span>,   </span><br><span class="line">              <span class="string">"eta"</span> = <span class="number">0.1</span>, <span class="comment"># lambda</span></span><br><span class="line">              <span class="string">"subsample"</span> = <span class="number">0.6</span>, <span class="comment"># proportion of observations</span></span><br><span class="line">              <span class="string">"colsample_bytree"</span> = <span class="number">0.6</span>, <span class="comment"># proportion of features</span></span><br><span class="line">              <span class="string">"max_depth"</span> = <span class="number">5</span>) <span class="comment"># depth of the decision tree</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train xgboost model</span></span><br><span class="line">model.cv &lt;- xgb.cv(params = params,</span><br><span class="line">                   data = train.dm,</span><br><span class="line">                   nrounds = <span class="number">1000</span>, <span class="comment"># the number of trees / iterations</span></span><br><span class="line">                   prediction = <span class="literal">FALSE</span>, <span class="comment"># storage of prediction under each tree</span></span><br><span class="line">                   print_every_n = <span class="number">25</span>,</span><br><span class="line">                   early_stopping_rounds = <span class="number">50</span>,</span><br><span class="line">                   maximize = <span class="literal">TRUE</span>, <span class="comment"># AUC metric -&gt; maximize</span></span><br><span class="line">                   nfold = <span class="number">6</span>) <span class="comment"># cv</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fit final model</span></span><br><span class="line">model &lt;- xgb.train(params = params,</span><br><span class="line">                   data = train.dm,</span><br><span class="line">                   nrounds = model.cv$best_iteration,</span><br><span class="line">                   prediction = <span class="literal">FALSE</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># format prediction</span></span><br><span class="line">xgb.prob &lt;- predict(model, test.dm)</span><br><span class="line">xgb.pred &lt;- rep(<span class="string">"No"</span>, sum(lengths(xgb.prob)))</span><br><span class="line">xgb.pred[xgb.prob &gt; <span class="number">0.5</span>] &lt;- <span class="string">"Yes"</span></span><br></pre></td></tr></table></figure>

<p>Feature importance:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; xgb.importance(feature_names = dimnames(train.dm)[[<span class="number">2</span>]], model = model)</span><br><span class="line">    Feature      Gain     Cover Frequency</span><br><span class="line"><span class="number">1</span>: Class3rd <span class="number">0.8347889</span> <span class="number">0.5957944</span>       <span class="number">0.5</span></span><br><span class="line"><span class="number">2</span>: Class2nd <span class="number">0.1652111</span> <span class="number">0.4042056</span>       <span class="number">0.5</span></span><br></pre></td></tr></table></figure>

<p>AUC result:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; auc(test$Survived_Ind, xgb.prob)</span><br><span class="line">Area under the curve: <span class="number">0.6033</span></span><br></pre></td></tr></table></figure>

<p>Confusion matrix:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; confusionMatrix(as.factor(xgb.pred), as.factor(test_2$Survived))</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">          Reference</span><br><span class="line">Prediction  No Yes</span><br><span class="line">       No  <span class="number">370</span> <span class="number">180</span></span><br><span class="line">       Yes   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line"></span><br><span class="line">               Accuracy : <span class="number">0.6727</span></span><br></pre></td></tr></table></figure>

<p>Unfortunately xgboost predict everything to be “No”</p>
<p>Comparing the test error rate with naiveBayes, KNN and logistic regression.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">naiveBayes (cv10):           <span class="number">0.2095</span></span><br><span class="line">KNN (cv10, K=<span class="number">1</span>):             <span class="number">0.1942</span></span><br><span class="line">classification tree:         <span class="number">0.1985</span></span><br><span class="line">classification tree (prune): <span class="number">0.2040</span></span><br><span class="line">random forest:               <span class="number">0.2040</span></span><br><span class="line">xgboost:                     <span class="number">0.3273</span></span><br></pre></td></tr></table></figure>

<h3 id="Ensemble-Model-Interpretation"><a href="#Ensemble-Model-Interpretation" class="headerlink" title="Ensemble Model Interpretation"></a>Ensemble Model Interpretation</h3><p><code>Feature Importance</code> ranks the contribution of each feature.
<code>Partial Dependence Plots</code> visualizes the model’s <code>average</code> dependence on a specific feature (or a pair of features).</p>
<h1 id="Unsupervised-Learning-8634"><a href="#Unsupervised-Learning-8634" class="headerlink" title="Unsupervised Learning &#8634;"></a><span id="03">Unsupervised Learning</span> <sup><a href="#nav">&#8634;</a></sup></h1><p>In <code>supervised learning</code>, we are provided a set of $n$ observations $X$, each containing $p$ features, and a response variable $Y$. We are interested at predicting $Y$ using the observations and features.</p>
<p>In <code>unsupervised learning</code>, we are interested at exploring hidden relationships within the data themself without involving any response variables. It is “unsupervised” in the sense that the learning outcome is subjective, unlike supervised learning in which specific metrics such as error rates are used to evaluate learning outcomes.</p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>See another <a href="https://jackliu234.com/2019/04/pca-deep-dive/">blog post</a> on PCA.</p>
<h2 id="K-Mean-Clustering"><a href="#K-Mean-Clustering" class="headerlink" title="K-Mean Clustering"></a>K-Mean Clustering</h2><p><code>Clustering</code> seek to partition data into homogeneous subgroups. The <code>K-Mean</code> clustering partitions data into $K$ distinct and non-overlapping clusters $C$, by minimizing the <code>objective function</code> of total in-cluster variation $W(C)$, which is the sum of all pair-wise squared Euclidean distances between the observations in the cluster, divided by the number of observations in the cluster.</p>
<p>$$ \text{minimize } {\ \sum_{k=1}^K W(C_k)\ } $$</p>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p>The algorithm divides data into $K$ <code>initial cluster</code>, and reassign observations to the cluster with the closest cluster <code>centroid</code> (mean of all previous observations in the cluster).</p>
<p>The K-Mean clustering algorithm finds a <code>local optimum</code>, and therefore depend on the initial cluster. So it is important to repeat the process with different initial points, and then select the best result based on minimum total in-cluster variation.</p>
<ul>
<li>The <code>nstart</code> parameter in the <code>kmean</code> function in R specifies the number of random starting centers to try and the one ended with the optimal objective function value will be selected.</li>
</ul>
<p>It is also important to check for <code>outliers</code>, as the algorithm would let the outlier become its own cluster and stops improving.</p>
<h3 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h3><p>It is a must to standardize the variables before performing k-mean cluster analysis, as the objective function (Euclidean distance, etc.) is calculated from the actual value of the variable.</p>
<h3 id="Curse-of-Dimensionality"><a href="#Curse-of-Dimensionality" class="headerlink" title="Curse of Dimensionality"></a>Curse of Dimensionality</h3><p>The <code>curse of dimensionality</code> describe the problems when performing clustering on three or more dimensional space, where:</p>
<ul>
<li>visualization becomes harder</li>
<li>as the number of dimensions increases, the Euclidean distance between data points are the same on average.</li>
</ul>
<p>The solution is to reduce the dimensionality before using clustering technique.</p>
<h3 id="The-Elbow-Method"><a href="#The-Elbow-Method" class="headerlink" title="The Elbow Method"></a>The Elbow Method</h3><p>Each cluster replaces its data with its center. In other words, with a clustering model we try to predict which cluster a data point belongs to.</p>
<p>A good model would explain more variance in the data with its cluster assignments. The <code>elbow</code> method looks at the $F$ statistics defined as:</p>
<p>$$ F = \dfrac{\text{between-group variance}}{\text{total variance}} $$</p>
<p>As soon as the additional F statistics drops/stops increasing when adding a new cluster, we use that number of clusters.</p>
<h2 id="Hierarchical-Clustering"><a href="#Hierarchical-Clustering" class="headerlink" title="Hierarchical Clustering"></a>Hierarchical Clustering</h2><p>The <code>hierarchical</code> clustering provide flexibilities in terms of the number of clusters $K$. It results in a tree-based representation of the data called <code>dendrogram</code>, which is built either <code>bottom-up/agglomerative</code> or <code>top-down/divisive</code>.</p>
<p>Hierarchical clustering assumes that there exists a hierarchical structure. In most feature generation cases, we prefer k-means clustering instead.</p>
<h3 id="Agglomerative"><a href="#Agglomerative" class="headerlink" title="Agglomerative"></a>Agglomerative</h3><p>An <code>agglomerative</code> hierarchical cluster starts off by assigning each data point in its own cluster. Each step in the clustering process two similar clusters with minimum <code>distance</code> among all are merged, where the distance is calculated between the elements within the cluster that are closest (<code>single-linkage</code>) or furthest (<code>complete-linkage</code>)</p>
<p><br><br><br></p>
<p>Reference:</p>
<ul>
<li>An Introduction to Statistical Learning with Applications in R, James, Witten, Hastie and Tibshirani</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2019/04/pca-deep-dive/" class="prev">PREV</a><a href="/2019/04/cpp/" class="next">NEXT</a></div><div id="container"></div><!-- link(rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css")--><link rel="stylesheet" href="/css/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
    clientID: '4ec287ddd4ac34ff5087',
    clientSecret: 'ae45426765f12ac3e2f903662b8938dc4881f703',
    repo: 'jackliu234.github.io',
    owner: 'jackliu234',
    admin: ['jackliu234'],
    perPage: 100,
    id: 'Sat Apr 13 2019 00:00:00 GMT-0500 GMT'.split('GMT')[0].replace(/\s/g, '-'),
    distractionFreeMode: false,
    pagerDirection: 'first'
})

gitalk.render('container')</script><!-- block copyright--></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-133275176-1",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>