<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>Understanding Gradient Descent in Neural Network · Rongjia Liu</title><meta name="description" content="Understanding Gradient Descent in Neural Network - Rongjia Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/arctic.css"><link rel="search" type="application/opensearchdescription+xml" href="http://jackliu234.com/atom.xml" title="Rongjia Liu"><script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/archive/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/search/" target="_self" class="nav-list-link">SEARCH</a></li><!-- li.nav-list-item--><!--    a.nav-list-link(class="search" href=url_for("search") target="_self") <i class="fa fa-search" aria-hidden="true"></i>--></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Understanding Gradient Descent in Neural Network</h1><div class="post-info">Aug 16, 2019</div><div class="post-content"><p><code>Gradient Descent</code> is an optimization method used in neural network, where the weight parameters <script type="math/tex">\boldsymbol{w}</script> are updated recursively by subtracting a small percentage <script type="math/tex">\alpha</script> of the gradient of the loss function <script type="math/tex">\nabla \mathcal{L}</script>, in order to minimize the loss function.</p>
<script type="math/tex; mode=display">\boldsymbol{w}_i = \boldsymbol{w}_{i-1} - \alpha\nabla \mathcal{L}(\boldsymbol{w}_{i-1})</script><a id="more"></a>
<h1 id="Mathematics"><a href="#Mathematics" class="headerlink" title="Mathematics"></a>Mathematics</h1><p>Let’s prove that the gradient descent method would lead to smaller losses after each step. Without loss of generality, we assume that the initial weight parameter <script type="math/tex">\boldsymbol{w}_0</script> is a two dimensional vector, <script type="math/tex">(x_0, y_0)</script>.</p>
<p>Given a new vector <script type="math/tex">(x, y)</script> that are close to <script type="math/tex">(x_0, y_0)</script>, the Taylor series expansion of the loss function <script type="math/tex">\mathcal{L}</script> can be approximated by the first-order partial derivatives only:</p>
<script type="math/tex; mode=display">\begin{align} \mathcal{L}(x, y) &= \mathcal{L}(x_0, y_0) + \dfrac{\partial\mathcal{L}(x_0, y_0)}{\partial x}(x - x_0) + \dfrac{\partial\mathcal{L}(x_0, y_0)}{\partial y}(y - y_0) \\
&= \mathcal{L}(x_0, y_0) + \nabla \mathcal{L}(x_0, y_0) \cdot (\Delta x, \Delta y) \end{align}</script><p>Since we would like to find <script type="math/tex">x</script> and <script type="math/tex">y</script> that minimize <script type="math/tex">\mathcal{L}(x, y)</script>, which is the same as minimizing the dot product above, with the constraint that <script type="math/tex">(x, y)</script> having close Eulidean distance to <script type="math/tex">(x_0, y_0)</script> in order to satisfy the Taylor approximation.</p>
<script type="math/tex; mode=display">|(\Delta x, \Delta y)| \leq \epsilon</script><p>To achieve this we select the vector <script type="math/tex">(x, y)</script> such that:</p>
<script type="math/tex; mode=display">(\Delta x, \Delta y) = - \alpha \nabla \mathcal{L}(x_0, y_0) \text{, or simply} \\
(x, y) = (x_0 - \alpha\cdot\dfrac{\partial\mathcal{L}(x_0, y_0)}{\partial x}, y_0 - \alpha\cdot\dfrac{\partial\mathcal{L}(x_0, y_0)}{\partial y})</script><p>Given an <script type="math/tex">\epsilon</script>, we choose <script type="math/tex">\alpha</script> such that the above constraint is satisfied. The negative sign ensure that the dot product is minimized. We therefore have proved that <script type="math/tex">\mathcal{L}(x, y)</script> will descend at each iteration provided that the loss function is differentiable and a sufficiently small <script type="math/tex">\alpha</script> is used.</p>
<h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><p>Let’s say we have a training set of 4 that maps three binary features to a binary response.  </p>
<script type="math/tex; mode=display">\begin{bmatrix}
    0 & 0 & 1 \\
    1 & 1 & 1 \\
    1 & 0 & 1 \\
    0 & 1 & 1
\end{bmatrix} \rightarrow \begin{bmatrix}
    0 \\
    1 \\
    1 \\
    0 \\
\end{bmatrix}</script><p>We first observe that the first feature has a 100% correlation with the response and can reasonably be used for future predictions. Now we construct a neural network to see if it can capture this relationship. First create a neural network class and randomly initialize three weights between <script type="math/tex">-1</script> and <script type="math/tex">1</script> for each feature.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        np.random.seed(<span class="number">1</span>)</span><br><span class="line">        self.weights = <span class="number">2</span> * np.random.random((<span class="number">3</span>, <span class="number">1</span>)) - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>Define the <code>sigmoid</code> activation function <script type="math/tex">\sigma</script>:</p>
<script type="math/tex; mode=display">\hat{y} = \sigma(wx) = \dfrac{1}{1 + e^{-wx}}</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure>
<p>Define the loss function as the mean square error:</p>
<script type="math/tex; mode=display">\mathcal{L}(w) = (\hat{y} - y)^2</script><p>Calculate the gradient w.r.t. weights <script type="math/tex">w</script>:</p>
<script type="math/tex; mode=display">\dfrac{\partial \mathcal{L}(w)}{\partial w} = 2(\hat{y} - y)\dfrac{\partial \hat{y}}{\partial w} = 2(\hat{y} - y) \ \hat{y}(1-\hat{y})\cdot x^T</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(self, x, y, y_hat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(x.T, (<span class="number">2</span> * (y_hat - y) * (y_hat * (<span class="number">1</span> - y_hat))))</span><br></pre></td></tr></table></figure>
<p>Forward and backward propogation.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propogation</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    x = inputs.astype(float)</span><br><span class="line">    <span class="keyword">return</span> self.sigmoid(np.dot(x, self.weights))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propogation</span><span class="params">(self, x, y, alpha=<span class="number">1</span>, iterations=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</span><br><span class="line">        y_hat = self.forward_propogation(x)</span><br><span class="line">        self.weights -= alpha * self.gradient(x, y, y_hat)</span><br></pre></td></tr></table></figure></p>
<p>Testing our initial hypothesis.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    nn = NeuralNetwork()</span><br><span class="line">    print(<span class="string">'\nrandom synoptic weights'</span>)</span><br><span class="line">    print(nn.weights)</span><br><span class="line"></span><br><span class="line">    x = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    y = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]]).T</span><br><span class="line">    nn.backward_propogation(x, y)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\nweights after training'</span>)</span><br><span class="line">    print(nn.weights)</span><br><span class="line"></span><br><span class="line">    outputs = nn.forward_propogation(x)</span><br><span class="line">    print(<span class="string">'\noutput after training'</span>)</span><br><span class="line">    print(outputs)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">random synoptic weights</span><br><span class="line">[[<span class="number">-0.16595599</span>]</span><br><span class="line"> [ <span class="number">0.44064899</span>]</span><br><span class="line"> [<span class="number">-0.99977125</span>]]</span><br><span class="line"></span><br><span class="line">weights after training</span><br><span class="line">[[<span class="number">10.38061249</span>]</span><br><span class="line"> [<span class="number">-0.20642264</span>]</span><br><span class="line"> [<span class="number">-4.98461681</span>]]</span><br><span class="line"></span><br><span class="line">output after training</span><br><span class="line">[[<span class="number">0.0067959</span> ]</span><br><span class="line"> [<span class="number">0.99445652</span>]</span><br><span class="line"> [<span class="number">0.99548577</span>]</span><br><span class="line"> [<span class="number">0.00553541</span>]]</span><br></pre></td></tr></table></figure>
<p>We can see that the neural network learns to put substantial weights on the first feature and makes very accurate predictions in-sample with <script type="math/tex">10,000</script> iterations.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/09/yosemite/" class="prev">PREV</a><a href="/2019/07/w2v/" class="next">NEXT</a></div><div id="container"></div><!-- link(rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css")--><link rel="stylesheet" href="/css/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
    clientID: '4ec287ddd4ac34ff5087',
    clientSecret: 'ae45426765f12ac3e2f903662b8938dc4881f703',
    repo: 'jackliu234.github.io',
    owner: 'jackliu234',
    admin: ['jackliu234'],
    perPage: 100,
    id: 'Fri Aug 16 2019 00:00:00 GMT-0400 GMT'.split('GMT')[0].replace(/\s/g, '-'),
    distractionFreeMode: false,
    pagerDirection: 'first'
})

gitalk.render('container')</script><!-- block copyright--></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-133275176-1",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>