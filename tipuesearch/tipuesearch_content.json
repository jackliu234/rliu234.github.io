{"pages":[{"title":"&#128214; Notes on LP","url":"/2019/12/soa-lp/","text":"Please enter the password: 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","tags":"notes"},{"title":"Bitcoin Trading Strategy (3): Perpetual Swap Funding","url":"/2019/12/algo-3/","text":"BackgroundIn this research, we dive into the bitcoin perpetual swap contract on the BitMEX exchange. Specifically, we are interested at the predicting power of its funding structure and the subsequent applications to algorithmic trading. Traditionally, future provides additional liquidity and leverage to market participants. With 10k USD, one may either buy 1 Bitcoin, or 100 Bitcoin futures which provide 100x more gain potentials. However, every future contract has an expiry date and can be traded at significantly spread. The first Bitcoin future in the U.S. was traded on Dec 10, 2017 on the Cboe Futures Exchange. The Bitcoin perpetual swap contract, on the contrast, does not have an expiry date thus removing the need to rollover. It trades much closer to the underlying Bitcoin price via a funding mechanism. On BitMEX, the swap holders must exchange fundings every 8 hours between the long and short counter-parties. This create price pressure for the swap price to converge to the actual Bitcoin price. For example, if swap price > Bitcoin price, then the funding would be positive and therefore the long positions will need to pay funding to its short counter-parties. This creates pressures for the swap price to decrease and move towards the Bitcoin price. StrategyThe funding creates a great monetary incentive if you are holding the contract on the right side and we would like to see if we can capture the funding gain overtime with an algorithmic trading strategy. Since the funding is announced 8 hours before the actual exchange happens, we have an 8 hour window of entry after knowing that a profitable funding will occur. After we enter the contract and collect the funding, we then have another 8 hour window for exiting (this assumes we only want to enter 1 contract at any given time). We will try to look for optimal enter/exit time combinations and evaluate performances. This is similar to the mean reversion strategy discussed by BitMEX’s founder Arthur Hayes in his blog[1][2]. We are carrying this strategy further, analyzing enter and exit options at more granular level and proposing a more optimal execution strategy. Dependency12345678910111213import pytzimport timeimport datetimeimport requestsimport numpy as npimport pandas as pdfrom random import randomimport statsmodels.api as smimport matplotlib.pyplot as pltfrom IPython.display import display, HTML, Imagefrom sklearn.linear_model import LinearRegressionfrom pandas.plotting import register_matplotlib_convertersregister_matplotlib_converters() 123456plt.rcParams['font.family'] = \"serif\"plt.rcParams['font.serif'] = \"DejaVu Serif\"plt.rcParams['figure.figsize'] = (12, 6)plt.rcParams['figure.dpi'] = 400plt.rcParams['lines.linewidth'] = 0.75pd.set_option('max_row', 6) 12def disp(df, max_rows=6): return display(HTML(df.to_html(max_rows=max_rows, header=True).replace('&lt;table border=\"1\" class=\"dataframe\"&gt;','&lt;table&gt;'))) DataWe can retrieve historical funding rates and minutely swap price data from the BitMEX api. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def get_funding(): start_date = datetime.datetime(2016, 5, 14, 0, 0, 0, 0, tzinfo=pytz.utc) end_date = datetime.datetime.now(tz=pytz.utc) - datetime.timedelta(days=14) endpoint = 'https://www.bitmex.com/api/v1/funding' payload = &#123;'count':'500', 'reverse':'false', 'symbol':'XBTUSD', 'startTime': start_date&#125; response = requests.get(endpoint, params=payload) funding_rates_df = pd.DataFrame(response.json()) funding_rates_df['timestamp'] = pd.to_datetime(funding_rates_df['timestamp'], utc=True) funding_rates_df.set_index('timestamp', drop=True, inplace=True) start_date = funding_rates_df.index[-1] + datetime.timedelta(hours=1) while start_date &lt; end_date: time.sleep(random()) # requesting too frequently will cause error endpoint = 'https://www.bitmex.com/api/v1/funding' payload = &#123;'count':'500', 'reverse':'false', 'symbol':'XBTUSD', 'startTime': start_date&#125; response = requests.get(endpoint, params=payload) funding_rates_df_tmp = pd.DataFrame(response.json()) funding_rates_df_tmp['timestamp'] = pd.to_datetime(funding_rates_df_tmp['timestamp'], utc=True) funding_rates_df_tmp.set_index('timestamp', drop=True, inplace=True) start_date = funding_rates_df_tmp.index[-1] + datetime.timedelta(hours=1) funding_rates_df = funding_rates_df.append([funding_rates_df_tmp]) funding_rates_df.to_csv('funding_rates_df.csv') return funding_rates_dfdef get_swap(): # start_date = datetime.datetime(2016, 5, 14, 0, 0, 0, 0, tzinfo=pytz.utc) # end_date = datetime.datetime.now(tz=pytz.utc) start_date = datetime.datetime(2019, 1, 1, 0, 0, 0, 0, tzinfo=pytz.utc) end_date = datetime.datetime(2019, 11, 14, 23, 59, 59, 0, tzinfo=pytz.utc) endpoint = 'https://www.bitmex.com/api/v1/trade/bucketed' payload = &#123;'count':'1000', 'reverse':'false', 'symbol':'XBTUSD', 'startTime': start_date, 'binSize': '1m'&#125; response = requests.get(endpoint, params=payload) print(response) swap_df = pd.DataFrame(response.json()) swap_df['timestamp'] = pd.to_datetime(swap_df['timestamp'], utc=True) swap_df.set_index('timestamp', drop=True, inplace=True) start_date = swap_df.index[-1] + datetime.timedelta(hours=1) while start_date &lt; end_date: try: time.sleep(random()) endpoint = 'https://www.bitmex.com/api/v1/trade/bucketed' payload = &#123;'count':'1000', 'reverse':'false', 'symbol':'XBTUSD', 'startTime': start_date, 'binSize': '1m'&#125; response = requests.get(endpoint, params=payload) swap_df_tmp = pd.DataFrame(response.json()) swap_df_tmp['timestamp'] = pd.to_datetime(swap_df_tmp['timestamp'], utc=True) swap_df_tmp.set_index('timestamp', drop=True, inplace=True) start_date = swap_df_tmp.index[-1] + datetime.timedelta(hours=1) swap_df = swap_df.append([swap_df_tmp]) print(start_date) except Exception as e: print(e) continue swap_df.to_csv(\"swap_df_1m_2019.csv\") return swap_df_tmp 123456789101112131415161718swap = pd.read_csv(\"swap_df_1m.csv\")swap['timestamp'] = pd.to_datetime(swap['timestamp'], utc=True)swap.set_index('timestamp', inplace=True)swap.fillna(method='ffill', inplace=True)swap.dropna(inplace=True)funding = pd.read_csv(\"funding_rates_df.csv\")funding['timestamp'] = pd.to_datetime(funding['timestamp'], utc=True)funding.set_index('timestamp', inplace=True)funding = funding['fundingRate'].to_frame()funding = funding.loc[funding.index &gt;= '2016-06-05']df = swap.join([funding])# swap return by holding from funding time -30m to +1mdf['swapRet'] = df.swapPrice.shift(-300) / df.swapPrice.shift(30) - 1df.dropna(inplace=True)disp(df) swapPrice fundingRate swapRet timestamp 2016-06-05 04:00:00+00:00 585.6001 0.000242 -0.002756 2016-06-05 12:00:00+00:00 581.3784 0.000237 -0.004034 2016-06-05 20:00:00+00:00 580.9900 0.000234 -0.000978 ... ... ... ... 2019-11-06 04:00:00+00:00 9314.4560 0.000100 0.013475 2019-11-06 12:00:00+00:00 9395.8470 0.000198 -0.008921 2019-11-06 20:00:00+00:00 9301.4603 0.000374 0.002900 1234567891011121314colors = np.where(df.fundingRate &gt;= 0, 'tab:green', 'tab:red')fig, ax1 = plt.subplots()ax1.plot(df.swapPrice, c='black', linewidth=0.3)ax1.set_ylabel('swapPrice')ax1.set_ylim(-22500, 22500)ax2 = ax1.twinx()ax2.scatter(df.index, df.fundingRate, c=colors, s=0.1)ax2.set_ylabel('fundingRate')ax2.set_ylim(-0.0075, 0.025)plt.title(\"Swap Price vs Funding Rate\")plt.show() AnalysisRegress the funding rates to the swap return at different enter/exit time. Here we are trying to look for statistically significant (ideally, negative) correlation between the two. Since a negative correlation would imply additional gain from price change on top of the funding profit. We only consider entering a contract if the funding is outside twice of its 60-day historical rolling standard deviations. 12345678910111213141516171819def filter_on_rolling_std(df, window, sigma_band, t1, t2, run_reg=False, show_summary=False, show_coef=True): df_sigma = df.copy() df_sigma['sigma'] = df_sigma.fundingRate.rolling(window).std() df_sigma = df_sigma.fillna(method = 'ffill').dropna() df_sigma = df_sigma.loc[(df_sigma.fundingRate &gt; sigma_band * df_sigma.sigma) | (df_sigma.fundingRate &lt; -sigma_band * df_sigma.sigma)] if run_reg: y = np.array(df_sigma['swapRet']) X = np.array(df_sigma[['fundingRate']]) X = sm.add_constant(X) model = sm.OLS(y,X).fit() coef = model.params[1].round(4) pval = model.pvalues[0].round(4) if show_summary: print(model.summary()) if show_coef: print('enter', t1, 'exit', t2, 'coef', coef, 'pval', pval) return df_sigma, coef, pval 1234567891011121314151617181920comp_exit = pd.DataFrame(columns=['exit time', 'coef', 'pval'])t1 = -100for i in np.arange(0, 481, 10): t2=i df = swap.join([funding]) df['swapRet'] = df.swapPrice.shift(-t2) / df.swapPrice.shift(-t1) - 1 df.dropna(inplace=True) df_sigma, coef, pval = filter_on_rolling_std(df, 180, 2, t1, t2, True, show_coef=False) comp_exit = comp_exit.append(&#123;'exit time': t2, 'coef': coef, 'pval': pval&#125;, ignore_index=True)comp_exit = comp_exit.set_index('exit time')plt.plot(comp_exit)plt.axhline(y=0.05, color='grey', linestyle='dashed')plt.legend(comp_exit.columns, frameon=False)plt.xlabel('Exit Time')plt.xticks(np.arange(0, 481, step=60))plt.show() Here we observe that as exit time becomes longer, the coefficient becomes more negative and p-value of the coefficient indicates higher significance. Thus we would want to hold the swap position more than 360 minutes/6 hours. Next we look at the impact from entry time. 123456789101112131415161718192021222324comp_enter = pd.DataFrame(columns=['enter time', 'coef', 'pval'])t1 = -60t2 = 420for i in np.arange(-120, 0, 1): t1=i df = swap.join([funding]) df['swapRet'] = df.swapPrice.shift(-t2) / df.swapPrice.shift(-t1) - 1 df.dropna(inplace=True) if t1 == -60: df_sigma, coef, pval = filter_on_rolling_std(df, 180, 2, t1, t2, True, show_summary=True, show_coef=False) else: df_sigma, coef, pval = filter_on_rolling_std(df, 180, 2, t1, t2, True, show_coef=False) comp_enter = comp_enter.append(&#123;'enter time': t1, 'coef': coef, 'pval': pval&#125;, ignore_index=True)comp_enter = comp_enter.set_index('enter time')plt.plot(comp_enter)plt.axhline(y=0.05, color='grey', linestyle='dashed')plt.legend(comp_enter.columns, frameon=False)plt.xlabel('Enter Time')plt.xticks(np.arange(-120, 0, step=30))plt.show() Similar trends are observed in the entry times and that earlier the entry, the more profit it seems to imply from price changes. We show a summary of the regression at enter time=-60 minutes and exit time=420 minutes. There is a moderate R-square of 0.039 and high significance in coefficient which suggest a mean reversion in price given that specific time window. 12345678910111213141516171819202122232425 OLS Regression Results ==============================================================================Dep. Variable: y R-squared: 0.039Model: OLS Adj. R-squared: 0.036Method: Least Squares F-statistic: 10.46Date: Thu, 05 Dec 2019 Prob (F-statistic): 0.00138Time: 03:38:11 Log-Likelihood: 552.01No. Observations: 257 AIC: -1100.Df Residuals: 255 BIC: -1093.Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975]------------------------------------------------------------------------------const 0.0035 0.002 1.809 0.072 -0.000 0.007x1 -2.2357 0.691 -3.235 0.001 -3.597 -0.875==============================================================================Omnibus: 17.788 Durbin-Watson: 2.247Prob(Omnibus): 0.000 Jarque-Bera (JB): 54.546Skew: 0.089 Prob(JB): 1.43e-12Kurtosis: 5.250 Cond. No. 391.==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. StrategyBased on the research above, this strategy will enter into a swap agreement to collect funding at t1= -1H and exit at t2= +7H. At t2, if the next funding does not fall outside of the 2-sigma band AND if we are in a position to collect the next funding, we will test two choices of holding for 1H more and exit after colllecting the funding exiting We will test the impact of a 10bps fee + slippage on each trade. 12345678910111213141516171819202122232425262728293031t1 = -60t2 = 420window = 180sigma_band = 2fee = 0.0010 # per two tradesdf = swap.join([funding])df['swapRet'] = df.swapPrice.shift(-t2) / df.swapPrice.shift(-t1) - 1df['swapRet1H'] = df.swapPrice.shift(1) / df.swapPrice.shift(-t1) - 1df.dropna(inplace=True)df['sigma'] = df.fundingRate.rolling(window).std()df.dropna(inplace=True)df['pnl'] = np.where((df.fundingRate &gt; sigma_band*df.sigma) \\ | (df.fundingRate &lt; -sigma_band*df.sigma), \\ 1 + np.abs(df.fundingRate) + \\ df.swapRet * -np.sign(df.fundingRate), 1)df['pnlOptimized'] = np.where((df.pnl != 1) &amp; (df.pnl.shift(-1) == 1) \\ &amp; (np.sign(df.fundingRate) == np.sign(df.fundingRate.shift(-1))), df.pnl + np.abs(df.fundingRate.shift(-1)) + \\ df.swapRet1H.shift(-1) * -np.sign(df.fundingRate.shift(-1)), df.pnl)df['pnlFee'] = np.where((df.pnl != 1) &amp; (df.pnl.shift(1) == 1), df.pnl - fee, df.pnl) # enter feedf['pnlFee'] = np.where((df.pnl != 1) &amp; (df.pnl.shift(-1) == 1), df.pnlFee - fee, df.pnlFee) # exit feedf['pnlFee'] = np.where((df.pnl != 1) &amp; (df.pnl.shift(1) != 1) \\ &amp; (np.sign(df.fundingRate) != np.sign(df.fundingRate.shift(1))), df.pnlFee - fee, df.pnlFee) # change position enter feedf['pnlFee'] = np.where((df.pnl != 1) &amp; (df.pnl.shift(-1) != 1) \\ &amp; (np.sign(df.fundingRate) != np.sign(df.fundingRate.shift(-1))), df.pnlFee - fee, df.pnlFee) # change position exit feedisp(df.iloc[289:293]) swapPrice fundingRate swapRet swapRet1H sigma pnl pnlOptimized pnlFee timestamp 2016-11-16 20:00:00+00:00 748.1800 0.000264 0.008467 0.014128 0.001042 1.000000 1.000000 1.000000 2016-11-17 04:00:00+00:00 744.8734 0.003750 0.008865 0.001431 0.001066 0.994885 0.994885 0.993885 2016-11-17 20:00:00+00:00 742.2400 0.003140 -0.007095 -0.001269 0.001081 1.010235 1.008677 1.009235 2016-11-18 04:00:00+00:00 740.0665 0.001242 0.012663 0.002800 0.001080 1.000000 1.000000 1.000000 12345plt.plot(df.pnl.cumprod(), c='black')plt.plot(df.pnlOptimized.cumprod(), c='tab:blue')plt.plot(df.pnlFee.cumprod(), c='grey')plt.legend(['PNL', 'PNL optimized', 'PNL with fee (not optimized)'])plt.show() 1234567def backtest_metric(df, pnl_column, mdd_interval=180): pnl = round(df[pnl_column].cumprod()[-1], 4) spr = round(np.mean(df[pnl_column]-1) / np.std(df[pnl_column]-1) * np.sqrt(365 * 3), 4) mdd = round(np.min((df[pnl_column].cumprod().rolling(mdd_interval).min() \\ - df[pnl_column].cumprod().shift(mdd_interval)) \\ / df[pnl_column].cumprod().shift(mdd_interval)), 4) return pnl, spr, mdd 12345678910result = pd.DataFrame(columns=['Strategy', 'P&amp;L', 'Sharpe Ratio', 'Maximum Drawdown'])pnl, spr, mdd = backtest_metric(df, 'pnl')result = result.append(&#123;'Strategy': 'Baseline', 'P&amp;L': pnl, 'Sharpe Ratio': spr, 'Maximum Drawdown': mdd&#125;, ignore_index=True)pnl, spr, mdd = backtest_metric(df, 'pnlOptimized')result = result.append(&#123;'Strategy': 'Optimized', 'P&amp;L': pnl, 'Sharpe Ratio': spr, 'Maximum Drawdown': mdd&#125;, ignore_index=True)disp(result) Strategy P&amp;L Sharpe Ratio Maximum Drawdown 0 Baseline 5.5625 2.2086 -0.1438 1 Optimized 6.7478 2.3101 -0.1221 We can see that this strategy does provide substantial P&amp;L from the historical periods tested with relatively limited capital exposure. Fees would impact the gains slightly, and using an optimize approach would further improve the performance. Reference: [1]: XBTUSD Funding Mean Reversion Strategy, https://blog.bitmex.com/xbtusd-funding-mean-reversion-strategy/[2]: Funding Mean Reversions 2018, https://blog.bitmex.com/funding-mean-reversions-2018/","tags":"notes"},{"title":"Bitcoin Trading Strategy (2): Momentum","url":"/2019/07/algo-2/","text":"In this research I studied on the performance of simple and exponential moving average crossover strategies, with window sizes chosen by optimizing in-sample PNL, sharpe ratio and 30-day maximum drawdown. The calibrated strategy performs well, earning 500% cumulative return compared to baseline and a sharpe ratio of 1.30. The the 30-day maximum drawdown is similar to the baseline. Strategy P&amp;L Sharpe Ratio Maximum Drawdown 0 Baseline 0.28 -1.48 0.35 1 MA 1.54 1.30 0.37 2 EWMA 1.45 1.10 0.38 MotivationIt is no secret that price manipulations have always plagued the rising crypto-market. In this [paper], the auther studies large transactions behind the tether coin, and showed more evidence supporting that each large move in the crypto-market usually only come from the act of only a few. In this type of regime, I argue that technical indicator may be a better bet to profit compared to any attempt to apply fundamental analysis, because an increase in price no longer comes from the increase in a crypto’s intrinsic value, but rather speculation and manipulation. In this exercise I will mainly focus on moving average crossover techniques and its optimization. Packages123456789101112131415161718import itertoolsfrom IPython.display import display, HTML, Imageimport matplotlib.pyplot as pltimport matplotlib.ticker as tickerimport numpy as npimport pandas as pdfrom pandas.plotting import register_matplotlib_convertersimport warningsregister_matplotlib_converters()warnings.filterwarnings(\"ignore\")plt.rcParams['font.family'] = \"serif\"plt.rcParams['font.serif'] = \"DejaVu Serif\"plt.rcParams['figure.figsize'] = (12, 6)plt.rcParams['figure.dpi'] = 150plt.rcParams['lines.linewidth'] = 0.75pd.set_option('max_row', 10) Function12def disp(df): return display(HTML(df.to_html(max_rows=10, header=True).replace('&lt;table border=\"1\" class=\"dataframe\"&gt;','&lt;table&gt;'))) Data ExplorationI got the preliminary bitcoin data from bitcoincharts. Data include price and volume information recorded by Bitstamp and split by seconds. This provide great granularity that can be grouped into any desirable levels later on. 123data = pd.read_csv('bitstampUSD.csv', header=None, names=['time', 'price', 'volume'])data['time'] = pd.to_datetime(data['time'], unit='s')data.set_index('time', inplace=True) Get 3-month treasury data. 12url = 'https://fred.stlouisfed.org/graph/fredgraph.csv?id=DTB3'tr = pd.read_csv(url, index_col=0, parse_dates=True) Data are grouped in to daily, with average applied to price and sum applied to trade volume. The backtest period is selected to be from 2018 to 2019, where the market was in continuous downturn. This ensure that our strategy performs well in adverse scenarios. 12345df1 = data.loc['2018-01-01':'2019-01-01'].resample('1D').agg(&#123;'price': np.mean, 'volume': np.sum&#125;)df2 = tr.loc['2018-01-01':'2019-01-01']df = df1.join(df2).replace('.', np.NaN).fillna(method='ffill').fillna(method='bfill').rename(&#123;'DTB3': 'tr'&#125;, axis=1)df.tr = df.tr.astype(float)/100disp(df) price volume tr time 2018-01-01 13386.429268 7688.030685 0.0142 2018-01-02 14042.643870 16299.669303 0.0142 2018-01-03 14947.898046 12275.001197 0.0139 2018-01-04 14802.363927 15004.018593 0.0139 2018-01-05 15967.972719 16248.914680 0.0137 ... ... ... ... 2018-12-28 3752.739978 13055.718407 0.0235 2018-12-29 3862.153295 6901.382332 0.0235 2018-12-30 3783.210991 5736.453708 0.0235 2018-12-31 3745.258717 6667.163737 0.0240 2019-01-01 3709.889253 5149.606277 0.0240 123plt.plot(df.price, c='tab:grey')plt.ylabel('Bitcoin Price in USD')plt.show() Simple Moving AverageA simple moving average strategy use the cross-over point of two moving averages as the trading signal. Here we use grid-search to find out the window size pair that optimizes our desired metrics, namely P&amp;L, Sharpe ratio and 30-day maximum drawdown. 123456789101112131415161718192021222324252627282930313233def moving_average(df0, ma1, ma2, transactionFee=0, runBaseline=False, returnStats=True, ewma=False): df = df0.copy() if ewma: df['ma'+str(ma1)] = df.price.ewm(span=ma1).mean() df['ma'+str(ma2)] = df.price.ewm(span=ma2).mean() else: df['ma'+str(ma1)] = df.price.rolling(ma1).mean() df['ma'+str(ma2)] = df.price.rolling(ma2).mean() df['ind'] = df['ma'+str(ma1)] &gt; df['ma'+str(ma2)] df.dropna(inplace=True) df['buy'] = (df.ind != df.ind.shift(1)) &amp; df.ind &amp; (df.index != df.index[0]) df['sell'] = (df.ind != df.ind.shift(1)) &amp; df.ind.shift(1) &amp; (df.buy.cumsum() &gt; 0) if runBaseline: df.ind = 1 df.buy = 1 df['pnl'] = df.ind * (df.buy.cumsum() &gt; 0) * df.price.shift(-1) / df.price df.pnl = df.pnl * np.where(df.ind != df.ind.shift(1), 1-transactionFee, 1) df.dropna(inplace=True) df.pnl.replace(0, 1, inplace=True) if returnStats: df['tr_daily'] = (1 + df.tr)**(1/365) - 1 pnl = round(df.pnl.cumprod()[-1], 2) sharpe_ratio = round(np.mean(df.pnl-1-df.tr_daily) / np.std(df.pnl-1) * np.sqrt(365), 2) mdd_dur = 30 max_draw_down = round(np.max(df.pnl.cumprod().rolling(mdd_dur).max() - df.pnl.cumprod().shift(mdd_dur)), 2) return pnl, sharpe_ratio, max_draw_down else: return df First let’s compute the baseline results, from a simple buy and hold strategy. 123pnl, spr, mdd = moving_average(df, 1, 1, runBaseline=True)comp = pd.DataFrame(&#123;'Strategy': 'Baseline', 'P&amp;L': pnl, 'Sharpe Ratio': spr, 'Maximum Drawdown': mdd&#125;, index=[0])disp(comp) Strategy P&amp;L Sharpe Ratio Maximum Drawdown 0 Baseline 0.28 -1.48 0.35 Performing grid-search for the optimal window size pair. Note that 25bps of transaction fee is added, this is to reflect the typical fee charged by crypto exchanges. I used coinbase pro’s fee here as an example. 12345678910111213fee = 0.0025test_range = np.arange(1, 61)result_ma = pd.DataFrame(columns=['Strategy','MA1', 'MA2', 'P&amp;L', 'Sharpe Ratio', 'Maximum Drawdown'])# grid-searchfor ma1 in test_range: for ma2 in test_range: if ma2 &gt; ma1 + 3: pnl, spr, mdd = moving_average(df, ma1, ma2, transactionFee=fee) result_ma = result_ma.append(&#123;'Strategy': 'MA', 'MA1': ma1, 'MA2': ma2, 'P&amp;L': pnl, 'Sharpe Ratio': spr, 'Maximum Drawdown': mdd&#125;, ignore_index=True) 1disp(result_ma.sort_values('P&amp;L', ascending=False).head()) Strategy MA1 MA2 P&amp;L Sharpe Ratio Maximum Drawdown 2 MA 1 7 1.54 1.30 0.37 3 MA 1 8 1.31 0.86 0.34 12 MA 1 17 1.26 0.81 0.33 11 MA 1 16 1.26 0.81 0.32 9 MA 1 14 1.25 0.78 0.32 1disp(result_ma.sort_values('Sharpe Ratio', ascending=False).head()) Strategy MA1 MA2 P&amp;L Sharpe Ratio Maximum Drawdown 2 MA 1 7 1.54 1.30 0.37 3 MA 1 8 1.31 0.86 0.34 11 MA 1 16 1.26 0.81 0.32 12 MA 1 17 1.26 0.81 0.33 9 MA 1 14 1.25 0.78 0.32 1disp(result_ma.sort_values('Maximum Drawdown', ascending=True).head()) Strategy MA1 MA2 P&amp;L Sharpe Ratio Maximum Drawdown 1129 MA 26 59 0.51 -3.38 0.04 1099 MA 25 60 0.54 -3.19 0.04 1130 MA 26 60 0.52 -3.29 0.04 1160 MA 27 60 0.52 -3.35 0.04 1128 MA 26 58 0.54 -3.01 0.06 Choosing 1-7 as our selected window pair. Plotting the PNL over the 1-year backtest period. 1234567891011bt = df.copy()bt['Baseline: Buy and Hold'] = bt.price/bt.price[0]bt['Strategy 1: MA 1-7'] = moving_average(df.copy(), 1, 7, returnStats=False).pnl.cumprod()bt['Strategy 2: MA 1-7 with Fee'] = moving_average(df.copy(), 1, 7, transactionFee=fee, returnStats=False).pnl.cumprod()plt.plot(bt.iloc[:, 3], c='tab:grey')plt.plot(bt.iloc[:, 4], c='tab:red')plt.plot(bt.iloc[:, 5], c='tab:red', alpha=0.5)plt.legend(bt.columns[3:6], frameon=False)plt.ylabel('Cumulative Asset Value Based on $1 Investment')plt.show() It seems that the trading fee does not have a material impact on the result. We plot the buy/sell signals as follow. 12345678bt = df.copy()ma = moving_average(bt, 1, 7, transactionFee=fee, returnStats=False)plt.plot(bt.price, c='black', label='Bitcoin Price')plt.plot(ma.price.loc[ma.buy], '^', markersize=3, color='g', label='Buy Signal')plt.plot(ma.price.loc[ma.sell], 'v', markersize=3, color='r', label='Sell Signal')plt.legend()plt.show() EWMAPerform the same grid-search optimization using EWMA (Exponentially Weighted Moving Averages). 123456789101112test_range = np.arange(1, 61)result_ewma = pd.DataFrame(columns=['Strategy','MA1', 'MA2', 'P&amp;L', 'Sharpe Ratio', 'Maximum Drawdown'])# grid-searchfor ma1 in test_range: for ma2 in test_range: if ma2 &gt; ma1 + 3: pnl, spr, mdd = moving_average(df, ma1, ma2, transactionFee=fee, ewma=True) result_ewma = result_ewma.append(&#123;'Strategy': 'EWMA', 'MA1': ma1, 'MA2': ma2, 'P&amp;L': pnl, 'Sharpe Ratio': spr, 'Maximum Drawdown': mdd&#125;, ignore_index=True) 1disp(result_ewma.sort_values('P&amp;L', ascending=False).head()) Strategy MA1 MA2 P&amp;L Sharpe Ratio Maximum Drawdown 0 EWMA 1 5 1.45 1.10 0.38 1 EWMA 1 6 1.39 0.98 0.39 5 EWMA 1 10 1.38 1.01 0.32 10 EWMA 1 15 1.36 0.99 0.38 6 EWMA 1 11 1.31 0.87 0.31 1disp(result_ewma.sort_values('Sharpe Ratio', ascending=False).head()) Strategy MA1 MA2 P&amp;L Sharpe Ratio Maximum Drawdown 0 EWMA 1 5 1.45 1.10 0.38 5 EWMA 1 10 1.38 1.01 0.32 10 EWMA 1 15 1.36 0.99 0.38 1 EWMA 1 6 1.39 0.98 0.39 12 EWMA 1 17 1.31 0.89 0.38 1disp(result_ewma.sort_values('Maximum Drawdown', ascending=True).head()) Strategy MA1 MA2 P&amp;L Sharpe Ratio Maximum Drawdown 797 EWMA 17 42 0.51 -2.34 0.18 1069 EWMA 25 30 0.53 -2.22 0.18 1068 EWMA 25 29 0.51 -2.29 0.18 1067 EWMA 24 60 0.67 -1.92 0.18 1066 EWMA 24 59 0.67 -1.92 0.18 Selecting 1-7 as our window pair. Plotting the cumulative strategy return and buy/sell signals. 1234567891011bt = df.copy()bt['Baseline: Buy and Hold'] = bt.price/bt.price[0]bt['Strategy 1: EMWA 1-5 (Best PNL)'] = moving_average(df.copy(), 1, 5, returnStats=False, ewma=True).pnl.cumprod()bt['Strategy 2: EMWA 1-5 (Best PNL) with Fee'] = moving_average(df.copy(), 1, 5, transactionFee=fee, returnStats=False, ewma=True).pnl.cumprod()plt.plot(bt.iloc[:, 3], c='tab:grey')plt.plot(bt.iloc[:, 4], c='tab:blue')plt.plot(bt.iloc[:, 5], c='tab:blue', alpha=0.5)plt.legend(bt.columns[3:6], frameon=False)plt.ylabel('Cumulative Asset Value Based on $1 Investment')plt.show() 12345678bt = df.copy()ma = moving_average(bt, 1, 5, transactionFee=fee, returnStats=False, ewma=True).copy()plt.plot(bt.price, c='black', label='Bitcoin Price')plt.plot(ma.price.loc[ma.buy], '^', markersize=3, color='g', label='Buy Signal')plt.plot(ma.price.loc[ma.sell], 'v', markersize=3, color='r', label='Sell Signal')plt.legend()plt.show() Comparing the MA and EWMA strategies. 1234567891011bt = df.copy()bt['Baseline: Buy and Hold'] = bt.price/bt.price[0]bt['Strategy 1: Moving Average 1-7'] = moving_average(df.copy(), 1, 7, transactionFee=fee, returnStats=False).pnl.cumprod()bt['Strategy 2: EWMA 1-5'] = moving_average(df.copy(), 1, 5, transactionFee=fee, returnStats=False, ewma=True).pnl.cumprod()plt.plot(bt.iloc[:, 3], c='tab:grey')plt.plot(bt.iloc[:, 4], c='tab:red')plt.plot(bt.iloc[:, 5], c='tab:blue')plt.legend(bt.columns[3:6], frameon=False)plt.ylabel('Cumulative Asset Value Based on $1 Investment')plt.show() As we can see, the MA strategy slightly outperforms the EWMA strategy in all three metrics. 123comp = comp.append(result_ma.iloc[2, [0, 3, 4, 5]], ignore_index=True)comp = comp.append(result_ewma.iloc[0, [0, 3, 4, 5]], ignore_index=True)disp(comp) Strategy P&amp;L Sharpe Ratio Maximum Drawdown 0 Baseline 0.28 -1.48 0.35 1 MA 1.54 1.30 0.37 2 EWMA 1.45 1.10 0.38 ImplementationStarting 08-01-2019, I have implemented the optimal MA strategy on a VPS (virtual private server), running 24/7 through the coinbase pro api. Will post update on this periodically.","tags":"tech"},{"title":"Bitcoin Trading Strategy (1): Classification Approach","url":"/2019/07/algo-1/","text":"In this research I looked at intraday Bitcoin trading based on price and volume information using classification models. Strategy Precision P&amp;L Sharpe Ratio 10 MLP Classifier 0.55 2.06 1.79 3 KNN 0.50 1.95 1.85 0 Baseline 0.00 1.69 1.14 4 Decision Tree 0.49 1.61 1.84 5 Random Forest 0.53 1.55 1.16 8 XGBoost 0.52 1.33 0.86 9 SVC 0.47 1.31 0.73 1 Logistic Regression 0.47 1.14 0.47 7 Gradient Boost 0.48 1.06 0.33 6 AdaBoost 0.50 0.86 -0.09 2 Linear Discriminant Analysis 0.47 0.85 -0.17 Packages1234567891011121314151617181920212223242526import warningsimport itertoolsimport numpy as npimport pandas as pdfrom sklearn import treefrom sklearn.svm import SVCimport matplotlib.pyplot as pltfrom xgboost import XGBClassifierimport matplotlib.ticker as tickerfrom sklearn.decomposition import PCAfrom imblearn.over_sampling import SMOTEfrom sklearn.metrics import confusion_matrixfrom multiprocessing import set_start_methodfrom sklearn.tree import DecisionTreeClassifierfrom IPython.display import display, HTML, Imagefrom sklearn.neural_network import MLPClassifierfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import KFold, GridSearchCVfrom pandas.plotting import register_matplotlib_convertersfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifierregister_matplotlib_converters()warnings.filterwarnings(\"ignore\") 123456plt.rcParams['font.family'] = \"serif\"plt.rcParams['font.serif'] = \"DejaVu Serif\"plt.rcParams['figure.figsize'] = (12, 6)plt.rcParams['figure.dpi'] = 100plt.rcParams['lines.linewidth'] = 0.75pd.set_option('max_row', 10) FunctionThis is a customized function used to plot confusion matrix in python. 12def disp(df, max_rows=10): return display(HTML(df.to_html(max_rows=max_rows, header=True).replace('&lt;table border=\"1\" class=\"dataframe\"&gt;','&lt;table&gt;'))) Data ExplorationI got the preliminary bitcoin data from bitcoincharts. Data include price and volume information recorded by Bitstamp and split by seconds. This provide great granularity that can be grouped into any desirable levels later on. 123data = pd.read_csv('bitstampUSD.csv', header=None, names=['time', 'price', 'volume'])data['time'] = pd.to_datetime(data['time'], unit='s')data.set_index('time', inplace=True) Get 3-month treasury bill price. 12url = 'https://fred.stlouisfed.org/graph/fredgraph.csv?id=DTB3'tr = pd.read_csv(url, index_col=0, parse_dates=True) We first resample our data by hour. Since most Bitcoin exchanges nowadays have transaction fees, which renders retail trading at a high frequency level unattainable. Therefore I leave out the second and minute level data and combine them into hours. Note that I average the price while summing the volume within an hour. A 2 year data window from 2017 to 2019 is used, as this is when Bitcoin and other crypto has come into the attention of the larger public, and mostly importantly, started to be heavily traded. Therefore the training set will be more representative of any future trading environment. The plot below illustrates the total dollar amount traded per hours over time. 1234df0 = data.resample('H').agg(&#123;'price': np.mean, 'volume': np.sum&#125;).fillna(method='ffill')plt.plot(df0.volume * df0.price, c='black')plt.title('Bitcoin Dollar Volume in Dollar Term')plt.show() 123456df1 = data.loc['2017-07-01':'2019-06-30'].resample('H').agg(&#123;'price': np.mean, 'volume': np.sum&#125;).fillna(method='ffill')df2 = tr.loc['2017-07-01':'2019-06-30']df = df1.join(df2).replace('.', np.NaN).fillna(method='ffill').fillna(method='bfill').rename(&#123;'DTB3': 'tr'&#125;, axis=1)df.tr = df.tr.astype(float)/100disp(df) price volume tr time 2017-07-01 00:00:00 2473.427264 200.793669 0.0104 2017-07-01 01:00:00 2463.946180 228.853771 0.0104 2017-07-01 02:00:00 2441.314976 475.068038 0.0104 2017-07-01 03:00:00 2449.063866 177.876034 0.0104 2017-07-01 04:00:00 2453.192311 120.916328 0.0104 ... ... ... ... 2019-06-30 19:00:00 11173.875377 389.958860 0.0208 2019-06-30 20:00:00 11276.492157 372.471619 0.0208 2019-06-30 21:00:00 11340.807808 295.522323 0.0208 2019-06-30 22:00:00 11037.539360 963.543871 0.0208 2019-06-30 23:00:00 10838.165248 1152.810243 0.0208 123plt.plot(df.price, c='black')plt.title('Bitcoin Price 2017-07-01 to 2019-06-30')plt.show() We then created several more data fields intending to extract more information from the previous n-hour window 1234567891011interval = [6, 12, 24, 48, 120] # 0.25, 0.5, 1, 2, 5 daysfor i in interval: for c in ['price', 'volume']: df[c+'_change_'+str(i)+'H'] = df[c]/df[c].shift(i)-1 df[c+'_high_'+str(i)+'H'] = df[c].rolling(i).max().shift(1) / df[c] df[c+'_low_'+str(i)+'H'] = df[c].rolling(i).min().shift(1) / df[c] df[c+'_avg_'+str(i)+'H'] = df[c].rolling(i).mean().shift(1) / df[c] df[c+'_std_'+str(i)+'H'] = df[c].rolling(i).std().shift(1) / df[c] * np.sqrt(24/i)df.dropna(inplace=True)disp(df.head()) price volume tr price_change_6H price_high_6H price_low_6H price_avg_6H price_std_6H volume_change_6H volume_high_6H volume_low_6H volume_avg_6H volume_std_6H price_change_12H price_high_12H price_low_12H price_avg_12H price_std_12H volume_change_12H volume_high_12H volume_low_12H volume_avg_12H volume_std_12H price_change_24H price_high_24H price_low_24H price_avg_24H price_std_24H volume_change_24H volume_high_24H volume_low_24H volume_avg_24H volume_std_24H price_change_48H price_high_48H price_low_48H price_avg_48H price_std_48H volume_change_48H volume_high_48H volume_low_48H volume_avg_48H volume_std_48H price_change_120H price_high_120H price_low_120H price_avg_120H price_std_120H volume_change_120H volume_high_120H volume_low_120H volume_avg_120H volume_std_120H time 2017-07-06 00:00:00 2607.823311 233.619901 0.0102 0.005149 1.001294 0.994877 0.998116 0.005057 -0.539692 2.172459 1.271991 1.710225 0.606196 0.019447 1.001294 0.980924 0.991722 0.010182 -0.137448 2.604881 0.986408 1.724699 0.659083 0.012479 1.001294 0.973805 0.985347 0.008630 -0.708220 3.444264 0.815879 1.940980 0.748083 0.019561 1.008966 0.973805 0.990540 0.006865 0.552344 4.357035 0.644187 1.832178 0.620839 0.054336 1.008966 0.916032 0.966001 0.011353 0.163482 8.713623 0.498690 1.772706 0.491582 2017-07-06 01:00:00 2592.974565 229.561261 0.0102 -0.005044 1.007028 1.001879 1.004691 0.004088 -0.415935 1.872772 1.017680 1.541597 0.657009 0.011195 1.007028 0.988929 0.999000 0.009511 -0.622775 2.650935 1.003848 1.741677 0.698711 0.011259 1.007028 0.979381 0.991506 0.009179 -0.707043 3.505159 0.830303 1.872374 0.713401 0.009083 1.014744 0.979381 0.996615 0.006895 0.168152 4.434067 0.830303 1.872115 0.625493 0.052367 1.014744 0.921278 0.971965 0.011479 0.003091 8.867680 0.507507 1.805239 0.499861 2017-07-06 02:00:00 2595.240970 111.498601 0.0102 -0.005029 1.006148 0.999127 1.002969 0.005542 -0.624789 3.855797 2.058871 2.929583 1.561810 0.009963 1.006148 0.989687 0.999049 0.008379 -0.765640 4.551893 2.058871 3.302634 1.296594 0.016319 1.006148 0.978526 0.991104 0.009312 -0.861432 7.216670 1.709488 3.647934 1.347291 0.000808 1.013858 0.978526 0.995932 0.006872 -0.870219 9.129174 1.709488 3.860618 1.283035 0.063050 1.013858 0.920474 0.971530 0.011491 -0.765300 18.257410 1.044891 3.716808 1.029132 2017-07-06 03:00:00 2601.939179 154.465403 0.0102 0.001575 1.003558 0.996555 0.999547 0.005543 -0.640708 2.783251 0.721835 1.914348 1.612821 0.013029 1.003558 0.987139 0.997297 0.007360 -0.329707 3.285718 0.721835 2.187443 1.098140 0.017659 1.003558 0.976007 0.989220 0.009328 -0.719538 4.131480 0.721835 2.446232 0.882973 -0.004151 1.011248 0.976007 0.993385 0.006859 -0.848249 6.589761 0.721835 2.685895 0.903309 0.062422 1.011248 0.918104 0.969522 0.011449 -0.131612 13.178846 0.721835 2.663309 0.746976 2017-07-06 04:00:00 2594.198903 323.934946 0.0102 -0.006510 1.006553 0.999528 1.002792 0.005453 -0.093037 1.273228 0.344201 0.771118 0.713993 0.006701 1.006553 0.993343 1.001348 0.005868 -0.345152 1.566764 0.344201 1.023516 0.558260 0.021535 1.006553 0.978919 0.992897 0.009496 -0.492401 1.970058 0.344201 1.115490 0.427613 -0.005803 1.014265 0.978919 0.996261 0.006822 0.102404 2.598252 0.344201 1.225214 0.392388 0.057479 1.014265 0.920843 0.972906 0.011490 1.679001 6.284212 0.344201 1.269372 0.356447 PCADue to the large number of features created in the last step, we use PCA to reduce the dimensionality of the data. Aside from price, 15 other principal components are retained. Since we mostly care about predicting accuracy, therefore we are okay with losing some interpretability in the PCA process. 12345X = StandardScaler().fit_transform(df.iloc[:, 3:])comp = 15pca = PCA(n_components=comp)X_pca = pca.fit_transform(X)np.round(pca.explained_variance_ratio_, 2) array([0.29, 0.23, 0.13, 0.06, 0.05, 0.03, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01]) 1np.round(np.sum(pca.explained_variance_ratio_), 2) 0.93 123df_pca = pd.DataFrame(X_pca, index=df.index, columns = ['PC' + str(i) for i in range(1, comp+1)])df = pd.DataFrame(df.iloc[:, 0:3]).join(df_pca)disp(df.head()) price volume tr PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 PC13 PC14 PC15 time 2017-07-06 00:00:00 2607.823311 233.619901 0.0102 1.204847 -1.442388 -0.936078 0.562964 -1.802252 -0.126177 -2.115852 0.258446 0.547159 -0.005956 -0.345683 -0.077257 -0.389290 -0.253596 0.501094 2017-07-06 01:00:00 2592.974565 229.561261 0.0102 1.072486 -0.509851 -1.208146 1.100969 -1.991122 0.106322 -1.995090 0.089313 0.690748 -0.003055 -0.152291 -0.266696 -0.403753 -0.415290 0.602009 2017-07-06 02:00:00 2595.240970 111.498601 0.0102 6.313332 0.286169 -0.274636 1.772318 -3.465762 0.816421 -5.245687 0.737407 1.652572 0.043234 0.118171 -0.658779 -0.606447 -0.557087 0.499793 2017-07-06 03:00:00 2601.939179 154.465403 0.0102 1.986983 -0.820551 -1.195466 0.697308 -1.574198 0.289253 -1.100313 0.593288 0.655685 -0.005791 -0.018679 -0.152047 -0.354950 -0.493463 0.640942 2017-07-06 04:00:00 2594.198903 323.934946 0.0102 -1.350755 -0.873287 -1.805126 0.663435 -0.972641 0.036059 -0.009574 0.000981 0.200082 0.315963 -0.225084 -0.120908 -0.308701 -0.390018 0.597231 ModelingTrain and test sets are created for modeling purpose. Since it is time series data, randomization will not be performed. Rather, both train and test sets are chosen such that they both include a market upturn and market downturn. 12train = df.loc['2017-07-01':'2018-06-30']test = df.loc['2018-07-01':'2019-06-30'] Here we specify some modeling parameters. The trading frequency is set to one day. 12345trade_interval = '1H'trade_interval_min = 60ann_factor = 24 * 365training_threshold = 0.0075transaction_fee = 0.0025 Create a model engine that fit the train data and use grid search CV to tune the parameter grid. A long trade will be executed only if the model predict a next-5-day up move in the last 24 consecutive hours. This limits the frequency of trade which reduce the impact of the relatively large transaction fee per trade. The training threshold is set to 75 bps, which means the model is train to identify a potential up move of more than 75 bps in the next 5 days. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def run_model(Model, model_name, param, param_init, param_grid, search=False): # prepare data train_copy = train.resample(trade_interval).first() test_copy = test.resample(trade_interval).first() train_copy.dropna(inplace=True) test_copy.dropna(inplace=True) indicator = 24 # hr offset = 120 # hr X_train = train_copy.iloc[:-offset, 3:] Y_train = (train_copy.price.shift(-offset)/train_copy.price)[:-offset] &gt; (1 + training_threshold) X_test = test_copy.iloc[:-offset, 3:] Y_test = (test_copy.price.shift(-offset)/test_copy.price)[:-offset] &gt; (1 + training_threshold) # run model if search: model = GridSearchCV(estimator=Model(**param_init), cv=KFold(n_splits=5, random_state=0), scoring='precision', param_grid=param_grid).fit(X_train, Y_train) print(f'cv precision: &#123;round(model.best_score_, 2)&#125;, best param: &#123;model.best_params_&#125;') else: model = Model(**param).fit(X_train, Y_train) Y_pred = model.predict(X_test) cm = confusion_matrix(Y_test, Y_pred) precision = round(cm[1][1]/(cm[1][1] + cm[0][1]), 2) # calculate pnl # test_copy['ind'] = np.append(Y_pred, False) # test_copy['pnl'] = test_copy.ind * (test_copy.price.shift(-1) / test_copy.price) test_copy['pred'] = np.append(Y_pred, [False] * offset) test_copy['ind'] = test_copy.pred.rolling(indicator).sum() == indicator test_copy['buy'] = test_copy.ind.rolling(offset).sum() &gt; 0 test_copy['pnl'] = test_copy.buy * (test_copy.price.shift(-1) / test_copy.price) test_copy.pnl.replace(0, 1, inplace=True) test_copy.dropna(inplace=True) test_copy['fee'] = np.where(test_copy.ind != test_copy.ind.shift(1), 1-transaction_fee, 1) test_copy.pnl *= test_copy.fee test_pnl = round(test_copy.pnl.cumprod()[-1], 2) test_spr = round(np.mean(test_copy.pnl - 1 - test_copy.tr/(ann_factor)) / (test_copy.pnl - 1).std() * np.sqrt(ann_factor), 2) print(f'test precision: &#123;precision&#125;; pnl: &#123;test_pnl&#125;, spr: &#123;test_spr&#125;') return test_copy.pnl, precision, test_pnl, test_spr Baseline12345678910baseline = test.resample(trade_interval).first()baseline['pnl'] = baseline.price / baseline.price.shift(1) - 1baseline_pnl = round(baseline.price.iloc[-1] / baseline.price.iloc[0], 2)baseline_spr = round(np.mean(baseline.pnl - baseline.tr/(ann_factor)) / baseline.pnl.std() * np.sqrt(ann_factor), 2)result = test[['price']].copy().rename(&#123;'price': 'Baseline'&#125;, axis=1)/test.price.iloc[0]*1000comp = pd.DataFrame(&#123;'Strategy': 'Baseline', 'Precision': 'NA', 'P&amp;L': baseline_pnl, 'Sharpe Ratio': baseline_spr&#125;, index=[0])print(f'test precision: &#123;np.NaN&#125;; pnl: &#123;baseline_pnl&#125;, spr: &#123;baseline_spr&#125;') test precision: nan; pnl: 1.69, spr: 1.14 Logistic Regression12345678910111213Model = LogisticRegressionmodel_name = 'Logistic Regression'param = &#123;'class_weight':'balanced', 'solver':'liblinear', 'random_state': 0, 'C': 0.001, 'penalty': 'l2'&#125;param_init = &#123;'random_state': 0, 'class_weight': 'balanced'&#125;param_grid = &#123;'C': [1e-2, 1e-1, 1, 10], 'penalty': ['l1', 'l2']&#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.52, best param: {‘C’: 0.01, ‘penalty’: ‘l1’}test precision: 0.47; pnl: 1.14, spr: 0.47 Linear Discriminant Analysis123456789Model = LinearDiscriminantAnalysismodel_name = 'Linear Discriminant Analysis'param = &#123;'solver': 'svd', 'n_components': None&#125;param_init = &#123;&#125;param_grid = &#123;'solver': ['svd', 'lsqr'], 'n_components': [None, 5, 10, 25]&#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.5, best param: {‘n_components’: None, ‘solver’: ‘svd’}test precision: 0.47; pnl: 0.85, spr: -0.17 KNN123456789Model = KNeighborsClassifiermodel_name = 'KNN'param = &#123;'p': 2, 'leaf_size': 2, 'n_neighbors': 100&#125;param_init = &#123;'p': 2&#125;param_grid = &#123;'n_neighbors': [5, 25, 100], 'leaf_size': [2, 25, 100]&#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.53, best param: {‘leaf_size’: 2, ‘n_neighbors’: 100}test precision: 0.5; pnl: 1.95, spr: 1.85 Decision Tree1234567891011Model = DecisionTreeClassifiermodel_name = 'Decision Tree'param = &#123;'random_state':0, 'criterion': 'gini', 'max_depth': None, 'max_features': 10&#125;param_init = &#123;'random_state':0&#125;param_grid = &#123;'criterion': ['gini', 'entropy'], 'max_depth': [None, 5, 10, 25], 'max_features': [None, 'auto', 5, 10]&#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.53, best param: {‘criterion’: ‘gini’, ‘max_depth’: 10, ‘max_features’: 5}test precision: 0.49; pnl: 1.61, spr: 1.84 Random Forest12345678910111213141516Model = RandomForestClassifiermodel_name = 'Random Forest'param = &#123;'class_weight':'balanced', 'random_state': 0, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 5, 'n_estimators': 200&#125;param_init = &#123;'class_weight':'balanced', 'random_state': 0, 'n_estimators': 200&#125;param_grid = &#123;'criterion': ['gini', 'entropy'], 'max_depth': [5, 25], 'max_features': [5, 10]&#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.53, best param: {‘criterion’: ‘gini’, ‘max_depth’: 5, ‘max_features’: 10}test precision: 0.53; pnl: 1.55, spr: 1.16 AdaBoost123456789Model = AdaBoostClassifiermodel_name = 'AdaBoost'param = &#123;'random_state': 0, 'algorithm': 'SAMME.R', 'n_estimators': 200, 'learning_rate': 0.1&#125;param_init = &#123;'random_state': 0, 'algorithm': 'SAMME.R', 'n_estimators': 200&#125;param_grid = &#123;'learning_rate': [1e-2, 1e-1, 1, 10]&#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.52, best param: {‘learning_rate’: 0.01}test precision: 0.5; pnl: 0.86, spr: -0.09 Gradient Boost1234567891011121314Model = GradientBoostingClassifiermodel_name = 'Gradient Boost'param = &#123;'random_state': 0, 'warm_start': True, 'n_estimators': 200, 'max_depth': 10, 'max_features': 10, 'learning_rate': 0.1&#125;param_init = &#123;'random_state': 0, 'warm_start': True, 'n_estimators': 200,&#125;param_grid = &#123;'max_depth': [5, 25], 'max_features': [5, 10], 'learning_rate': [1e-2, 1e-1, 1, 10]&#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.54, best param: {‘learning_rate’: 10, ‘max_depth’: 5, ‘max_features’: 5}test precision: 0.48; pnl: 1.06, spr: 0.33 XGBoost1set_start_method('forkserver', force=True) # enabling multi-threading 12345678910111213141516Model = XGBClassifiermodel_name = 'XGBoost'param = &#123;'n_jobs':4, 'seed':0, 'n_estimators': 200, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 10, 'learning_rate': 0.01&#125;param_init = &#123;'n_jobs':4, 'seed':0, 'n_estimators': 200&#125;param_grid = &#123;'max_depth': [5, 25], 'min_child_weight': [1, 5], 'gamma': [1e-2, 1e-1, 1, 10], 'learning_rate': [1e-2, 1e-1, 1, 10]&#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.52, best param: {‘gamma’: 10, ‘learning_rate’: 0.01, ‘max_depth’: 25, ‘min_child_weight’: 1}test precision: 0.52; pnl: 1.33, spr: 0.86 SVC12345678910Model = SVCmodel_name = 'SVC'param = &#123;'probability':True, 'class_weight':'balanced', 'C': 10, 'gamma': 0.01&#125;param_init = &#123;'probability':True, 'class_weight':'balanced'&#125;param_grid = &#123;'C': [1e-2, 1e-1, 1, 10], 'gamma': [1e-2, 1e-1, 1, 10]&#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.64, best param: {‘C’: 0.1, ‘gamma’: 1}test precision: 0.47; pnl: 1.31, spr: 0.73 MLP Classificer12345678910111213141516Model = MLPClassifiermodel_name = 'MLP Classifier'param = &#123;'random_state': 0, 'hidden_layer_sizes': (25, 25), 'alpha': 0.01&#125;param_init = &#123;'random_state': 0&#125;param_grid = &#123;# 'hidden_layer_sizes': [x for x in itertools.product((5, 25, 100),repeat=2)], 'alpha' : [1e-2, 1e-1, 1, 10], # 'activation' : ['identity', 'logistic', 'tanh', 'relu'], # 'solver' : ['lbfgs', 'sgd', 'adam'], # 'learning_rate' : ['constant', 'invscaling', 'adaptive'], # 'max_itr' : [100, 200, 1000] &#125;a, b, c, d = run_model(Model, model_name, param, param_init, param_grid, search=True)result = result.join(a.cumprod() * 1000).rename(&#123;'pnl': model_name&#125;, axis=1).dropna()comp = comp.append(&#123;'Strategy': model_name, 'Precision': b, 'P&amp;L': c, 'Sharpe Ratio': d&#125;, ignore_index=True) cv precision: 0.53, best param: {‘alpha’: 1}test precision: 0.55; pnl: 2.06, spr: 1.79 ResultThe results are summarized as follow. 1disp(comp.replace('NA', 0).sort_values('P&amp;L', ascending=False), 20) Strategy Precision P&amp;L Sharpe Ratio 10 MLP Classifier 0.55 2.06 1.79 3 KNN 0.50 1.95 1.85 0 Baseline 0.00 1.69 1.14 4 Decision Tree 0.49 1.61 1.84 5 Random Forest 0.53 1.55 1.16 8 XGBoost 0.52 1.33 0.86 9 SVC 0.47 1.31 0.73 1 Logistic Regression 0.47 1.14 0.47 7 Gradient Boost 0.48 1.06 0.33 6 AdaBoost 0.50 0.86 -0.09 2 Linear Discriminant Analysis 0.47 0.85 -0.17 Plotting the cumulative return for each strategy with transaction fee reflected. 123456plt.plot(result)plt.legend(result.columns, frameon=False)plt.xticks(rotation=30)plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(30))plt.ylabel('Cumulative Value Based on $1000 Investment')plt.show()","tags":"tech"},{"title":"&#128214; Notes on ISLR","url":"/2019/04/islr/","text":"This is a study note on the book An Introduction to Statistical Learning with Applications in R, with my own experimental R-code for each topic. Cheers! Navigation00. Introduction01. Linear Model02. Tree-Based Method03. Unsupervised Learning04. PCA - A Deeper Dive05. A Side Note on Hypothesis Test and Confidence Interval Introduction &#8634;Suppose we observe a quantitative response Y and p different predicting variables X = (X_1, X_2...X_p). We assume that there is a underlying relationship f between Y and X: Y=f(X) + \\epsilonWe want to estimate f mainly for two purpose: prediction: in the case where Y is not easily obtained, we want to estimate f with \\hat{f}, and use \\hat{f} to predict Y with \\hat{Y}. Here \\hat{f} can be a black box, such as highly non-linear approaches which offers accuracy over interpretability. \\hat{Y} = \\hat{f}(X) inference: in the case where we are more interested in how Y is affected by the change in each X_n. We need to know the exact form of \\hat{f}. For example, linear model is often used which offer interpretable inference but sometimes inaccurate. FeatureThere are important and subtle differences between a feature and a variable. variable: raw data feature: data that is transformed, derived from raw data. A feature can be more predictive and have a direct relationship with the target variable, but it isn’t immediately represented by the raw variable. The need for feature engineering arises from limitations of modeling algorithms: the curse of dimensionality (leading to statistical insiginificance) the need to represent the signal in a meaningful and interpretable way the need to capture complex signals in the data accurately computational feasibility when the number of features gets large. Feature TransformationThe Occam’s Razor principle states that a simpler solutions are more likely to be corret than complex ones. Consider the following example of modeling a exponentially distributed response. After applying a log transformation, we can view the relation from a different viewpoint provided by the new feature space, in which a simpler model may achieve more predictive power than a complex model in the original input space.123456789x1 &lt;- runif(100, 1,10)x2 &lt;- exp(x1)df &lt;- data.frame(x1 = x1, x2 = x2)df$logx2 &lt;- log(df$x2)ssoptions(repr.plot.width=6, repr.plot.height=3)p1 &lt;- ggplot(data = df, aes(x = x1, y = x2)) + geom_point(size=0.3)p2 &lt;- ggplot(data = df, aes(x = x1, y = logx2)) + geom_point(size=0.3)grid.arrange(p1, p2, nrow=1) Consider another classification problem, in which we want to identify the boundary between the two classes. A complex model would draw a circle as the divider. A simpler approach would be to create a new feature with distances of each point from the origin. The divider becomes a much simpler straight line.123456789101112131415161718x1 &lt;- runif(1000,-1,1)x2 &lt;- runif(1000,-1,1)class &lt;- ifelse(sqrt(x1^2 +x2^2) &lt; 0.5, \"A\", \"B\")df &lt;- data.frame(x1 = x1, x2 = x2, class = class)p1 &lt;- ggplot(data = df, aes(x = x1, y = x2, color = class)) + geom_point(size=1)df$dist_from_0 &lt;- sqrt(df$x1^2 + df$x2^2)p2 &lt;- ggplot(data = df, aes(x = 0, y = dist_from_0, color = class)) + geom_point(position = \"jitter\", size=1) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) + annotate(\"segment\", x = -0.5, xend = 0.5, y = 0.5, yend = 0.5)options(repr.plot.width=8, repr.plot.height=3)grid.arrange(p1, p2, nrow=1) Feature SelectionFor p predictive variables, there are a total of 2^p models. We use feature selection to choose a smaller subset of the variable to model. Forward Selection We begin with the null model with no variables and an intercept. We then fit n simple linear regression to choose our first variable with the lowest RSS. Same way to choose the next variable to be added until some stoppoing rule. Backward Selection We begin with the full model and remove the variable with the largest coefficient p-value. Re-fit and remove the next. Mixed Selection We begin with the null model and the forward selection technique. Whenever the p-value for a variable exceeds a threshold we remove it. Regression ProblemsIn regression problems the variables are quantitative, we use mean squared error, or MSE, to measure the quality of estimator \\hat{f}: MSE = \\dfrac{\\sum_{i=1}^n [y_i - \\hat{f}(x_i)]^2}{n}A fundamental property of statistical learning that holds regardless of the particular dataset and statistical method is that as model flexibility increases, we can observe a monotone decrease in training MSE and an U-shape in test MSE. Bias vs VarianceThe bias-variance trade off decompose the expected test MSE of a single data point x_0: \\begin{align} \\mathbb{E}MSE^{test}(x_0) &= \\mathbb{E}[y_0 - \\hat{f}(x_0)]^2 \\\\ &= Var[\\hat{f}(x_0)] + Bias[\\hat{f}(x_0)]^2 + Var[\\epsilon] \\end{align}where: variance refers to the variance of the estimator among different datasets. A highly flexible \\hat{f} lead to a high variance, as even small variance in data induce change in the \\hat{f}‘s form. bias refers to the error due to estimator \\hat{f} inflexibility. For example, using linear \\hat{f} to estimate non-linear relationship leads to high bias. Classification ProblemsIn regression problems the variables are qualitative, we use error rate to measure the quality of estimator \\hat{f}: \\text{error rate} = \\dfrac{\\sum_{i=1}^n \\textbf{1}\\{y_i \\neq \\hat{f}(x_i)\\}}{n}The Bayes ClassifierThe Bayes classifier predict the classification based on the combination of the prior probability and its likelihood given predictor values. With categories C_1, C_2, C_3..., and predictor values \\textbf{x} = (x_1, x_2, x_3...), \\hat{y} is assigned to category C_k which has the maximum posterior probability: \\hat{y} = \\hat{f}^{Bayes}(\\textbf{x}) = C_k, \\;where\\; k = argmax_k \\;p(C_k | \\textbf{x})Where: p(C_k | \\textbf{x}) = \\dfrac{p(x_1, x_2, x_3...|C_k)p(C_k)}{p(x_1, x_2, x_3...)}The naive Bayes classifier assumes independence between the predictor X_i‘s, and the formula becomes: \\hat{y} = \\hat{f}^{naiveBayes}(\\textbf{x}) = C_k, where\\; k = argmax_k \\;p(C_k) \\times \\prod p(x_i | C_k)When x_i is continuous, the Guassian naive Bayes classifier assumes that p(x_i | C_k) \\sim \\mathcal{N}(\\mu_{i, C_k}, \\sigma^2_{i, C_k}) In R, we use the naiveBayes function from the e1071 package to predict Survival from the Titanic dataset.1234567891011121314151617181920library(e1071)library(caret)set.seed(9999)df=as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq),]df$Freq &lt;- NULL# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$Survived, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]# fit naive bayes classifierbayes &lt;- naiveBayes(Survived ~ ., data=train)# 10-fold validationbayes.cv &lt;- train(Survived ~ ., data=train, method = \"nb\", trControl = trainControl(method = \"cv\", number = 10)) Viewing the model results, the prior probabilities p(C_k) are shown in the “A-priori probabilities” section.1234567891011&gt; bayesNaive Bayes Classifier for Discrete PredictorsCall:naiveBayes.default(x = X, y = Y, laplace = laplace)A-priori probabilities:Y No Yes0.6767554 0.3232446 The likelihood p(x_i | C_k) are shown in the “Conditional probabilities” section123456789101112131415Conditional probabilities: ClassY 1st 2nd 3rd Crew No 0.09481216 0.10554562 0.34615385 0.45348837 Yes 0.28838951 0.15730337 0.24344569 0.31086142 SexY Male Female No 0.91323792 0.08676208 Yes 0.53183521 0.46816479 AgeY Child Adult No 0.03130590 0.96869410 Yes 0.06928839 0.93071161 The confusionMatrix function from the caret package returns a test Accuracy of 0.7978, which corresponds to an Bayes error rate of 0.2023. \\text{error rate}^{Bayes} = 1 - \\mathbb{E}max_k\\;p[Y=C_k|X=(x_1, x_2, x_3...)]Theoretically, the Bayes classifier produces the lowest error rate if we know the true conditional probability p(C_k | \\textbf{x}), which is not the case with real data. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.123456789&gt; confusionMatrix(predict(bayes, test), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 343 82 Yes 29 95 Accuracy : 0.7978 With 10-fold cross validation, the test error rate is 0.2095.123456789&gt; confusionMatrix(predict(bayes.cv, test), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 332 75 Yes 40 102 Accuracy : 0.7905 K-Nearest NeighborsThe KNN classifier estimate the conditional probability p(C_k | \\textbf{x}) based on the set of K predictors in the training data that are the most similar to \\textbf{x}, representing by \\mathcal{N}. \\hat{y} = \\hat{f}^{KNN}(\\textbf{x}) = C_k, \\;where\\; k = argmax_k \\;p(C_k | \\textbf{x}) \\\\ with\\; p(C_k | \\textbf{x}) = \\dfrac{\\sum_{i\\in\\mathcal{N}}\\textbf{1}_{y_i = C_k}}{K}Note that the p(C_k | \\textbf{x}) is one minus the \\mathcal{N}-local error rate of a C_k estimate , and therefore with KNN we are picking the C_k that minimizes the \\mathcal{N}-local error rate given \\textbf{x}. When K=1, the estimator \\hat{f}^{KNN} produces a training error rate of 0, but the test error rate might be quite high due to overfitting. The method is therefore very flexible with low bias and high variance. As K\\rightarrow\\infty, the estimator becomes more linear. In R, we use the knn function (with K=1) in the class library to predict Survival from the Titanic dataset.1234567891011121314151617181920212223242526272829303132library(e1071)library(class)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq),]df$Freq &lt;- NULL# The \"knn\" function in the \"class\" library only works with numeric datadf$iClass &lt;- as.integer(df$Class)df$iSex &lt;- as.integer(df$Sex)df$iAge &lt;- as.integer(df$Age)df$iSurvived &lt;- as.integer(df$Survived)# Create random 75/25 train/test splittrain_ind &lt;- sample(seq_len(nrow(df)), size = floor(0.75 * nrow(df)))df_train &lt;- df[train_ind, c(5:7)]df_test &lt;- df[-train_ind, c(5:7)]df_train_cl &lt;- df[train_ind, 4]df_test_cl &lt;- df[-train_ind, 4]# Fit KNNknn &lt;- knn(train = df_train, test = df_test, cl = df_train_cl, k=1)# 10-fold CVknn.cv &lt;- tune.knn(x = df[, c(5:7)], y = df[, 4], k = 1:20, tunecontrol=tune.control(sampling = \"cross\"), cross=10) The confusion matrix shows an error rate of 0.1942.123456789&gt; confusionMatrix(knn, df_test_cl)Confusion Matrix and Statistics ReferencePrediction No Yes No 364 101 Yes 6 80 Accuracy : 0.8058 Now run KNN with the tune wrapper to perform 10-fold cross validation. The result recommends KNN with K=1, which turns out to be the same as what we originally tested.1234567891011&gt; knn.cvParameter tuning of ‘knn.wrapper’:- sampling method: 10-fold cross validation- best parameters: k 1- best performance: 0.2157576 Summarizing the test error rate for naiveBayes and KNN.12naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942 Linear Model &#8634;Simple Linear RegressionA simple linear regression assumes that: Y \\sim \\beta_0 + \\beta_1XCoefficient EstimateGiven data points (x_i, y_i), let \\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i. We define the residual sum of squares, or RSS, as: RSS = \\sum (y_i - \\hat{y_i})^2Minimizing RSS as an objective function, we can solve for \\hat{\\beta_0}, \\hat{\\beta_1}: \\begin{align} \\hat{\\beta_1} &= \\dfrac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i-\\bar{x})^2} \\\\ \\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1}\\bar{x} \\end{align}Coefficient Estimate - Gaussian ResidualFurthermore, if we assume the individual error terms are i.i.d Gaussian, i.e.: Y = \\beta_0 + \\beta_1x + \\epsilon \\\\ \\text{where, } \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)We now have a conditional pdf of Y given x: p(y_i|x_i; \\beta_0, \\beta_1, \\sigma^2) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\dfrac{[y_i - (\\beta_0 + \\beta_1x_i)]^2}{2\\sigma^2}}The maximum log-likelihood estimator \\hat{l}for the paramter estimate b_0, b_1, and s^2 can be computed as follow: \\begin{align} \\hat{l}(b_0, b_1, s^2| x_i, y_i) &= log \\prod p(y_i|x_i; b_0, b_1, s^2) \\\\ &= -\\dfrac{n}{2}log{2\\pi} - nlogs - \\dfrac{1}{2s^2}\\sum [y_i - (b_0+b_1x)]^2 \\end{align}Setting the partial-derivative of the estimator with respect to each of the parameter to zero, we can obtain the maximum likelihood parameters: \\begin{align} \\hat{\\beta_1} &= \\dfrac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i-\\bar{x})^2} \\\\ \\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1}\\bar{x} \\\\ \\hat{\\sigma^2} &= \\dfrac{1}{n}\\sum [y_i - (b_0+b_1x_i)]^2 \\end{align}MLE, or maximum likelihood estimation, is a frequentist approach for estimating model parameters based on the assumed underlying model form and error distribution, by maximizing the probability of seeing what we saw in the data. MLE is a near-optimal method of estimation, and is optimal in many cases. If we assume a Gaussian error distribution and a linear model, then the conclusion above states that maximizing the MLE objective function is the SAME as minimizing the RSS objective function. More on frequentist vs Bayesian in this SOA work paper. Also see this CMU lecture note and for more detail regarding the derivation. Model FitRecall that we assume there is a underlying relationship f between Y and X: Y=f(X) + \\epsilonThe residual standard error, or RSE, estimates the standard deviation of \\epsilon. Note that RSE is an absolute measure of the lack of fit of the model and depends on units of Y. RSE = \\sqrt{RSS/(n-2)}The R^2 measures the proportion of variance explained by the regression. TSS is the total sum of squares which measures the total variance in Y R^2 = 1 - \\dfrac{RSS}{TSS}In a simple regression setting, R^2 = Corr(X, Y)^2 Residual PlotHere is a good article on how to interpret your residual plot. Summarizing the approaches for different residual issues: Y-axis Unbalanced: transform target X-axis Unbalanced: transform predictor Heteroscedasticity: transform target/predictor Non-Linearity: transform predictor; create non-linear model Outlier: transform target/predictor; remove/assess the outlier; Multiple Linear RegressionThe multiple linear regression takes the form: Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilonF-statisticWe use the F-statistic to test the null hypothesis that there are no relationships between the predictors and target variable. H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0 \\\\ H_a: \\text{ at least one } \\beta \\text{ is non-zero}We calculate the F-statistic as follow: F = \\dfrac{(TSS - RSS)/p}{RSS/(n-p-1)}We expect F=1 if H_0 is true. Potential ProblemsNon-Linearity Residual plots are useful to detect whether the underlying relationship f is non-linear. One of the solutions to this problem is to fit transformations of the predictors such as log{X}, \\sqrt{X}, and X^2. Collinearity This is where two or more predictors are closely correlated with each other, or “collinear”. Collinearity reduces the accuracy of the coefficient estimates by increasing its standard deviation and p-value. One way to detect collinearity is from the correlation matrix or the “pairs” plot in R. There are two solutions: dropping one of the collinear predictor, or combine the collinear predictors into a single predictor. Multi-collinearity This is where collinearity exist between three or more predictors when none of the pairwise correlations are high. The best way to assess multi-collinearity if through variance inflation factor, or VIF. VIF(\\beta_j) = \\dfrac{var(\\beta_j) \\text{ in full model}}{var(\\beta_j) \\text{ in single } \\beta_j \\text{ model}}A VIF of 1 indicates no collinearity. A VIF of 10+ indicates high collinearity. Outliers Use residual plot to detect and potentially remove outliers. Linear Model Selection and RegularizationLinear model can be improved by using alternative fitting procedures, which produce better prediction accuracy and model interpretability. Subset Selection Select a subset of the original predictors and then fit the model. Shrinkage/Regularization Fit the model by shrinking coefficient estimates towards zero, therefore reducing variances of the coefficient estimates. Dimension Reduction Project the predictors onto a M-dimensional subspace, then use the M projections as predictors to fit a linear model with least square. Subset SelectionThe beset subset selection fits a separate least square regression for each combination from the p predictors, creating 2^p models to compare. The forward stepwise selection and backward stepwise selection fits a total of 1+p(p+1)/2 models. Specifically, at each step a predictor is added/removed to the model only if it gives the greatest additional improvement (lowest RSS or highest adjusted R^2) among all the predictors. After the selection process, we need to determine the optimal model that gives the lowest potential test error, either through: Cross-validation, or Adjusted train error Example 1: \\boldsymbol{C_p} := \\dfrac{1}{n}(RSS + 2d\\hat{\\sigma}^2), where d is the number of predictors in the subset, and \\hat{\\sigma}^2 is the variance of error estimated using the full models containing all p predictors. Essentially, a penalty term 2d\\hat{\\sigma}^2 is added to the train RSS to adjust for the fact that the training error tends to underestimate the test error. Example 2: \\boldsymbol{AIC} = \\dfrac{1}{n\\hat{\\sigma}^2}(RSS + 2d\\hat{\\sigma}^2). The AIC, or Akaike information criterion, uses the maximum likelihood function to assess the relative quality of statistical models give a set of data. In linear models with Gaussian error terms, the maximum likelihood function is equivalent to RSS, and therefore C_p and AIC are proportional to each other. Example 3: \\boldsymbol{BIC} = \\dfrac{1}{n\\hat{\\sigma}^2}(RSS + log(n)d\\hat{\\sigma}^2) Example 4: \\boldsymbol{Adjusted}\\; \\boldsymbol{R^2} = 1 - \\dfrac{RSS/(n-d-1)}{TSS/(n-1)}. While RSS always decreases in the stepwise selection as the number of predictors increases, RSS/(n-d-1) may or may not decrease. ShrinkageRidge RegressionRecall that in least square regression, the coefficient \\beta are estimated by minimizing the objective function RSS. Objective\\ Function = RSS = \\sum (y_i - \\hat{y_i})^2In ridge regression, the objective function include an additional shrinkage penalty, where \\lambda>0 is a tuning parameter. Note that we do not want to shrink the intercept \\beta_0, which is simply a measure of the mean of the responses: Objective\\ Function = RSS + \\lambda\\sum_{i>0}\\beta_i^2 As \\lambda increases, the variance decreases and the bias increases. The model fit usually is improved initially as the variance decreases, but worsen at some point when bias starts to increases rapidly. Cross-validation is often used to select the optimal \\lambda. As \\lambda\\rightarrow\\infty, the coefficient approaches 0. The ridge regression works when the linear model has low bias and high variance, e.g. when the underlying relationship is close to linear. The ridge regression trades off a small increase in bias for large decrease in variance. Additionally, it is important to standardize all features when applying regularization. Imagining a feature in dollar and in thousand dollar: the model with the dollar feature will have much higher coefficient compared to the thousand dollar one, leading to larger regularization effect for the dollar feature. Lasso RegressionAlthough the ridge regression shrinks the coefficients, it does not eliminiate excess predictors. Model interpretation might be an issue for the ridge regression where the number of predictors are large. The lasso regression overcomes this issue and force some coefficients to be exactly 0. Objective\\ Function = RSS + \\lambda\\sum_{i>0}|\\beta_i|However, there are limitations of feature selections using regularization techniques such as lasso, such as model interpretability. In addition, the feature we selected are optimized in linear models, and may not necessarily translate to other model forms. Note the difference between L2 (ridge) and L1 (lasso) penalty: when the coefficients (absolute value) are greater than 1 (when the parameters are large), the L2 penalty is greater than the L1, and ridge provides more shrinkage. when the coefficients (absolute value) are smaller than 1 (when the parameters are small), the L1 penalty is greater than the L2, and lasso provides more shrinkage. In R, we use the glmnet package to compute ridge and lasso regressions to predict mpg from the mtcars built-in data set.1234567891011121314151617181920212223242526272829303132333435363738394041library(glmnet)library(caret)set.seed(9999)# datadf &lt;- as.data.frame(mtcars)x &lt;- model.matrix(mpg~., df)[, -1]y &lt;- df$mpg# 75/25 train/test splitpartition &lt;- createDataPartition(df$mpg, list = FALSE, p = .75)df_train &lt;- df[partition, ]df_test &lt;- df[-partition, ]x_train &lt;- x[partition, ]x_test &lt;- x[-partition, ]y_train &lt;- y[partition]y_test &lt;- y[-partition]# fit regressionm1 &lt;- lm(mpg ~ ., df_train)m1.pred &lt;- predict(m1, df_test)m1.mse &lt;- round(mean((y_test - m1.pred)^2), 2)# fit ridge regressionm2 &lt;- cv.glmnet(x_train, y_train, alpha=0, nfolds=6)m2.bestlambda &lt;- m2$lambda.minm2.pred &lt;- predict(m2, s=m2.bestlambda, x_test)m2.mse &lt;- round(mean((y_test - m2.pred)^2), 2)# fit lasso regressionm3 &lt;- cv.glmnet(x_train, y_train, alpha=1, nfolds=6)m3.bestlambda &lt;- m3$lambda.minm3.pred &lt;- predict(m3, s=m3.bestlambda, x_test)m3.mse &lt;- round(mean((y_test - m3.pred)^2), 2)# get coefficientsm2.best &lt;- glmnet(x_train, y_train, alpha=0, lambda=m2.bestlambda)m3.best &lt;- glmnet(x_train, y_train, alpha=1, lambda=m3.bestlambda)comp &lt;- cbind(coef(m1), coef(m2.best), coef(m3.best))colnames(comp) &lt;- c(\"original\", \"ridge\", \"lasso\") The test MSE are as follow. Note that both ridge and lasso regression perform better than the original regression.123456&gt; m1.mse[1] 16.64&gt; m2.mse[1] 3.66&gt; m3.mse[1] 6.61 We also made a comparison of the coefficients, based on the normal regression and the regularized regression with cv-optimal lambda.1234567891011121314&gt; comp11 x 3 sparse Matrix of class \"dgCMatrix\" original ridge lasso(Intercept) -19.52071389 19.420775180 14.002192375cyl 1.69431225 -0.275408324 . disp 0.01185998 -0.004705579 . hp -0.01449594 -0.011129305 -0.002545135drat 3.08676192 1.272808791 1.334526777wt -3.19280650 -1.137507747 -2.254408320qsec 1.02473436 0.117671597 0.331182874vs 0.97127211 0.677494227 . am 2.63740010 1.633418877 1.703501439gear 3.36943552 0.794847062 1.571031414carb -1.45443855 -0.588427657 -1.162118484 Resampling MethodResampling methods involve repeatedly drawing samples from a training set to re-fit the model. Cross-ValidationWe often use the test error rate to determine and compare how well a statistical learning model perform. However, in the absence of a large designated test set, the test error rate can be difficult to estimate. The train error rate is often quite different from the test error rate. Therefore, cross-validation can be used to estimate the test error rates by creating validation sets off the train data. A k-fold cross validation involves randomly dividing the observations into k-groups. The process is repeated k-times where each group is treated as a validation set while fitting the remaining k-1 groups. The k-fold CV test MSE is the average of all the MSE from each validation set: MSE_{CV} = \\dfrac{1}{k}\\sum_{i=1}^{k} MSE_iThe leave-one-out cross validation, or LOOCV, is a special case of k-fold CV with k=n. Since LOOCV requires fitting the model n times, with n being equal to number of train data points. The k-fold CV with k=5\\ or \\ 10 are more feasible computationally as it only needs to fit the model 5\\ or\\ 10 times. BootstrapBootstrap provides a measure of accruacy of either a parameter estimate or a given statistical learning method. It can be used to estimate variance of a parameter by repeatedly re-sampling the same data set with replacement (i.e. duplicate data entries allowed) while re-calculating the parameter based on each re-sample. Hyper-Parameter TuningWe specify numerious constants called hyperparameter during our modeling process, e.g. \\lambda, \\alpha, etc. To set these constant such that our model can predict accruately while avoiding over-complexity and overfitting, we tune our hyperparameter with cross validation. For more details see the auto claim notebook. Generalized Linear ModelGeneralized linear models were developed by Nelder and Wedderburn in a paper published in 1972 to provide a flexible framework which introduces a link function that transform the linear combinations of predictors. An Ordinary Linear Model has many limitations: As ordinary linear model produces a numeric response, it requires the assumptions of orderings to predict qualitative responses. Negative values may be predicted when not allowed. When the variance of the target variable depends on the mean, the homoscedasticity assumption is violated, and therefore the least square estimator is no longer the MLE estimator and various statistical test would not hold. Sensitive to outliers. Does not perform well with non-linear relationships. The Generalized Linear Models relaxes the assumptions of OLM. First, GLM relaxes the normal residual assumption of OLM, and allow the target variable Y to follow any distribution within the exponential distribution family: f(y_i|x_i) \\sim \\text{exponential distribution family}With regard to this distribution, there exists a canonical link function associated with it that simplifies the mathematics of solving GLM analytically. Normal =&gt; Identity: \\phi(a) = a Exponential/Gamma =&gt; Negative Inverse: \\phi(a) = - a^{-1} Inverse Gaussian =&gt; Inverse Square: \\phi(a) = a^{-2} Poisson =&gt; Log: \\phi(a) = ln(a) Bernoulli/Binomial/Multinomial =&gt; Logit: \\phi(a) = ln[a/(1-a)] We can either choose the canonical link function or pick another one (which may not lead to a converged GLM solution, however). With this link function, GLM assumes that the expectation of the target is the inverse linked linear combination of predictors: \\mathbb{E}(y_i|x_i) = \\phi^{-1}(\\beta x_i)With all above assumptions satisfy, the coefficient \\beta of a GLM model can then be solved: \\phi(Y) \\sim \\beta XLogistic RegressionThe logistic regression model is popular for classification problems. With two response classes, we can calculate the probability of assigning the response in each class and predict the response by choosing the class with the higher probability. or more than two response classes, multiple-class logistic regression is available but the discrimentant analysis is more popular. We define p(X) = \\mathbb{P}[Y = 1|X] as our new response variable and the link function: logit function, short for logistic function, as such: \\phi(p(X)) = logit(p(X)) = log[\\dfrac{p(X)}{1-p(X)}]Since we assume a linear relationship between our predictor X and the linked reponse logit(p(X)), we have: log[\\dfrac{p(X)}{1-p(X)}] = \\beta_0+\\beta_1X_1+\\beta_2X_2\\dotsTherefore, p(X) = \\dfrac{e^{\\beta_0+\\beta_1X_1+\\beta_2X_2\\dots}}{1+e^{\\beta_0+\\beta_1X_1+\\beta_2X_2\\dots}}Now we have a nice property of p(X) \\in (0, 1), which is exactly what we wanted to model probability responses. The quantity p(X)/(1-p(X)) is called the odds. In R, we use the glm function (with family=binomial) in the predict Survival from the Titanic dataset.12345678910111213141516171819202122232425262728293031323334353637library(e1071)library(caret)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]df &lt;- subset(df, select = -c(Freq))# Binarize class, sex and agedf.new &lt;- predict(dummyVars(\"~Class+Sex+Age\", df, sep=\"_\", fullRank=TRUE), df)df.new &lt;- as.data.frame(df.new)df.new[\"Survived\"] &lt;- df$Surviveddf &lt;- df.new# Create random 80/20 train/test splittrain_ind &lt;- sample(seq_len(nrow(df)), size = floor(0.80 * nrow(df)))df_train &lt;- df[train_ind, ]df_test &lt;- df[-train_ind, ]# Fit Logistic Regressionmodel &lt;- glm(Survived~., df_train, family=binomial)summary(model)contrasts(df$Survived)# Predict In-Sampleprob_train &lt;- predict(model, df_train, type=\"response\")pred_train &lt;- rep(\"No\", nrow(df_train))pred_train[prob_train &gt; .5] &lt;- \"Yes\"# Predict Out-Of-Sampleprob_test &lt;- predict(model, df_test, type=\"response\")pred_test &lt;- rep(\"No\", nrow(df_test))pred_test[prob_test &gt; .5] &lt;- \"Yes\" From summary(model), note that most coeefficients are significant.123456789&gt; summary(model)Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.3094 0.3132 0.988 0.323 Class_2nd -1.1163 0.2187 -5.105 3.31e-07 `Class_3rd -1.7041 0.1918 -8.883 &lt; 2e-16 `Class_Crew -0.8848 0.1773 -4.991 6.01e-07 `Sex_Female 2.3691 0.1555 15.235 &lt; 2e-16 `Age_Adult -0.6289 0.2767 -2.273 0.023 * Note that probability of 1 correspond to “Yes” in the Survived variable.1234&gt; contrasts(df$Survived) YesNo 0Yes 1 From in-sample confusion matrix.123456789&gt; confusionMatrix(as.factor(pred_train), df_train$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 1087 296 Yes 102 275 Accuracy : 0.7739 From out-of-sample confusion matrix.123456789&gt; confusionMatrix(as.factor(pred_test), df_test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 277 66 Yes 24 74 Accuracy : 0.7959 Comparing the test error rate between naiveBayes, KNN and logistic regression.123naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942logistic regression: 0.2041 Poisson RegressionThe Poisson distribution expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. \\mathbb{P}(\\text{k events in interval}) = e^{\\lambda}\\dfrac{\\lambda^k}{k!}We can fit Posisson regression if we observe that the frequencies of response variable Y exhibits a Poisson shape. We will create a new response vector \\theta and assumes that log(\\theta) has an underlying linear relationship in X. Y \\sim poisson(\\boldsymbol {\\theta}) \\\\ \\text{and, } log(\\boldsymbol {\\theta}) = \\beta XThat is, we assume that for each X_i, Y_i \\sim poisson(e^{\\beta X_i}). The log link function ensures that \\boldsymbol{\\theta} is strictly positive. Note that we had made a strong assumption that for each X_i, the mean and variance of Y_i are the same, as dictated by the Poisson distribution. However, if the data shows larger variance than expected, or overdispersion, we can then use the quasi-Poisson regression, which is essenstially the negative binomial distribution with looser assumptions than Poisson. In the diamonds dataset from the ggplot2 package, we plotted the histogram of the price data from 50,000 observations. 123library(ggplot2)df &lt;- as.data.frame(diamonds)ggplot(df, aes(x=price)) + geom_histogram(binwidth=1) The price data shows resemblence to a Poisson distribution. We fitted four different models: linear, ridge, lasso, and Poisson regressions.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162library(e1071)library(ggplot2)library(caret)library(glmnet)set.seed(9999)df &lt;- as.data.frame(diamonds)# caret::dummyVars does not work well with ordered factor. change to unordered.df[\"cut_1\"] &lt;- factor(df$cut, order=FALSE)df[\"color_1\"] &lt;- factor(df$color, order=FALSE)df[\"clarity_1\"] &lt;- factor(df$clarity, order=FALSE)# Binarize Category Variabledummy &lt;- dummyVars(~ cut_1 + color_1 + clarity_1, df, sep=\"_\", fullRank=TRUE)df.new &lt;- as.data.frame(predict(dummy, newdata = df))df.new[\"carat\"] &lt;- df$caratdf.new[\"price\"] &lt;- df$pricedf &lt;- df.newx &lt;- model.matrix(price~., df)[, -1]y &lt;- df$price# Create random 80/20 train/test splitpartition &lt;- sample(seq_len(nrow(df)), size = floor(0.80 * nrow(df)))df_train &lt;- df[partition, ]df_test &lt;- df[-partition, ]x_train &lt;- x[partition, ]x_test &lt;- x[-partition, ]y_train &lt;- y[partition]y_test &lt;- y[-partition]# Fit Linear Regressionm1 &lt;- lm(price~., df_train)m1.pred &lt;- predict(m1, df_test)m1.mse &lt;- round(mean((y_test - m1.pred)^2), 2)# Fit Ridge Regressionm2 &lt;- cv.glmnet(x_train, y_train, alpha=0, nfolds=10)m2.bestlambda &lt;- m2$lambda.minm2.pred &lt;- predict(m2, s=m2.bestlambda, x_test)m2.mse &lt;- round(mean((y_test - m2.pred)^2), 2)# Fit Lasso Regressionm3 &lt;- cv.glmnet(x_train, y_train, alpha=1, nfolds=10)m3.bestlambda &lt;- m3$lambda.minm3.pred &lt;- predict(m3, s=m3.bestlambda, x_test)m3.mse &lt;- round(mean((y_test - m3.pred)^2), 2)# Fit Poisson Regressionm4 &lt;- glm(price~., df_train, family=poisson(link=\"log\"))m4.pred &lt;- predict(m4, df_test)m4.mse &lt;- round(mean((y_test - exp(m4.pred))^2), 2)# Fit quasiPoisson Regressionm5 &lt;- glm(price~., df_train, family=quasipoisson(link=\"log\"))m5.pred &lt;- predict(m5, df_test)m5.mse &lt;- round(mean((y_test - exp(m5.pred))^2), 2)# Compare Coefficientm2.best &lt;- glmnet(x_train, y_train, alpha=0, lambda=m2.bestlambda)m3.best &lt;- glmnet(x_train, y_train, alpha=1, lambda=m3.bestlambda)comp &lt;- cbind(coef(m1), coef(m2.best), coef(m3.best), coef(m4), coef(m5))colnames(comp) &lt;- c(\"original\", \"ridge\", \"lasso\", \"Poisson\", \"quasiPoi\") Showing the results. We can see that lasso regression improved upon ridge. However, the Poisson regression show very high MSE, and not improved by using quasi-Poisson to deal with overdispersion.1234567891011121314&gt; m1.mse[1] 1296082&gt; m2.mse[1] 1662769&gt; m3.mse[1] 1296773&gt; m4.mse[1] 5237564&gt; m5.mse[1] 5237564 Comparing coefficients:12345678910111213141516171819202122&gt; comp19 x 5 sparse Matrix of class \"dgCMatrix\" original ridge lasso Poisson quasiPoi(Intercept) -7369.2859 -2751.693983 -7083.4777 5.17185599 5.17185599cut_1_Good 686.6877 112.163397 657.1100 0.17515789 0.17515789`cut_1_Very Good` 864.3160 319.828814 838.9901 0.20528993 0.20528993cut_1_Premium 893.6342 367.364011 866.6581 0.18009057 0.18009057cut_1_Ideal 1025.5579 421.231162 999.8031 0.20406528 0.20406528color_1_E -225.3985 -23.486004 -205.5637 -0.05660526 -0.05660526color_1_F -294.1807 2.717515 -274.2126 -0.03079075 -0.03079075color_1_G -522.9599 -112.556133 -501.1661 -0.10597258 -0.10597258color_1_H -997.6266 -490.885083 -975.9837 -0.25411004 -0.25411004color_1_I -1452.1455 -749.323400 -1426.9279 -0.42200863 -0.42200863color_1_J -2343.2864 -1450.247841 -2315.1429 -0.63819087 -0.63819087clarity_1_SI2 2612.5463 -472.362109 2345.9392 1.14176901 1.14176901clarity_1_SI1 3555.6013 149.192295 3287.0687 1.38540593 1.38540593clarity_1_VS2 4210.3193 671.522828 3940.7103 1.50843614 1.50843614clarity_1_VS1 4507.7144 861.334888 4235.9431 1.58850692 1.58850692clarity_1_VVS2 4965.2210 1192.175019 4691.8855 1.66443836 1.66443836clarity_1_VVS1 5072.6388 1162.725817 4796.5710 1.61060572 1.61060572clarity_1_IF 5436.6567 1476.377453 5158.0224 1.72336616 1.72336616carat 8899.8818 7672.223943 8883.8976 1.65798106 1.65798106 We are curious as to why the Poisson regression perform much worse than a simple linear regression, when the reponse variable clearly shows Poisson patterns. 1234567891011p1 &lt;- ggplot() + geom_point(data=df_test, aes(x=as.numeric(row.names(df_test)), y=price), shape=\".\", color=\"black\") + geom_point(aes(x=as.numeric(row.names(df_test)), y=m1.pred), shape=\".\", color=\"red\")p2 &lt;- ggplot() + geom_point(data=df_test, aes(x=as.numeric(row.names(df_test)), y=price), shape=\".\", color=\"black\") + geom_point(aes(x=as.numeric(row.names(df_test)), y=exp(m4.pred)), shape=\".\", color=\"red\") We first look at the linear regression fit, where the black dots are the original data and the red dots are the fitted data: We then look at the Poisson regression fit. Unfortunately the Poisson regression create ultra-high predictions for some values, which skew the MSE matrix. This is we forget to log the numerical variable (carat) when we use log link function, which results in a exponential shape for the prediction. We change the code as follow:123456789# Fit Poisson Regressionm4 &lt;- glm(price~-carat+log(carat), df_train, family=poisson(link=\"log\"))m4.pred &lt;- predict(m4, df_test)m4.mse &lt;- round(mean((y_test - exp(m4.pred))^2), 2)# Fit quasiPoisson Regressionm5 &lt;- glm(price~-carat+log(carat)., df_train, family=quasipoisson(link=\"log\"))m5.pred &lt;- predict(m5, df_test)m5.mse &lt;- round(mean((y_test - exp(m5.pred))^2), 2) The mse are now better:1234&gt; m4.mse[1] 2544181&gt; m5.mse[1] 2544181 Plotting the prediction. We can see that other than one prediction outlier, the overall predictions are better than when we did not log the carat. Goodness of FitDeviance is a measure of the goodness of fit of a generalized linear model, similar to a RSS in the simple linear model. . The default value is called null deviance and is the deviance calculated when the response variable is predicted using its sample mean. Adding additional feature to the model would generally decrease the deviance and decrease the degree of freedoms. Tree-Based Method &#8634;The tree-based method involve segmenting the predictor space into several regions to make a prediction for a given observation. There are several advantages to the tree-based methods: Easy to explain Intuitive to human reasoning Graphic and interpretable No need to dummy variables for qualitative data On the other hand, disadvantages: Lower predictive accuracy Sensitive to change in data Several techniques can make significant improvement to compensate the disadvantages, namely bagging, random forest and boosting. Regression TreeIn a linear regression model, the underlying relationship is assumed to be: f(X) = \\beta_0 + \\sum\\beta_iX_iWhereas in a regression tree model, the underlying is assumed to be: f(X) = \\sum c_i\\textbf{1}_{X_i\\in R_i}Where each R_i represent a partition of the feature space. The goal is to solve for the partition set \\{R_i\\}_{i\\in I} which minimize the objective function: Objective\\; Function = RSS = \\sum_{i\\in I}\\sum_{X_j\\in R_i} (y_j-\\bar{y}_{R_i})^2To find \\{R_i\\}_{i\\in I} efficiently, we introduce recursive binary splitting, which is a top-down and greedy approach. It is greedy because it is short-sighted in that it always chooses the current best split, instead of the optimal split overall. Due to the greedy nature, it is preferred that we first grow a complex tree and then prune it back, so that all potential large reductions in RSS are captured. Cost Complexity PruningThe cost complexity pruning approach aim to minimize the objective function, give each value of \\alpha: Objective\\; Function = \\sum_{i\\in I_T}\\sum_{X_j\\in R_i} (y_j-\\bar{y}_{R_i})^2 + \\alpha|T|Where |T| is the number of terminal nodes of subtree T\\subset T_0 where T_0 is the original un-prune tree. The tuning parameter \\alpha controls the complexity of the subtree T, penalizing any increase in nodes. The goal is to prune the tree with various \\alpha and then use cross-validation to select the best \\alpha. This is similar to the lasso equation, which also introduce a tuning parameter \\lambda to control the complexity of a linear model. Objective\\; Function = RSS + \\lambda\\sum_{i>0}|\\beta_i|In R, we use the rpart library, which stands for recursive partitioning and regression trees, to fit a regression tree to predict mpg in our mtcars dataset.123456789101112131415library(caret)library(rpart)library(rpart.plot)set.seed(9999)df &lt;- as.data.frame(mtcars)# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$mpg, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]# fit regression treet1 &lt;- rpart(mpg ~ ., train)t1.predict &lt;- predict(t1, test)t1.mse &lt;- round(mean((test$mpg - t1.predict)^2), 2) The test MSE, as compared to the previous linear models.1234567891011# regression tree&gt; t1.mse[1] 11.59# linear regression&gt; m1.mse[1] 16.64&gt; m2.mse[1] 3.66&gt; m3.mse[1] 6.61 Plotting the tree with rpart.plot(t1) command. Classification TreeClassification tree is very similar to regression trees expect we need an alternative method to RSS when deciding a split. There are three common approaches. Classification error rate: E = 1 - \\underset{k}{max}(\\hat{p}_{mk}) Where \\hat{p}_{mk} represents the porportion of train observations from the m-th parent node that are from the k-th child node. However, the this approach is not sufficiently sensitive to node impurity for tree-growing, compared to the next two. Gini index: G = \\sum_k \\hat{p}_{mk}(1-\\hat{p}_{mk}) Note that the Gini index decreases as all \\hat{p}_{mk} get closer to 0 or 1. Therefore it is a measure of the node purity. Entropy: D = -\\sum_k \\hat{p}_{mk} log_2(\\hat{p}_{mk}) The Entropy is also a measure of the node purity and similar to the Gini index numerically. For a two-class decision tree, the impurity measures calculated from different methods for a given \\hat{p}_{mk} are simulated below with python. 12345678910111213import matplotlib.pyplot as pltimport numpy as npx = np.arange(0, 1, 0.001)y1 = x*np.log(x)/np.log(2)*-1 + (1-x)*np.log(1-x)/np.log(2)*-1y2 = x*(1-x) + (1-x)*(1-(1-x))y3 = 1-np.maximum(x, 1-x)fig = plt.figure(figsize=(6, 6))plt.plot(x, y1)plt.plot(x, y2)plt.plot(x, y3)plt.legend(['entropy', 'gini', 'class error'], loc='upper right') We can see that all three method are similar and consistent with each other. Entropy and the Gini are more sensitive to changes in the node probabilities, therefore preferrable when growing the trees. The classification error is more often used during complexity pruning. Information GainWhen the measure of node purity is calculated, we want to maximize the information gain after each split. We use P and C to denote the parent and child node, with entropy as the measure. N is the number of observations under the parent node: IG = Entropy(P) - \\sum_{k=1}^{K}\\dfrac{N_k}{N}Entropy(C_k)In R, we use the rpart library to create a classification tree to predict Survived in the Titanic data set.12345678910111213141516171819202122library(caret)library(rpart)library(rpart.plot)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]df &lt;- subset(df, select = -c(Freq))# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$Survived, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]# fit classification treet2 &lt;- rpart(Survived ~ ., train)t2.prob &lt;- predict(t2, test)t2.pred &lt;- rep(\"No\", nrow(t2.prob))t2.pred[t2.prob[, 2] &gt; .5] &lt;- \"Yes\"# pruningt2.prune &lt;- prune(t2, cp = 0.05)rpart.plot(t2.prune) The confusion matrix shows:123456789&gt; confusionMatrix(as.factor(t2.pred), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 368 105 Yes 4 72 Accuracy : 0.8015 Plotting the tree with rpart.plot(t2). Plotting the complexity parameter against the cross-validation relative error with plotcp(t2). Although the relative error is at the lowest at 5 nodes, comparable level of relative error was achieved at 2 nodes. Because decision tree is prone to overfitting, here we manually prune the tree back to 2 nodes. Plotting the tree with rpart.plot(t2.prune). The confusion matrix after pruning. We lose a small bit of out-of-sample accuracy due to manual pruning.123456789&gt; confusionMatrix(as.factor(t2.prune.pred), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 343 83 Yes 29 94 Accuracy : 0.796 Comparing the test error rate with naiveBayes, KNN and logistic regression.1234naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942classification tree: 0.1985classification tree (prune): 0.2040 Confusion MatrixThe confusion matrix is a convenient summary of the model prediction. In our previous example with the un-pruned tree: 12345678910111213&gt; confusionMatrix(as.factor(t2.pred), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 368 105 Yes 4 72# mapping the number to definition ReferencePrediction No Yes No TN FN Yes FP TP There are four types of prediction: True Positive (TP): 72 True Negative (TN): 368 False Positive (FP), or Type I Error: 4 False Negative (FN), or Type II Error: 105 Several metrics can be computed: Accuracy: (TP + TN) / N = (72 + 368) / 549 = 0.8015 Error Rate: (FP + FN) / N = 1 - Accuracy = 0.1985 Precision: TP / (Predicted Positive) = 72 / (72 + 4) = 0.9474 Sensitivity: TP / (Actually Positive) = 72 / (72 + 105) = 0.4068 Receiver Operator Characteristic CurveThe ROC curve can be used to evaluate the performacne of our model. The ROC curve plots the TPR (true positive rate) against FPR (false positive rate) over a range of cutoff values: TPR = TP / (Actually Positive) = 0.4068 FPR = FP / (Actually Negative) = 0.0107 In python, we can create a ROC curve from our previous prediction TPR and FPR:1234567891011121314import matplotlib.pyplot as pltimport numpy as npx = np.arange(0, 1, 0.0001)plt.plot(x, x)plt.xlabel('FPR')plt.ylabel('TPR')x2=0.0107y2=0.4680plt.scatter(x2, y2, s=20, color='red')plt.plot([0, x2], [0, y2], 'r-', color='red')plt.plot([x2, 1], [y2, 1], 'r-', color='red')plt.title('ROC Plot') The baseline line refers to a model/cutoff where all observations in the testing set are predicted to be positive (point x=1, y=1), or negative (point x=0, y=0). The area under the red lines and above the x-axis is an estimate of the model fit and is called AUC, or area under the ROC curve. An AUC of 1 means that the model has a TPR of 1 and FPR of 0. Ensemble MethodsBaggingThe decision tree method in general suffer from high model variance, compared to linear regression which shows low variance. Bootstrap aggregation, or bagging is a general-purpose procedure for reducing variance of a statistical model without affecting the bias. It is particular useful in the decision tree model context. For regression trees, construct B regression trees using B bootstrapped (repeatedly sampled) training sets. These trees grow deep and are not pruned, therefore having high variance. At the end, average the trees to reduce the variance. For classification trees, construct B classification trees. When predicting a test observation, take the majority classification resulted from the B trees. Note that the bagging results are more accruate but less visual. We can obtain the variable importances by computing the total RSS/Gini decreases by splits over each predictors, hence providing better interpretations of the results. Random ForestRandom forest improves upon bagged trees by de-correlating the trees. At each split, only m\\approx\\sqrt{p} predictors are considered, isolating effects on single feature with large influences. In R, use the randomForest package:123456789101112131415161718library(caret)library(randomForest)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]df &lt;- subset(df, select = -c(Freq))# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$Survived, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]# fit random forestrf &lt;- randomForest(formula = Survived ~ ., data = train, ntree = 100, importance = TRUE) In-sample confusion matrix:12345678910111213&gt; rfCall: randomForest(formula = Survived ~ ., data = train, ntree = 100, importance = TRUE) Type of random forest: classification Number of trees: 100No. of variables tried at each split: 1 OOB estimate of error rate: 23.43%Confusion matrix: No Yes class.errorNo 1063 55 0.04919499Yes 332 202 0.62172285 Out-of-sample confusino matrix:123456789&gt; confusionMatrix(as.factor(predict(rf, test)), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 368 108 Yes 4 69 Accuracy : 0.796 Comparing the test error rate with naiveBayes, KNN and logistic regression.12345naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942classification tree: 0.1985classification tree (prune): 0.2040random forest: 0.2040 BoostingBoosting is also a general-purpose procedure for improving accuracy of the statistical model. In boosting, we repeatedly fit new trees to the residuals from the previous tree, and add the new trees to the main tree such that a loss function is minimized (subject to shrinkage parameter \\lambda, typical 0.01, to control overfitting). CV is often used to determine the total number of trees to be fitted. A gradient boosting machine is an algorithm that calculates the gradient of the loss function and update the paramters such that the model moves in the direction of the negative gradient, thus closer to a minimum point of the loss function. XGBoost is an open-source software library that provides a gradient boosting framework for R. XGBoost initially started as a research project by Tianqi Chen as part of the Distributed (Deep) Machine Learning Community (DMLC) group 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465llibrary(caret)library(xgboost)library(pROC)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]df &lt;- subset(df, select = -c(Freq))# turn survival into 0 and 1Survived_Ind &lt;- rep(0, nrow(df))Survived_Ind[df$Survived == \"Yes\"] &lt;- 1df$Survived_Ind &lt;- Survived_Ind# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$Survived_Ind, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]test_2 &lt;- testtrain &lt;- subset(train, select = -c(Survived))test &lt;- subset(test, select = -c(Survived))# create model frame for xgboost inputtrain.mf &lt;- model.frame(as.formula(\"Survived_Ind ~.\"), data = train)test.mf &lt;- model.frame(as.formula(\"Survived_Ind ~.\"), data = test)# create a model matrix only contains numerical values.train.mm &lt;- model.matrix(attr(train.mf, \"terms\"), data = train)test.mm &lt;- model.matrix(attr(test.mf, \"terms\"), data = test)# [optional] create A XGB dense matrix contains an R matrix and metadatatrain.dm &lt;- xgb.DMatrix(train.mm, label = train$Survived, missing = -1)test.dm &lt;- xgb.DMatrix(test.mm, label = test$Survived, missing = -1)# create xgboost parameter listparams &lt;- list(\"booster\" = \"gbtree\", # \"gblinear\" for glm \"objective\" = \"binary:logistic\", # the output here is a probability \"eval_metric\" = \"auc\", \"eta\" = 0.1, # lambda \"subsample\" = 0.6, # proportion of observations \"colsample_bytree\" = 0.6, # proportion of features \"max_depth\" = 5) # depth of the decision tree# train xgboost modelmodel.cv &lt;- xgb.cv(params = params, data = train.dm, nrounds = 1000, # the number of trees / iterations prediction = FALSE, # storage of prediction under each tree print_every_n = 25, early_stopping_rounds = 50, maximize = TRUE, # AUC metric -&gt; maximize nfold = 6) # cv# fit final modelmodel &lt;- xgb.train(params = params, data = train.dm, nrounds = model.cv$best_iteration, prediction = FALSE)# format predictionxgb.prob &lt;- predict(model, test.dm)xgb.pred &lt;- rep(\"No\", sum(lengths(xgb.prob)))xgb.pred[xgb.prob &gt; 0.5] &lt;- \"Yes\" Feature importance:1234&gt; xgb.importance(feature_names = dimnames(train.dm)[[2]], model = model) Feature Gain Cover Frequency1: Class3rd 0.8347889 0.5957944 0.52: Class2nd 0.1652111 0.4042056 0.5 AUC result:12&gt; auc(test$Survived_Ind, xgb.prob)Area under the curve: 0.6033 Confusion matrix:123456789&gt; confusionMatrix(as.factor(xgb.pred), as.factor(test_2$Survived))Confusion Matrix and Statistics ReferencePrediction No Yes No 370 180 Yes 0 0 Accuracy : 0.6727 Unfortunately xgboost predict everything to be “No” Comparing the test error rate with naiveBayes, KNN and logistic regression.123456naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942classification tree: 0.1985classification tree (prune): 0.2040random forest: 0.2040xgboost: 0.3273 Ensemble Model InterpretationFeature Importance ranks the contribution of each feature.Partial Dependence Plots visualizes the model’s average dependence on a specific feature (or a pair of features). Unsupervised Learning &#8634;In supervised learning, we are provided a set of n observations X, each containing p features, and a response variable Y. We are interested at predicting Y using the observations and features. In unsupervised learning, we are interested at exploring hidden relationships within the data themself without involving any response variables. It is “unsupervised” in the sense that the learning outcome is subjective, unlike supervised learning in which specific metrics such as error rates are used to evaluate learning outcomes. PCASee the PCA section below. K-Mean ClusteringClustering seek to partition data into homogeneous subgroups. The K-Mean clustering partitions data into K distinct and non-overlapping clusters C, by minimizing the objective function of total in-cluster variation W(C), which is the sum of all pair-wise squared Euclidean distances between the observations in the cluster, divided by the number of observations in the cluster. \\text{minimize } \\{\\ \\sum_{k=1}^K W(C_k)\\ \\}AlgorithmThe algorithm divides data into K initial cluster, and reassign observations to the cluster with the closest cluster centroid (mean of all previous observations in the cluster). The K-Mean clustering algorithm finds a local optimum, and therefore depend on the initial cluster. So it is important to repeat the process with different initial points, and then select the best result based on minimum total in-cluster variation. The nstart parameter in the kmean function in R specifies the number of random starting centers to try and the one ended with the optimal objective function value will be selected. It is also important to check for outliers, as the algorithm would let the outlier become its own cluster and stops improving. StandardizationIt is a must to standardize the variables before performing k-mean cluster analysis, as the objective function (Euclidean distance, etc.) is calculated from the actual value of the variable. Curse of DimensionalityThe curse of dimensionality describe the problems when performing clustering on three or more dimensional space, where: visualization becomes harder as the number of dimensions increases, the Euclidean distance between data points are the same on average. The solution is to reduce the dimensionality before using clustering technique. The Elbow MethodEach cluster replaces its data with its center. In other words, with a clustering model we try to predict which cluster a data point belongs to. A good model would explain more variance in the data with its cluster assignments. The elbow method looks at the F statistics defined as: F = \\dfrac{\\text{between-group variance}}{\\text{total variance}}As soon as the additional F statistics drops/stops increasing when adding a new cluster, we use that number of clusters. Hierarchical ClusteringThe hierarchical clustering provide flexibilities in terms of the number of clusters K. It results in a tree-based representation of the data called dendrogram, which is built either bottom-up/agglomerative or top-down/divisive. Hierarchical clustering assumes that there exists a hierarchical structure. In most feature generation cases, we prefer k-means clustering instead. AgglomerativeAn agglomerative hierarchical cluster starts off by assigning each data point in its own cluster. Each step in the clustering process two similar clusters with minimum distance among all are merged, where the distance is calculated between the elements within the cluster that are closest (single-linkage) or furthest (complete-linkage) PCA - A Deeper Dive &#8634;PCA finds low dimensional representation of a dataset that contains as much as possible of the variation. As each of the n observations lives on a p-dimensional space, and not all dimensions are equally interesting. Linear Algebra ReviewLet A be a n\\times n matrix. With n=2, \\ 3, the determinant of A can be calculated as follow. det(\\begin{bmatrix} a & b \\\\ c & d \\\\ \\end{bmatrix} ) = ad - bc\\begin{align} det ( \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\\\ \\end{bmatrix} ) &= aei + bfg + cdh - cdg - bdi - afh \\\\ &= a \\times det( \\begin{bmatrix} e & f \\\\ h & i \\\\ \\end{bmatrix} ) + b \\times det( \\begin{bmatrix} d & f \\\\ g & i \\\\ \\end{bmatrix} ) + c \\times det( \\begin{bmatrix} d & e \\\\ g & h \\\\ \\end{bmatrix} ) \\end{align}Properties of determinant: \\begin{align} det(A^T) &= det(A) \\\\ det(A^{-1}) &= det(A)^{-1} \\\\ det(AB) &= det(A)det(B) \\end{align}A real number \\lambda is an eigenvalue of A if there exists a non-zero vector x (eigenvector) in \\mathbb{R}^n such that: Ax = \\lambda xThe determinant of matrix A - \\lambda I is called the characteristic polynomial of A. The equation det(A - \\lambda I) is called the characteristic equation of A, where the eigenvalues \\lambda are the real roots of the equation. It can be shown that: \\prod_{i=1}^n \\lambda_i = det(A) \\\\ \\sum_{i=1}^n \\lambda_i = \\sum_{i=1}^n a_{i, \\ i} = trace(A)Matrix A is invertible if there exists a n\\times n matrix B such that AB = BA = I. A square matrix is invertible if and only if its determinant is non-zero. A non-square matrix do not have an inverse. Matrix A is called diagonalizable if and only if it has linearly independent eigenvectors. Let \\textbf{U} denote the eigen vectors of A and \\textbf{D} denote the diagonal \\lambda vector. Then: A = \\textbf{UDU}^{-1} \\rightarrow A^x = \\textbf{UD}^x\\textbf{U}^{-1}If matrix A is symmetric, then: all eigenvalues of A are real numbers all eigenvectors of A from distinct eigenvalues are orthogonal Matrix A is positive semi-definite if and only if any of the following: for any n\\times 1 matrix x, x^TAx \\geq 0 all eigenvalues of A are non-negative all the upper left submatrices A_K have non-negative determinants. Matrix A is positive definite if and only if any of the following: for any n\\times 1 matrix x, x^TAx > 0 all eigenvalues of A are positive all the upper left submatrices A_K have positive determinants. All covariance, correlation matrices must be symmetric and positive semi-definite. If there is no perfect linear dependence between random variables, then it must be positive definite. Let A be an invertible matrix, the LU decomposition breaks down A as the product of a lower triangle matrix L and upper triangle matrix U. Some applications are: solve Ax=b: LUx=b \\rightarrow Ly=b \\text{ ; } Ux=y solve det(A): det(A) = det(L)\\ det(U)=\\prod L_{i, \\ i}\\prod U_{j, \\ j} Let A be a symmetric positive definite matrix, the Cholesky decomponsition expand on the LU decomposition and breaks down A=U^TU, where U is a unique upper triangular matrix with positive diagonal entries. Cholesky decomposition can be used to generate correltaed random variables in Monte Carlo simulation Matrix InterpretationConsider a n\\times p matrix: \\begin{bmatrix} x_{11} & x_{12} & x_{13} & \\dots & x_{1p} \\\\ x_{21} & x_{22} & x_{23} & \\dots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & x_{n3} & \\dots & x_{np} \\end{bmatrix}To find the first principal component F^1, we define it as the normalized linear combination of X that has the largest variance, where its loading \\phi^1_j are normalized: \\sum^p_{j=1} (\\phi^1_j)^2 = 1 F^1 = \\phi^1_1X_1 + \\phi^1_2X_2 + \\dots + \\phi^1_pX_pOr equivalently, for each score: F^1_i = \\sum_{j=1}^{p} \\phi^1_jx_{ij} In matrix form: \\begin{bmatrix} x_{11} & x_{12} & x_{13} & \\dots & x_{1p} \\\\ x_{21} & x_{22} & x_{23} & \\dots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & x_{n3} & \\dots & x_{np} \\end{bmatrix}\\times \\begin{bmatrix} \\phi^1_1 \\\\ \\phi^1_2 \\\\ \\vdots \\\\ \\phi^1_p \\\\ \\end{bmatrix}= \\begin{bmatrix} f^1_1 \\\\ f^1_2 \\\\ \\vdots \\\\ f^1_n \\\\ \\end{bmatrix}Finally, the first principal component loading vector \\phi^1 solves the optimization problem that maximize the sample variance of the scores f^1. An objective function can be formulated as follow and solved via an eigen decomposition: \\text{maximize }\\{\\ \\dfrac{1}{n}\\sum_{i=1}^n(f^1_i)^2\\ \\} \\text{ subject to } \\sum^p_{j=1} (\\phi^1_j)^2 = 1To find the second principal component loading \\phi^2, use the same objective function with \\phi^2 replacement and include an additional constraint that \\phi^2 is orthogonal to \\phi^1. Geometric InterpretationThe p\\times k loading matrix L = [\\phi^1 \\dots \\phi^k] defines a linear transformation that projects the data from the feature space \\mathbb{R}^p into a subspace \\mathbb{R}^k, in which the data has the most variance. The result of the projection is the factor matrix F = [F^1 \\dots F^k], also known as the principal components. \\underbrace{ \\begin{bmatrix} x_{11} & x_{12} & x_{13} & \\dots & x_{1p} \\\\ x_{21} & x_{22} & x_{23} & \\dots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & x_{n3} & \\dots & x_{np} \\end{bmatrix} }_{data} \\times \\underbrace{ \\begin{bmatrix} \\phi^1_1 & \\dots & \\phi^k_1\\\\ \\phi^1_2 & \\dots & \\phi^k_2\\\\ \\vdots & \\ddots & \\vdots \\\\ \\phi^1_p & \\dots & \\phi^k_p\\\\ \\end{bmatrix} }_{loadings}= \\underbrace{ \\begin{bmatrix} f^1_1 & \\dots & f^k_1 \\\\ f^1_2 & \\dots & f^k_2 \\\\ \\vdots & \\ddots & \\vdots \\\\ f^1_n & \\dots & f^k_n \\\\ \\end{bmatrix} }_{\\text{principal components} \\\\ \\text{or, factor scores}}In other words, the principal components vectors F^1 ... F^k forms a low-dimensional linear subspace that are the closest (shortest average squared Euclidean distance) to the observations. Eigen DecompositionGiven n\\times p data matrix X, the objective of PCA is to find a lower dimension representation factor matrix F, from which a n\\times p matrix \\tilde{X} can be constructed where distance between the covariance matrices cov(X) and cov(\\tilde{X}) are minimized. The covariance matrix of X is a p\\times p symmetric positive semi-definite matrix, therefore we have the following decomposition where \\textbf{u}‘s’ are p\\times 1 eigenvectors of cov(X) and \\lambda‘s are the eigenvalues. Note that \\textbf{u} can be a zero vector if the columns of cov(X) are linearly dependent. \\begin{align} cov(X) &= \\dfrac{1}{n-1}X^TX \\\\ &=\\dfrac{1}{n-1} \\begin{bmatrix} \\textbf{u}_1 & \\dots & \\textbf{u}_p \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & \\lambda_p \\end{bmatrix} \\begin{bmatrix} \\textbf{u}_1 & \\dots & \\textbf{u}_p \\end{bmatrix}^T \\\\ &= \\dfrac{1}{n-1}\\sum_{i=1}^p \\lambda_i\\textbf{u}_i\\textbf{u}_i^T \\end{align}If we ignore the constant 1/(n-1), and define the p\\times p loading matrix L_0=[\\textbf{u}_1, \\dots, \\textbf{u}_p] and n\\times p factor matrix F_0 where F_0^TF_0=\\Lambda. Then: X = F_0L_0^TNow comes the PCA idea: Let’s rank the \\lambda_i‘s in descending order, pick k < p such that: \\dfrac{1}{n-1}\\sum_{i=1}^{k} \\lambda_i\\textbf{u}_i\\textbf{u}_i^T \\approx \\dfrac{1}{n-1}\\sum_{i=1}^p \\lambda_i\\textbf{u}_i\\textbf{u}_i^T = cov(X) \\\\ \\text{and we denote it } cov(\\tilde{X}) \\text{, i.e. } cov(\\tilde{X}) = \\dfrac{1}{n-1}\\sum_{i=1}^{k} \\lambda_i\\textbf{u}_i\\textbf{u}_i^TNow we observe that the matrix cov(\\tilde{X}) is also a p\\times p positive semi-definite matrix. Following similar decomposition, we obtain a p\\times k matrix L and n\\times k matrix F, where: \\tilde{X} = FL^THere we have it, a dimension-reduced n\\times k factor matrix F, where its projection back to n\\times p space, \\tilde{X}, has similar covariance as the original n\\times p dataset X. Practical ConsiderationsPCA excels at identifying latent variables from the measurable variables. PCA can only be applied to numeric data, while categorical variables need to be binarized beforehand. Centering: yes. Scaling: if the range and scale of the variables are different, correlation matrix is typically used to perform PCA, i.e. each variables are scaled to have standard deviation of 1 otherwise if the variables are in the same units of measure, using the covariance matrix (not standardizing) the variables could reveal interesting properties of the data Uniqueness: each loading vector \\phi^1 is unique up to a sign flip, as the it can take on opposite direction in the same subspace. Same applies to the score vector Z^1, as var(Z^1) = var(-Z^1) Propotional of Variance Explained: we can compute the total variance in a data set in the first formula below. The variance explained by the m-th principal component is: \\dfrac{1}{n} \\sum_{i=1}^n (z^m_i)^2. Therefore, the second formula can be computed for the PVE: \\sum_{j=1}^p Var(X_j) = \\sum_{j=1}^p [ \\dfrac{1}{n} \\sum_{i=1}^n x^2_{ij} ] \\\\ PVE^m = \\dfrac{\\sum_{i=1}^n (z^m_i)^2}{\\sum_{j=1}^p\\sum_{i=1}^n x^2_{ij}} A Side Note on Hypothesis Test and Confidence Interval &#8634;A formal statistical data analysis includes: hypothesis testings: seeing whether a structure is present confidence interval: setting error limits on estimates prediction interval: setting error limits on future outcomes We will discuss a typical setting as follow: Let \\beta be the unknown parameter, and \\hat{\\beta}_n is the parameter estimator based on n observations Let \\hat{\\sigma}^2_n be the estimator of \\sigma^2_n, equal to var(\\sqrt{n}(\\hat{\\beta}_n - \\beta)) In rare circumstances where \\sqrt{n}(\\hat{\\beta}_n - \\beta)/\\hat{\\sigma} is independent of the unknown \\beta, such as in regression settings where the error terms \\epsilon_i are i.i.d. normally distributed, we can show that it follows a t-distribution with n-1 degree of freedom. \\dfrac{\\sqrt{n}(\\hat{\\beta}_n - \\beta)}{\\hat{\\sigma}} \\sim t_{n-1} However, asymptotically as n\\rightarrow\\infty, the Central Limit Theorem applies which release us from the assumption restriction above. Under a variety of regular conditions: \\lim_{n\\rightarrow\\infty} \\dfrac{\\sqrt{n}(\\hat{\\beta}_n - \\beta)}{\\hat{\\sigma}} \\sim \\mathcal{N}(0, 1) The 1-\\alpha confidence interval can then be computed as: \\beta \\in CI = \\hat{\\beta}_n \\pm t^{\\alpha}_{n-1}\\hat{\\sigma}_n/\\sqrt{n} \\\\ \\text{or asymptotically, swapping } t^{\\alpha}_{n-1} \\text{ by } z^{\\alpha} The 1-\\alpha hypothesis test of \\beta=\\beta_0 would: accept H_0 that \\beta=\\beta_0 if \\beta \\in CI reject H_0 that \\beta=\\beta_0 if \\beta \\notin CI The p-value is \\alpha such that the hypothesis test is indifference between accept or reject. In practice, t-distribution is typically used for regression data as it is more conservative (t^{\\alpha}_{n-1} > z^{\\alpha}). For non-regression data, normal distribution is often used. Some background on LLN and CLT: Given n i.i.d. random variable X_i. Let \\bar{X} = \\sum X_i/n The Law of Large Numbers states that if \\mathbb{E}|X|","tags":"math"},{"title":"About","url":"/2019/01/about/","text":"","tags":"travel"}]}