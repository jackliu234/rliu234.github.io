{"pages":[{"title":"Can We Optimize Buy-In Size with Kelly's Criterion?","url":"/2019/11/kelly/","text":"The Kelly&#39;s Criterion, famous for its various application in sports betting and asset management, is detailed in a paper that J. L. Kelly published in 1956 while working under the Bell lab. In a situation where a gambler places repeated bets on an event with success probability of p\\%, Kelly proves that the optimal bet size each time is (2p-1)\\% of the gambler’s total capital. This is due to the fact that as the bets continues, the logarithm of gambler’s wealth is concave with respective to the bet size. Quick ProofLet p denote the true probability of an event which the gambler bets on. V_0 and V_N denotes the initial capital and capital after N bettings, and B denotes the percentage bet size relative to the capital. Among the N bets, W denotes the number of successes. Therefore: V_N = (1+B)^W(1-B)^{(N-W)}V_0Taking the logarithm of both sides and then take derivative w.r.t. B: \\log{V_N} = W\\log{(1+B)} + (N-W)\\log{(1-B)} + \\log{V_0} \\\\ \\dfrac{\\partial}{\\partial B} \\log{V_N} = \\dfrac{W}{1+B} - \\dfrac{N-W}{1-B}Taking the second derivative and we can see that \\log{V_N} is concave w.r.t. B: \\dfrac{\\partial^2}{\\partial B^2} \\log{V_N} = - \\dfrac{W}{(1+B)^2} - \\dfrac{N-W}{(1-B)^2} < 0Setting the partial derivative to zero we can then solve for B' that maximizes \\log{V_N}: B' = 2\\dfrac{W}{N} - 1As N goes to infinity, the optimal bet size becomes 2p-1: \\lim_{N\\rightarrow\\infty} B' = 2p - 1No-Limit Hold’em ApplicationSuppose we have a bankroll of 2000 and wants to play 1/2 No-Limit Hold’em. A standard buy-in size is 100BB which amounts to 200. Suppose we have an inherent edge in this game, how can we vary[1] the buy-in to maximize our long-term profit based on the Kelly’s Criterion? Let’s say we adopt a play style where we play very tight and always all-in pre-flop. We continues until someone calls our all-in and will exit the game no matter the outcome. For now, we will ignore the blinds we are losing by waiting for a hand, since we earn some blinds when players fold to our all-ins. This game now becomes very similar to the gambler situation above, that we either double our buy-in or lose it. Since we assumed that we had an inherent edge[2] p>50\\%, we should be always betting a buy-in of (2p-1)\\% of our bankroll. But how do we know our edge? We know that sample mean is an unbiased estimator of population mean. We may use our average historical win percentage[3] as an estimate. Suppose we played 10 sessions first all with standard buy-in and made 200 in profits. Then our edge can be calculated: \\hat{p} = \\dfrac{1}{n}\\sum p_i = \\dfrac{1}{10} \\dfrac{2200}{400} = 55\\%Based on Kelly’s Criterion, in the 11th session, we want to bet 10\\% of our total bankroll of 2200, which is a buy-in of 220. Now suppose we win the 11th session, then: \\hat{p} = \\dfrac{1}{n}\\sum p_i = \\dfrac{1}{11} (\\dfrac{2200}{400}+\\dfrac{440}{440}) = 59\\%Our next optimal buy-in will become 2420 * 18\\% = 435.6. LimitationsIn the scenario above, we can observe large swings in the estimated win rate and updated optimal buy-in amount after a single session. This is because the high variance of the aggressive, all-in-only play style we adopted. In normal poker plays, the variance will be much lower and as the play history grows, the updates will become incremental. The question now becomes whether Kelly’s Criterion still applies if there are more than 2 outcomes. In fact, Kelly had made a general case for multiple outcome scenarios in his paper. I will continue this exploration in a future post. [1]: We are also making the assumption that varying buy-in will not change our inherent edge/win rate. This assumption can be supported by implementing a floor of 100BB and a cap to our buy-in, no matter what Kelly’s Criterion suggests. [2]: In reality, this play style is difficult to earn an edge, as players will only call our all-in with an even tighter range, causing our win rate to drop below 50%. [3] Note that this only works if our winning is i.i.d. However, as we sit at a table longer, players will be more familiar with our strategy and therefore negatively affect our win rate.","tags":"math"},{"title":"How to Set Up A Photo Library with Raspberry Pi","url":"/2019/10/pi/","text":"Recent I brought a Raspberry Pi 4 and started playing with it. I first moved my Bitcoin algorithmic trading strategy from a VPS to the Pi, which dropped the monthly VPS fee from my credit card bill. My second project is to create a photo library hosted through HTTP, where I can share pictures with my family on the other side of the Pacific ocean. Comparing to other personal cloud storages, this project come with several great advantages: easy to use (only requires a link and a browser) cheap (can use an idle hard drive as storage) customizable (easy to change the ascetics and layout) It is also a great project for one learn about the inner working of computer networking as well as front-end developing. So let’s get started. First we will need a Raspberry Pi 4. I installed the Raspbian GUI operating system on it which is a pretty neat Linux environment to work on. Next you will have to set up NGINX and PHP. NGINX will allow us to host web server on Pi and PHP is a back-end programming language that is useful in interacting with the operating system. For this project we will need to write a PHP script which will go through all designated folders to grab all images automatically. Next up we want to set up SSH so that we can access the Pi terminal through local networks. Then we will use the rsync tool to synchronize the pictures from my laptop to the Pi folder.1rsync -avz -e ssh desktop/image pi@xx.xx.xx.xx:server Here I have a /image folder set up on my Mac desktop, and I want to sync with the /home/pi/server/image folder on my Pi. Use cron to schedule this command if needed. Now that we have all the images sync-ed over to your Pi, we can start NGINX and host a static website. Here is my setup. We run our web service on standard port 80 for HTTP and 443 for HTTPS (HTTPS is HTTP with SSL encryption, which is far more secure)/etc/nginx/sites-enable/server12345678910111213141516171819202122232425server &#123; listen 80; server_name _; return 301 https://$host$request_uri;&#125;server &#123; listen 443; server_name _; root /home/pi/server; index index.php; location / &#123; try_files $uri $uri/ =404; &#125; location ~ \\.php$ &#123; include snippets/fastcgi-php.conf; fastcgi_pass unix:/var/run/php/php7.3-fpm.sock; &#125; location ~ /.well-known &#123; allow all; &#125;&#125; /home/pi/server/index.php12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to My Photo Library!&lt;/title&gt; &lt;style&gt; body &#123; width: auto; margin-left: 2em; margin-right: 2em; font-family: Garamond, serif; &#125; * &#123; margin: 0; padding: 0; &#125; img &#123; image-orientation: from-image; &#125; .imgbox &#123; display: grid; height: 100%; &#125; .center-fit &#123; max-width: 100%; max-height: 100vh; margin: auto; &#125; li &#123; list-style-type:none; margin-right:10px; margin-bottom:10px; float:left; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Welcome to My Photo Library!&lt;/h1&gt; &lt;div class=\"imgbox\"&gt; &lt;ul&gt; &lt;?php $handle = opendir(dirname(realpath(__FILE__)).'/image/'); while($file = readdir($handle))&#123; if($file !== '.' &amp;&amp; $file !== '..' &amp;&amp; $file !== '.DS_Store')&#123; echo '&lt;img class=\"center-fit\" src=\"image/'.$file.'\"&gt;'; &#125; &#125; ?&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; Now open up &lt;local ip address of my pi&gt;:443 on my Macbook connected to the same WLAN as my Pi, I will be able to see the webpage I host. Next we need to configure our router for Port Forwarding and then set up a Dynamic DNS service. The No-IP website currently provides free DDNS service. After setting up DDNS, we can then use the hostname from your DDNS provider to access the dynamic IP[1] address of our router, which will be forwarded to the static IP of our raspberry pi with the port that NGINX used to host the website. For example, we just need to type https://&lt;my hostname from ddns provider&gt;:&lt;my router port designated to forward&gt; in the browser to view the photo library anywhere in the world:) [1] In my case my ISP uses Carrier-Grade NAT, which means my public IP is different than my WAN IP. I will need to configure the DDNS provider to access my WAN IP, in order for this to work.","tags":"tech"},{"title":"Understand Gradient Descent in Neural Network","url":"/2019/08/gradient-descent/","text":"Gradient Descent is an optimization method used in neural network, where the weight parameters \\boldsymbol{w} are updated recursively by subtracting a small percentage \\alpha of the gradient of the loss function \\nabla \\mathcal{L}, in order to minimize the loss function. \\boldsymbol{w}_i = \\boldsymbol{w}_{i-1} - \\alpha\\nabla \\mathcal{L}(\\boldsymbol{w}_{i-1}) MathematicsLet’s prove that the gradient descent method would lead to smaller losses after each step. Without loss of generality, we assume that the initial weight parameter \\boldsymbol{w}_0 is a two dimensional vector, (x_0, y_0). Given a new vector (x, y) that are close to (x_0, y_0), the Taylor series expansion of the loss function \\mathcal{L} can be approximated by the first-order partial derivatives only: \\begin{align} \\mathcal{L}(x, y) &= \\mathcal{L}(x_0, y_0) + \\dfrac{\\partial\\mathcal{L}(x_0, y_0)}{\\partial x}(x - x_0) + \\dfrac{\\partial\\mathcal{L}(x_0, y_0)}{\\partial y}(y - y_0) \\\\ &= \\mathcal{L}(x_0, y_0) + \\nabla \\mathcal{L}(x_0, y_0) \\cdot (\\Delta x, \\Delta y) \\end{align}Since we would like to find x and y that minimize \\mathcal{L}(x, y), which is the same as minimizing the dot product above, with the constraint that (x, y) having close Eulidean distance to (x_0, y_0) in order to satisfy the Taylor approximation. |(\\Delta x, \\Delta y)| \\leq \\epsilonTo achieve this we select the vector (x, y) such that: (\\Delta x, \\Delta y) = - \\alpha \\nabla \\mathcal{L}(x_0, y_0) \\text{, or simply} \\\\ (x, y) = (x_0 - \\alpha\\cdot\\dfrac{\\partial\\mathcal{L}(x_0, y_0)}{\\partial x}, y_0 - \\alpha\\cdot\\dfrac{\\partial\\mathcal{L}(x_0, y_0)}{\\partial y})Given an \\epsilon, we choose \\alpha such that the above constraint is satisfied. The negative sign ensure that the dot product is minimized. We therefore have proved that \\mathcal{L}(x, y) will descend at each iteration provided that the loss function is differentiable and a sufficiently small \\alpha is used. ExampleLet’s say we have a training set of 4 that maps three binary features to a binary response. \\begin{bmatrix} 0 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ \\end{bmatrix}We first observe that the first feature has a 100% correlation with the response and can reasonably be used for future predictions. Now we construct a neural network to see if it can capture this relationship. First create a neural network class and randomly initialize three weights between -1 and 1 for each feature. 12345import numpy as npclass NeuralNetwork(): def __init__(self): np.random.seed(1) self.weights = 2 * np.random.random((3, 1)) - 1 Define the sigmoid activation function \\sigma: \\hat{y} = \\sigma(wx) = \\dfrac{1}{1 + e^{-wx}}12def sigmoid(self, x): return 1 / (1 + np.exp(-x)) Define the loss function as the mean square error: \\mathcal{L}(w) = (\\hat{y} - y)^2Calculate the gradient w.r.t. weights w: \\dfrac{\\partial \\mathcal{L}(w)}{\\partial w} = 2(\\hat{y} - y)\\dfrac{\\partial \\hat{y}}{\\partial w} = 2(\\hat{y} - y) \\ \\hat{y}(1-\\hat{y})\\cdot x^T12def gradient(self, x, y, y_hat): return np.dot(x.T, (2 * (y_hat - y) * (y_hat * (1 - y_hat)))) Forward and backward propogation.123456789def forward_propogation(self, x): x = inputs.astype(float) return self.sigmoid(np.dot(x, self.weights))def backward_propogation(self, x, y, alpha=1, iterations=10000): for i in range(iterations): y_hat = self.forward_propogation(x) self.weights -= alpha * self.gradient(x, y, y_hat) Testing our initial hypothesis.12345678910111213141516171819if __name__ == \"__main__\": nn = NeuralNetwork() print('\\nrandom synoptic weights') print(nn.weights) x = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) y = np.array([[0, 1, 1, 0]]).T nn.backward_propogation(x, y) print('\\nweights after training') print(nn.weights) outputs = nn.forward_propogation(x) print('\\noutput after training') print(outputs) 123456789101112131415random synoptic weights[[-0.16595599] [ 0.44064899] [-0.99977125]]weights after training[[10.38061249] [-0.20642264] [-4.98461681]]output after training[[0.0067959 ] [0.99445652] [0.99548577] [0.00553541]] We can see that the neural network learns to put substantial weights on the first feature and makes very accurate predictions in-sample with 10,000 iterations.","tags":"math"},{"title":"&#128214; Notes on ISLR","url":"/2019/04/islr/","text":"This is a study note on the book An Introduction to Statistical Learning with Applications in R, with my own experimental R-code for each topic. Cheers! Navigation00. Introduction01. Linear Model02. Tree-Based Method03. Unsupervised Learning04. PCA - A Deeper Dive05. A Side Note on Hypothesis Test and Confidence Interval Introduction &#8634;Suppose we observe a quantitative response Y and p different predicting variables X = (X_1, X_2...X_p). We assume that there is a underlying relationship f between Y and X: Y=f(X) + \\epsilonWe want to estimate f mainly for two purpose: prediction: in the case where Y is not easily obtained, we want to estimate f with \\hat{f}, and use \\hat{f} to predict Y with \\hat{Y}. Here \\hat{f} can be a black box, such as highly non-linear approaches which offers accuracy over interpretability. \\hat{Y} = \\hat{f}(X) inference: in the case where we are more interested in how Y is affected by the change in each X_n. We need to know the exact form of \\hat{f}. For example, linear model is often used which offer interpretable inference but sometimes inaccurate. FeatureThere are important and subtle differences between a feature and a variable. variable: raw data feature: data that is transformed, derived from raw data. A feature can be more predictive and have a direct relationship with the target variable, but it isn’t immediately represented by the raw variable. The need for feature engineering arises from limitations of modeling algorithms: the curse of dimensionality (leading to statistical insiginificance) the need to represent the signal in a meaningful and interpretable way the need to capture complex signals in the data accurately computational feasibility when the number of features gets large. Feature TransformationThe Occam’s Razor principle states that a simpler solutions are more likely to be corret than complex ones. Consider the following example of modeling a exponentially distributed response. After applying a log transformation, we can view the relation from a different viewpoint provided by the new feature space, in which a simpler model may achieve more predictive power than a complex model in the original input space.123456789x1 &lt;- runif(100, 1,10)x2 &lt;- exp(x1)df &lt;- data.frame(x1 = x1, x2 = x2)df$logx2 &lt;- log(df$x2)ssoptions(repr.plot.width=6, repr.plot.height=3)p1 &lt;- ggplot(data = df, aes(x = x1, y = x2)) + geom_point(size=0.3)p2 &lt;- ggplot(data = df, aes(x = x1, y = logx2)) + geom_point(size=0.3)grid.arrange(p1, p2, nrow=1) Consider another classification problem, in which we want to identify the boundary between the two classes. A complex model would draw a circle as the divider. A simpler approach would be to create a new feature with distances of each point from the origin. The divider becomes a much simpler straight line.123456789101112131415161718x1 &lt;- runif(1000,-1,1)x2 &lt;- runif(1000,-1,1)class &lt;- ifelse(sqrt(x1^2 +x2^2) &lt; 0.5, \"A\", \"B\")df &lt;- data.frame(x1 = x1, x2 = x2, class = class)p1 &lt;- ggplot(data = df, aes(x = x1, y = x2, color = class)) + geom_point(size=1)df$dist_from_0 &lt;- sqrt(df$x1^2 + df$x2^2)p2 &lt;- ggplot(data = df, aes(x = 0, y = dist_from_0, color = class)) + geom_point(position = \"jitter\", size=1) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) + annotate(\"segment\", x = -0.5, xend = 0.5, y = 0.5, yend = 0.5)options(repr.plot.width=8, repr.plot.height=3)grid.arrange(p1, p2, nrow=1) Feature SelectionFor p predictive variables, there are a total of 2^p models. We use feature selection to choose a smaller subset of the variable to model. Forward Selection We begin with the null model with no variables and an intercept. We then fit n simple linear regression to choose our first variable with the lowest RSS. Same way to choose the next variable to be added until some stoppoing rule. Backward Selection We begin with the full model and remove the variable with the largest coefficient p-value. Re-fit and remove the next. Mixed Selection We begin with the null model and the forward selection technique. Whenever the p-value for a variable exceeds a threshold we remove it. Regression ProblemsIn regression problems the variables are quantitative, we use mean squared error, or MSE, to measure the quality of estimator \\hat{f}: MSE = \\dfrac{\\sum_{i=1}^n [y_i - \\hat{f}(x_i)]^2}{n}A fundamental property of statistical learning that holds regardless of the particular dataset and statistical method is that as model flexibility increases, we can observe a monotone decrease in training MSE and an U-shape in test MSE. Bias vs VarianceThe bias-variance trade off decompose the expected test MSE of a single data point x_0: \\begin{align} \\mathbb{E}MSE^{test}(x_0) &= \\mathbb{E}[y_0 - \\hat{f}(x_0)]^2 \\\\ &= Var[\\hat{f}(x_0)] + Bias[\\hat{f}(x_0)]^2 + Var[\\epsilon] \\end{align}where: variance refers to the variance of the estimator among different datasets. A highly flexible \\hat{f} lead to a high variance, as even small variance in data induce change in the \\hat{f}‘s form. bias refers to the error due to estimator \\hat{f} inflexibility. For example, using linear \\hat{f} to estimate non-linear relationship leads to high bias. Classification ProblemsIn regression problems the variables are qualitative, we use error rate to measure the quality of estimator \\hat{f}: \\text{error rate} = \\dfrac{\\sum_{i=1}^n \\textbf{1}\\{y_i \\neq \\hat{f}(x_i)\\}}{n}The Bayes ClassifierThe Bayes classifier predict the classification based on the combination of the prior probability and its likelihood given predictor values. With categories C_1, C_2, C_3..., and predictor values \\textbf{x} = (x_1, x_2, x_3...), \\hat{y} is assigned to category C_k which has the maximum posterior probability: \\hat{y} = \\hat{f}^{Bayes}(\\textbf{x}) = C_k, \\;where\\; k = argmax_k \\;p(C_k | \\textbf{x})Where: p(C_k | \\textbf{x}) = \\dfrac{p(x_1, x_2, x_3...|C_k)p(C_k)}{p(x_1, x_2, x_3...)}The naive Bayes classifier assumes independence between the predictor X_i‘s, and the formula becomes: \\hat{y} = \\hat{f}^{naiveBayes}(\\textbf{x}) = C_k, where\\; k = argmax_k \\;p(C_k) \\times \\prod p(x_i | C_k)When x_i is continuous, the Guassian naive Bayes classifier assumes that p(x_i | C_k) \\sim \\mathcal{N}(\\mu_{i, C_k}, \\sigma^2_{i, C_k}) In R, we use the naiveBayes function from the e1071 package to predict Survival from the Titanic dataset.1234567891011121314151617181920library(e1071)library(caret)set.seed(9999)df=as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq),]df$Freq &lt;- NULL# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$Survived, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]# fit naive bayes classifierbayes &lt;- naiveBayes(Survived ~ ., data=train)# 10-fold validationbayes.cv &lt;- train(Survived ~ ., data=train, method = \"nb\", trControl = trainControl(method = \"cv\", number = 10)) Viewing the model results, the prior probabilities p(C_k) are shown in the “A-priori probabilities” section.1234567891011&gt; bayesNaive Bayes Classifier for Discrete PredictorsCall:naiveBayes.default(x = X, y = Y, laplace = laplace)A-priori probabilities:Y No Yes0.6767554 0.3232446 The likelihood p(x_i | C_k) are shown in the “Conditional probabilities” section123456789101112131415Conditional probabilities: ClassY 1st 2nd 3rd Crew No 0.09481216 0.10554562 0.34615385 0.45348837 Yes 0.28838951 0.15730337 0.24344569 0.31086142 SexY Male Female No 0.91323792 0.08676208 Yes 0.53183521 0.46816479 AgeY Child Adult No 0.03130590 0.96869410 Yes 0.06928839 0.93071161 The confusionMatrix function from the caret package returns a test Accuracy of 0.7978, which corresponds to an Bayes error rate of 0.2023. \\text{error rate}^{Bayes} = 1 - \\mathbb{E}max_k\\;p[Y=C_k|X=(x_1, x_2, x_3...)]Theoretically, the Bayes classifier produces the lowest error rate if we know the true conditional probability p(C_k | \\textbf{x}), which is not the case with real data. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.123456789&gt; confusionMatrix(predict(bayes, test), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 343 82 Yes 29 95 Accuracy : 0.7978 With 10-fold cross validation, the test error rate is 0.2095.123456789&gt; confusionMatrix(predict(bayes.cv, test), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 332 75 Yes 40 102 Accuracy : 0.7905 K-Nearest NeighborsThe KNN classifier estimate the conditional probability p(C_k | \\textbf{x}) based on the set of K predictors in the training data that are the most similar to \\textbf{x}, representing by \\mathcal{N}. \\hat{y} = \\hat{f}^{KNN}(\\textbf{x}) = C_k, \\;where\\; k = argmax_k \\;p(C_k | \\textbf{x}) \\\\ with\\; p(C_k | \\textbf{x}) = \\dfrac{\\sum_{i\\in\\mathcal{N}}\\textbf{1}_{y_i = C_k}}{K}Note that the p(C_k | \\textbf{x}) is one minus the \\mathcal{N}-local error rate of a C_k estimate , and therefore with KNN we are picking the C_k that minimizes the \\mathcal{N}-local error rate given \\textbf{x}. When K=1, the estimator \\hat{f}^{KNN} produces a training error rate of 0, but the test error rate might be quite high due to overfitting. The method is therefore very flexible with low bias and high variance. As K\\rightarrow\\infty, the estimator becomes more linear. In R, we use the knn function (with K=1) in the class library to predict Survival from the Titanic dataset.1234567891011121314151617181920212223242526272829303132library(e1071)library(class)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq),]df$Freq &lt;- NULL# The \"knn\" function in the \"class\" library only works with numeric datadf$iClass &lt;- as.integer(df$Class)df$iSex &lt;- as.integer(df$Sex)df$iAge &lt;- as.integer(df$Age)df$iSurvived &lt;- as.integer(df$Survived)# Create random 75/25 train/test splittrain_ind &lt;- sample(seq_len(nrow(df)), size = floor(0.75 * nrow(df)))df_train &lt;- df[train_ind, c(5:7)]df_test &lt;- df[-train_ind, c(5:7)]df_train_cl &lt;- df[train_ind, 4]df_test_cl &lt;- df[-train_ind, 4]# Fit KNNknn &lt;- knn(train = df_train, test = df_test, cl = df_train_cl, k=1)# 10-fold CVknn.cv &lt;- tune.knn(x = df[, c(5:7)], y = df[, 4], k = 1:20, tunecontrol=tune.control(sampling = \"cross\"), cross=10) The confusion matrix shows an error rate of 0.1942.123456789&gt; confusionMatrix(knn, df_test_cl)Confusion Matrix and Statistics ReferencePrediction No Yes No 364 101 Yes 6 80 Accuracy : 0.8058 Now run KNN with the tune wrapper to perform 10-fold cross validation. The result recommends KNN with K=1, which turns out to be the same as what we originally tested.1234567891011&gt; knn.cvParameter tuning of ‘knn.wrapper’:- sampling method: 10-fold cross validation- best parameters: k 1- best performance: 0.2157576 Summarizing the test error rate for naiveBayes and KNN.12naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942 Linear Model &#8634;Simple Linear RegressionA simple linear regression assumes that: Y \\sim \\beta_0 + \\beta_1XCoefficient EstimateGiven data points (x_i, y_i), let \\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i. We define the residual sum of squares, or RSS, as: RSS = \\sum (y_i - \\hat{y_i})^2Minimizing RSS as an objective function, we can solve for \\hat{\\beta_0}, \\hat{\\beta_1}: \\begin{align} \\hat{\\beta_1} &= \\dfrac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i-\\bar{x})^2} \\\\ \\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1}\\bar{x} \\end{align}Coefficient Estimate - Gaussian ResidualFurthermore, if we assume the individual error terms are i.i.d Gaussian, i.e.: Y = \\beta_0 + \\beta_1x + \\epsilon \\\\ \\text{where, } \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)We now have a conditional pdf of Y given x: p(y_i|x_i; \\beta_0, \\beta_1, \\sigma^2) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\dfrac{[y_i - (\\beta_0 + \\beta_1x_i)]^2}{2\\sigma^2}}The maximum log-likelihood estimator \\hat{l}for the paramter estimate b_0, b_1, and s^2 can be computed as follow: \\begin{align} \\hat{l}(b_0, b_1, s^2| x_i, y_i) &= log \\prod p(y_i|x_i; b_0, b_1, s^2) \\\\ &= -\\dfrac{n}{2}log{2\\pi} - nlogs - \\dfrac{1}{2s^2}\\sum [y_i - (b_0+b_1x)]^2 \\end{align}Setting the partial-derivative of the estimator with respect to each of the parameter to zero, we can obtain the maximum likelihood parameters: \\begin{align} \\hat{\\beta_1} &= \\dfrac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i-\\bar{x})^2} \\\\ \\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1}\\bar{x} \\\\ \\hat{\\sigma^2} &= \\dfrac{1}{n}\\sum [y_i - (b_0+b_1x_i)]^2 \\end{align}MLE, or maximum likelihood estimation, is a frequentist approach for estimating model parameters based on the assumed underlying model form and error distribution, by maximizing the probability of seeing what we saw in the data. MLE is a near-optimal method of estimation, and is optimal in many cases. If we assume a Gaussian error distribution and a linear model, then the conclusion above states that maximizing the MLE objective function is the SAME as minimizing the RSS objective function. More on frequentist vs Bayesian in this SOA work paper. Also see this CMU lecture note and for more detail regarding the derivation. Model FitRecall that we assume there is a underlying relationship f between Y and X: Y=f(X) + \\epsilonThe residual standard error, or RSE, estimates the standard deviation of \\epsilon. Note that RSE is an absolute measure of the lack of fit of the model and depends on units of Y. RSE = \\sqrt{RSS/(n-2)}The R^2 measures the proportion of variance explained by the regression. TSS is the total sum of squares which measures the total variance in Y R^2 = 1 - \\dfrac{RSS}{TSS}In a simple regression setting, R^2 = Corr(X, Y)^2 Residual PlotHere is a good article on how to interpret your residual plot. Summarizing the approaches for different residual issues: Y-axis Unbalanced: transform target X-axis Unbalanced: transform predictor Heteroscedasticity: transform target/predictor Non-Linearity: transform predictor; create non-linear model Outlier: transform target/predictor; remove/assess the outlier; Multiple Linear RegressionThe multiple linear regression takes the form: Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilonF-statisticWe use the F-statistic to test the null hypothesis that there are no relationships between the predictors and target variable. H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0 \\\\ H_a: \\text{ at least one } \\beta \\text{ is non-zero}We calculate the F-statistic as follow: F = \\dfrac{(TSS - RSS)/p}{RSS/(n-p-1)}We expect F=1 if H_0 is true. Potential ProblemsNon-Linearity Residual plots are useful to detect whether the underlying relationship f is non-linear. One of the solutions to this problem is to fit transformations of the predictors such as log{X}, \\sqrt{X}, and X^2. Collinearity This is where two or more predictors are closely correlated with each other, or “collinear”. Collinearity reduces the accuracy of the coefficient estimates by increasing its standard deviation and p-value. One way to detect collinearity is from the correlation matrix or the “pairs” plot in R. There are two solutions: dropping one of the collinear predictor, or combine the collinear predictors into a single predictor. Multi-collinearity This is where collinearity exist between three or more predictors when none of the pairwise correlations are high. The best way to assess multi-collinearity if through variance inflation factor, or VIF. VIF(\\beta_j) = \\dfrac{var(\\beta_j) \\text{ in full model}}{var(\\beta_j) \\text{ in single } \\beta_j \\text{ model}}A VIF of 1 indicates no collinearity. A VIF of 10+ indicates high collinearity. Outliers Use residual plot to detect and potentially remove outliers. Linear Model Selection and RegularizationLinear model can be improved by using alternative fitting procedures, which produce better prediction accuracy and model interpretability. Subset Selection Select a subset of the original predictors and then fit the model. Shrinkage/Regularization Fit the model by shrinking coefficient estimates towards zero, therefore reducing variances of the coefficient estimates. Dimension Reduction Project the predictors onto a M-dimensional subspace, then use the M projections as predictors to fit a linear model with least square. Subset SelectionThe beset subset selection fits a separate least square regression for each combination from the p predictors, creating 2^p models to compare. The forward stepwise selection and backward stepwise selection fits a total of 1+p(p+1)/2 models. Specifically, at each step a predictor is added/removed to the model only if it gives the greatest additional improvement (lowest RSS or highest adjusted R^2) among all the predictors. After the selection process, we need to determine the optimal model that gives the lowest potential test error, either through: Cross-validation, or Adjusted train error Example 1: \\boldsymbol{C_p} := \\dfrac{1}{n}(RSS + 2d\\hat{\\sigma}^2), where d is the number of predictors in the subset, and \\hat{\\sigma}^2 is the variance of error estimated using the full models containing all p predictors. Essentially, a penalty term 2d\\hat{\\sigma}^2 is added to the train RSS to adjust for the fact that the training error tends to underestimate the test error. Example 2: \\boldsymbol{AIC} = \\dfrac{1}{n\\hat{\\sigma}^2}(RSS + 2d\\hat{\\sigma}^2). The AIC, or Akaike information criterion, uses the maximum likelihood function to assess the relative quality of statistical models give a set of data. In linear models with Gaussian error terms, the maximum likelihood function is equivalent to RSS, and therefore C_p and AIC are proportional to each other. Example 3: \\boldsymbol{BIC} = \\dfrac{1}{n\\hat{\\sigma}^2}(RSS + log(n)d\\hat{\\sigma}^2) Example 4: \\boldsymbol{Adjusted}\\; \\boldsymbol{R^2} = 1 - \\dfrac{RSS/(n-d-1)}{TSS/(n-1)}. While RSS always decreases in the stepwise selection as the number of predictors increases, RSS/(n-d-1) may or may not decrease. ShrinkageRidge RegressionRecall that in least square regression, the coefficient \\beta are estimated by minimizing the objective function RSS. Objective\\ Function = RSS = \\sum (y_i - \\hat{y_i})^2In ridge regression, the objective function include an additional shrinkage penalty, where \\lambda>0 is a tuning parameter. Note that we do not want to shrink the intercept \\beta_0, which is simply a measure of the mean of the responses: Objective\\ Function = RSS + \\lambda\\sum_{i>0}\\beta_i^2 As \\lambda increases, the variance decreases and the bias increases. The model fit usually is improved initially as the variance decreases, but worsen at some point when bias starts to increases rapidly. Cross-validation is often used to select the optimal \\lambda. As \\lambda\\rightarrow\\infty, the coefficient approaches 0. The ridge regression works when the linear model has low bias and high variance, e.g. when the underlying relationship is close to linear. The ridge regression trades off a small increase in bias for large decrease in variance. Additionally, it is important to standardize all features when applying regularization. Imagining a feature in dollar and in thousand dollar: the model with the dollar feature will have much higher coefficient compared to the thousand dollar one, leading to larger regularization effect for the dollar feature. Lasso RegressionAlthough the ridge regression shrinks the coefficients, it does not eliminiate excess predictors. Model interpretation might be an issue for the ridge regression where the number of predictors are large. The lasso regression overcomes this issue and force some coefficients to be exactly 0. Objective\\ Function = RSS + \\lambda\\sum_{i>0}|\\beta_i|However, there are limitations of feature selections using regularization techniques such as lasso, such as model interpretability. In addition, the feature we selected are optimized in linear models, and may not necessarily translate to other model forms. Note the difference between L2 (ridge) and L1 (lasso) penalty: when the coefficients (absolute value) are greater than 1 (when the parameters are large), the L2 penalty is greater than the L1, and ridge provides more shrinkage. when the coefficients (absolute value) are smaller than 1 (when the parameters are small), the L1 penalty is greater than the L2, and lasso provides more shrinkage. In R, we use the glmnet package to compute ridge and lasso regressions to predict mpg from the mtcars built-in data set.1234567891011121314151617181920212223242526272829303132333435363738394041library(glmnet)library(caret)set.seed(9999)# datadf &lt;- as.data.frame(mtcars)x &lt;- model.matrix(mpg~., df)[, -1]y &lt;- df$mpg# 75/25 train/test splitpartition &lt;- createDataPartition(df$mpg, list = FALSE, p = .75)df_train &lt;- df[partition, ]df_test &lt;- df[-partition, ]x_train &lt;- x[partition, ]x_test &lt;- x[-partition, ]y_train &lt;- y[partition]y_test &lt;- y[-partition]# fit regressionm1 &lt;- lm(mpg ~ ., df_train)m1.pred &lt;- predict(m1, df_test)m1.mse &lt;- round(mean((y_test - m1.pred)^2), 2)# fit ridge regressionm2 &lt;- cv.glmnet(x_train, y_train, alpha=0, nfolds=6)m2.bestlambda &lt;- m2$lambda.minm2.pred &lt;- predict(m2, s=m2.bestlambda, x_test)m2.mse &lt;- round(mean((y_test - m2.pred)^2), 2)# fit lasso regressionm3 &lt;- cv.glmnet(x_train, y_train, alpha=1, nfolds=6)m3.bestlambda &lt;- m3$lambda.minm3.pred &lt;- predict(m3, s=m3.bestlambda, x_test)m3.mse &lt;- round(mean((y_test - m3.pred)^2), 2)# get coefficientsm2.best &lt;- glmnet(x_train, y_train, alpha=0, lambda=m2.bestlambda)m3.best &lt;- glmnet(x_train, y_train, alpha=1, lambda=m3.bestlambda)comp &lt;- cbind(coef(m1), coef(m2.best), coef(m3.best))colnames(comp) &lt;- c(\"original\", \"ridge\", \"lasso\") The test MSE are as follow. Note that both ridge and lasso regression perform better than the original regression.123456&gt; m1.mse[1] 16.64&gt; m2.mse[1] 3.66&gt; m3.mse[1] 6.61 We also made a comparison of the coefficients, based on the normal regression and the regularized regression with cv-optimal lambda.1234567891011121314&gt; comp11 x 3 sparse Matrix of class \"dgCMatrix\" original ridge lasso(Intercept) -19.52071389 19.420775180 14.002192375cyl 1.69431225 -0.275408324 . disp 0.01185998 -0.004705579 . hp -0.01449594 -0.011129305 -0.002545135drat 3.08676192 1.272808791 1.334526777wt -3.19280650 -1.137507747 -2.254408320qsec 1.02473436 0.117671597 0.331182874vs 0.97127211 0.677494227 . am 2.63740010 1.633418877 1.703501439gear 3.36943552 0.794847062 1.571031414carb -1.45443855 -0.588427657 -1.162118484 Resampling MethodResampling methods involve repeatedly drawing samples from a training set to re-fit the model. Cross-ValidationWe often use the test error rate to determine and compare how well a statistical learning model perform. However, in the absence of a large designated test set, the test error rate can be difficult to estimate. The train error rate is often quite different from the test error rate. Therefore, cross-validation can be used to estimate the test error rates by creating validation sets off the train data. A k-fold cross validation involves randomly dividing the observations into k-groups. The process is repeated k-times where each group is treated as a validation set while fitting the remaining k-1 groups. The k-fold CV test MSE is the average of all the MSE from each validation set: MSE_{CV} = \\dfrac{1}{k}\\sum_{i=1}^{k} MSE_iThe leave-one-out cross validation, or LOOCV, is a special case of k-fold CV with k=n. Since LOOCV requires fitting the model n times, with n being equal to number of train data points. The k-fold CV with k=5\\ or \\ 10 are more feasible computationally as it only needs to fit the model 5\\ or\\ 10 times. BootstrapBootstrap provides a measure of accruacy of either a parameter estimate or a given statistical learning method. It can be used to estimate variance of a parameter by repeatedly re-sampling the same data set with replacement (i.e. duplicate data entries allowed) while re-calculating the parameter based on each re-sample. Hyper-Parameter TuningWe specify numerious constants called hyperparameter during our modeling process, e.g. \\lambda, \\alpha, etc. To set these constant such that our model can predict accruately while avoiding over-complexity and overfitting, we tune our hyperparameter with cross validation. For more details see the auto claim notebook. Generalized Linear ModelGeneralized linear models were developed by Nelder and Wedderburn in a paper published in 1972 to provide a flexible framework which introduces a link function that transform the linear combinations of predictors. An Ordinary Linear Model has many limitations: As ordinary linear model produces a numeric response, it requires the assumptions of orderings to predict qualitative responses. Negative values may be predicted when not allowed. When the variance of the target variable depends on the mean, the homoscedasticity assumption is violated, and therefore the least square estimator is no longer the MLE estimator and various statistical test would not hold. Sensitive to outliers. Does not perform well with non-linear relationships. The Generalized Linear Models relaxes the assumptions of OLM. First, GLM relaxes the normal residual assumption of OLM, and allow the target variable Y to follow any distribution within the exponential distribution family: f(y_i|x_i) \\sim \\text{exponential distribution family}With regard to this distribution, there exists a canonical link function associated with it that simplifies the mathematics of solving GLM analytically. Normal =&gt; Identity: \\phi(a) = a Exponential/Gamma =&gt; Negative Inverse: \\phi(a) = - a^{-1} Inverse Gaussian =&gt; Inverse Square: \\phi(a) = a^{-2} Poisson =&gt; Log: \\phi(a) = ln(a) Bernoulli/Binomial/Multinomial =&gt; Logit: \\phi(a) = ln[a/(1-a)] We can either choose the canonical link function or pick another one (which may not lead to a converged GLM solution, however). With this link function, GLM assumes that the expectation of the target is the inverse linked linear combination of predictors: \\mathbb{E}(y_i|x_i) = \\phi^{-1}(\\beta x_i)With all above assumptions satisfy, the coefficient \\beta of a GLM model can then be solved: \\phi(Y) \\sim \\beta XLogistic RegressionThe logistic regression model is popular for classification problems. With two response classes, we can calculate the probability of assigning the response in each class and predict the response by choosing the class with the higher probability. or more than two response classes, multiple-class logistic regression is available but the discrimentant analysis is more popular. We define p(X) = \\mathbb{P}[Y = 1|X] as our new response variable and the link function: logit function, short for logistic function, as such: \\phi(p(X)) = logit(p(X)) = log[\\dfrac{p(X)}{1-p(X)}]Since we assume a linear relationship between our predictor X and the linked reponse logit(p(X)), we have: log[\\dfrac{p(X)}{1-p(X)}] = \\beta_0+\\beta_1X_1+\\beta_2X_2\\dotsTherefore, p(X) = \\dfrac{e^{\\beta_0+\\beta_1X_1+\\beta_2X_2\\dots}}{1+e^{\\beta_0+\\beta_1X_1+\\beta_2X_2\\dots}}Now we have a nice property of p(X) \\in (0, 1), which is exactly what we wanted to model probability responses. The quantity p(X)/(1-p(X)) is called the odds. In R, we use the glm function (with family=binomial) in the predict Survival from the Titanic dataset.12345678910111213141516171819202122232425262728293031323334353637library(e1071)library(caret)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]df &lt;- subset(df, select = -c(Freq))# Binarize class, sex and agedf.new &lt;- predict(dummyVars(\"~Class+Sex+Age\", df, sep=\"_\", fullRank=TRUE), df)df.new &lt;- as.data.frame(df.new)df.new[\"Survived\"] &lt;- df$Surviveddf &lt;- df.new# Create random 80/20 train/test splittrain_ind &lt;- sample(seq_len(nrow(df)), size = floor(0.80 * nrow(df)))df_train &lt;- df[train_ind, ]df_test &lt;- df[-train_ind, ]# Fit Logistic Regressionmodel &lt;- glm(Survived~., df_train, family=binomial)summary(model)contrasts(df$Survived)# Predict In-Sampleprob_train &lt;- predict(model, df_train, type=\"response\")pred_train &lt;- rep(\"No\", nrow(df_train))pred_train[prob_train &gt; .5] &lt;- \"Yes\"# Predict Out-Of-Sampleprob_test &lt;- predict(model, df_test, type=\"response\")pred_test &lt;- rep(\"No\", nrow(df_test))pred_test[prob_test &gt; .5] &lt;- \"Yes\" From summary(model), note that most coeefficients are significant.123456789&gt; summary(model)Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.3094 0.3132 0.988 0.323 Class_2nd -1.1163 0.2187 -5.105 3.31e-07 `Class_3rd -1.7041 0.1918 -8.883 &lt; 2e-16 `Class_Crew -0.8848 0.1773 -4.991 6.01e-07 `Sex_Female 2.3691 0.1555 15.235 &lt; 2e-16 `Age_Adult -0.6289 0.2767 -2.273 0.023 * Note that probability of 1 correspond to “Yes” in the Survived variable.1234&gt; contrasts(df$Survived) YesNo 0Yes 1 From in-sample confusion matrix.123456789&gt; confusionMatrix(as.factor(pred_train), df_train$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 1087 296 Yes 102 275 Accuracy : 0.7739 From out-of-sample confusion matrix.123456789&gt; confusionMatrix(as.factor(pred_test), df_test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 277 66 Yes 24 74 Accuracy : 0.7959 Comparing the test error rate between naiveBayes, KNN and logistic regression.123naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942logistic regression: 0.2041 Poisson RegressionThe Poisson distribution expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. \\mathbb{P}(\\text{k events in interval}) = e^{\\lambda}\\dfrac{\\lambda^k}{k!}We can fit Posisson regression if we observe that the frequencies of response variable Y exhibits a Poisson shape. We will create a new response vector \\theta and assumes that log(\\theta) has an underlying linear relationship in X. Y \\sim poisson(\\boldsymbol {\\theta}) \\\\ \\text{and, } log(\\boldsymbol {\\theta}) = \\beta XThat is, we assume that for each X_i, Y_i \\sim poisson(e^{\\beta X_i}). The log link function ensures that \\boldsymbol{\\theta} is strictly positive. Note that we had made a strong assumption that for each X_i, the mean and variance of Y_i are the same, as dictated by the Poisson distribution. However, if the data shows larger variance than expected, or overdispersion, we can then use the quasi-Poisson regression, which is essenstially the negative binomial distribution with looser assumptions than Poisson. In the diamonds dataset from the ggplot2 package, we plotted the histogram of the price data from 50,000 observations. 123library(ggplot2)df &lt;- as.data.frame(diamonds)ggplot(df, aes(x=price)) + geom_histogram(binwidth=1) The price data shows resemblence to a Poisson distribution. We fitted four different models: linear, ridge, lasso, and Poisson regressions.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162library(e1071)library(ggplot2)library(caret)library(glmnet)set.seed(9999)df &lt;- as.data.frame(diamonds)# caret::dummyVars does not work well with ordered factor. change to unordered.df[\"cut_1\"] &lt;- factor(df$cut, order=FALSE)df[\"color_1\"] &lt;- factor(df$color, order=FALSE)df[\"clarity_1\"] &lt;- factor(df$clarity, order=FALSE)# Binarize Category Variabledummy &lt;- dummyVars(~ cut_1 + color_1 + clarity_1, df, sep=\"_\", fullRank=TRUE)df.new &lt;- as.data.frame(predict(dummy, newdata = df))df.new[\"carat\"] &lt;- df$caratdf.new[\"price\"] &lt;- df$pricedf &lt;- df.newx &lt;- model.matrix(price~., df)[, -1]y &lt;- df$price# Create random 80/20 train/test splitpartition &lt;- sample(seq_len(nrow(df)), size = floor(0.80 * nrow(df)))df_train &lt;- df[partition, ]df_test &lt;- df[-partition, ]x_train &lt;- x[partition, ]x_test &lt;- x[-partition, ]y_train &lt;- y[partition]y_test &lt;- y[-partition]# Fit Linear Regressionm1 &lt;- lm(price~., df_train)m1.pred &lt;- predict(m1, df_test)m1.mse &lt;- round(mean((y_test - m1.pred)^2), 2)# Fit Ridge Regressionm2 &lt;- cv.glmnet(x_train, y_train, alpha=0, nfolds=10)m2.bestlambda &lt;- m2$lambda.minm2.pred &lt;- predict(m2, s=m2.bestlambda, x_test)m2.mse &lt;- round(mean((y_test - m2.pred)^2), 2)# Fit Lasso Regressionm3 &lt;- cv.glmnet(x_train, y_train, alpha=1, nfolds=10)m3.bestlambda &lt;- m3$lambda.minm3.pred &lt;- predict(m3, s=m3.bestlambda, x_test)m3.mse &lt;- round(mean((y_test - m3.pred)^2), 2)# Fit Poisson Regressionm4 &lt;- glm(price~., df_train, family=poisson(link=\"log\"))m4.pred &lt;- predict(m4, df_test)m4.mse &lt;- round(mean((y_test - exp(m4.pred))^2), 2)# Fit quasiPoisson Regressionm5 &lt;- glm(price~., df_train, family=quasipoisson(link=\"log\"))m5.pred &lt;- predict(m5, df_test)m5.mse &lt;- round(mean((y_test - exp(m5.pred))^2), 2)# Compare Coefficientm2.best &lt;- glmnet(x_train, y_train, alpha=0, lambda=m2.bestlambda)m3.best &lt;- glmnet(x_train, y_train, alpha=1, lambda=m3.bestlambda)comp &lt;- cbind(coef(m1), coef(m2.best), coef(m3.best), coef(m4), coef(m5))colnames(comp) &lt;- c(\"original\", \"ridge\", \"lasso\", \"Poisson\", \"quasiPoi\") Showing the results. We can see that lasso regression improved upon ridge. However, the Poisson regression show very high MSE, and not improved by using quasi-Poisson to deal with overdispersion.1234567891011121314&gt; m1.mse[1] 1296082&gt; m2.mse[1] 1662769&gt; m3.mse[1] 1296773&gt; m4.mse[1] 5237564&gt; m5.mse[1] 5237564 Comparing coefficients:12345678910111213141516171819202122&gt; comp19 x 5 sparse Matrix of class \"dgCMatrix\" original ridge lasso Poisson quasiPoi(Intercept) -7369.2859 -2751.693983 -7083.4777 5.17185599 5.17185599cut_1_Good 686.6877 112.163397 657.1100 0.17515789 0.17515789`cut_1_Very Good` 864.3160 319.828814 838.9901 0.20528993 0.20528993cut_1_Premium 893.6342 367.364011 866.6581 0.18009057 0.18009057cut_1_Ideal 1025.5579 421.231162 999.8031 0.20406528 0.20406528color_1_E -225.3985 -23.486004 -205.5637 -0.05660526 -0.05660526color_1_F -294.1807 2.717515 -274.2126 -0.03079075 -0.03079075color_1_G -522.9599 -112.556133 -501.1661 -0.10597258 -0.10597258color_1_H -997.6266 -490.885083 -975.9837 -0.25411004 -0.25411004color_1_I -1452.1455 -749.323400 -1426.9279 -0.42200863 -0.42200863color_1_J -2343.2864 -1450.247841 -2315.1429 -0.63819087 -0.63819087clarity_1_SI2 2612.5463 -472.362109 2345.9392 1.14176901 1.14176901clarity_1_SI1 3555.6013 149.192295 3287.0687 1.38540593 1.38540593clarity_1_VS2 4210.3193 671.522828 3940.7103 1.50843614 1.50843614clarity_1_VS1 4507.7144 861.334888 4235.9431 1.58850692 1.58850692clarity_1_VVS2 4965.2210 1192.175019 4691.8855 1.66443836 1.66443836clarity_1_VVS1 5072.6388 1162.725817 4796.5710 1.61060572 1.61060572clarity_1_IF 5436.6567 1476.377453 5158.0224 1.72336616 1.72336616carat 8899.8818 7672.223943 8883.8976 1.65798106 1.65798106 We are curious as to why the Poisson regression perform much worse than a simple linear regression, when the reponse variable clearly shows Poisson patterns. 1234567891011p1 &lt;- ggplot() + geom_point(data=df_test, aes(x=as.numeric(row.names(df_test)), y=price), shape=\".\", color=\"black\") + geom_point(aes(x=as.numeric(row.names(df_test)), y=m1.pred), shape=\".\", color=\"red\")p2 &lt;- ggplot() + geom_point(data=df_test, aes(x=as.numeric(row.names(df_test)), y=price), shape=\".\", color=\"black\") + geom_point(aes(x=as.numeric(row.names(df_test)), y=exp(m4.pred)), shape=\".\", color=\"red\") We first look at the linear regression fit, where the black dots are the original data and the red dots are the fitted data: We then look at the Poisson regression fit. Unfortunately the Poisson regression create ultra-high predictions for some values, which skew the MSE matrix. This is we forget to log the numerical variable (carat) when we use log link function, which results in a exponential shape for the prediction. We change the code as follow:123456789# Fit Poisson Regressionm4 &lt;- glm(price~-carat+log(carat), df_train, family=poisson(link=\"log\"))m4.pred &lt;- predict(m4, df_test)m4.mse &lt;- round(mean((y_test - exp(m4.pred))^2), 2)# Fit quasiPoisson Regressionm5 &lt;- glm(price~-carat+log(carat)., df_train, family=quasipoisson(link=\"log\"))m5.pred &lt;- predict(m5, df_test)m5.mse &lt;- round(mean((y_test - exp(m5.pred))^2), 2) The mse are now better:1234&gt; m4.mse[1] 2544181&gt; m5.mse[1] 2544181 Plotting the prediction. We can see that other than one prediction outlier, the overall predictions are better than when we did not log the carat. Goodness of FitDeviance is a measure of the goodness of fit of a generalized linear model, similar to a RSS in the simple linear model. . The default value is called null deviance and is the deviance calculated when the response variable is predicted using its sample mean. Adding additional feature to the model would generally decrease the deviance and decrease the degree of freedoms. Tree-Based Method &#8634;The tree-based method involve segmenting the predictor space into several regions to make a prediction for a given observation. There are several advantages to the tree-based methods: Easy to explain Intuitive to human reasoning Graphic and interpretable No need to dummy variables for qualitative data On the other hand, disadvantages: Lower predictive accuracy Sensitive to change in data Several techniques can make significant improvement to compensate the disadvantages, namely bagging, random forest and boosting. Regression TreeIn a linear regression model, the underlying relationship is assumed to be: f(X) = \\beta_0 + \\sum\\beta_iX_iWhereas in a regression tree model, the underlying is assumed to be: f(X) = \\sum c_i\\textbf{1}_{X_i\\in R_i}Where each R_i represent a partition of the feature space. The goal is to solve for the partition set \\{R_i\\}_{i\\in I} which minimize the objective function: Objective\\; Function = RSS = \\sum_{i\\in I}\\sum_{X_j\\in R_i} (y_j-\\bar{y}_{R_i})^2To find \\{R_i\\}_{i\\in I} efficiently, we introduce recursive binary splitting, which is a top-down and greedy approach. It is greedy because it is short-sighted in that it always chooses the current best split, instead of the optimal split overall. Due to the greedy nature, it is preferred that we first grow a complex tree and then prune it back, so that all potential large reductions in RSS are captured. Cost Complexity PruningThe cost complexity pruning approach aim to minimize the objective function, give each value of \\alpha: Objective\\; Function = \\sum_{i\\in I_T}\\sum_{X_j\\in R_i} (y_j-\\bar{y}_{R_i})^2 + \\alpha|T|Where |T| is the number of terminal nodes of subtree T\\subset T_0 where T_0 is the original un-prune tree. The tuning parameter \\alpha controls the complexity of the subtree T, penalizing any increase in nodes. The goal is to prune the tree with various \\alpha and then use cross-validation to select the best \\alpha. This is similar to the lasso equation, which also introduce a tuning parameter \\lambda to control the complexity of a linear model. Objective\\; Function = RSS + \\lambda\\sum_{i>0}|\\beta_i|In R, we use the rpart library, which stands for recursive partitioning and regression trees, to fit a regression tree to predict mpg in our mtcars dataset.123456789101112131415library(caret)library(rpart)library(rpart.plot)set.seed(9999)df &lt;- as.data.frame(mtcars)# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$mpg, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]# fit regression treet1 &lt;- rpart(mpg ~ ., train)t1.predict &lt;- predict(t1, test)t1.mse &lt;- round(mean((test$mpg - t1.predict)^2), 2) The test MSE, as compared to the previous linear models.1234567891011# regression tree&gt; t1.mse[1] 11.59# linear regression&gt; m1.mse[1] 16.64&gt; m2.mse[1] 3.66&gt; m3.mse[1] 6.61 Plotting the tree with rpart.plot(t1) command. Classification TreeClassification tree is very similar to regression trees expect we need an alternative method to RSS when deciding a split. There are three common approaches. Classification error rate: E = 1 - \\underset{k}{max}(\\hat{p}_{mk}) Where \\hat{p}_{mk} represents the porportion of train observations from the m-th parent node that are from the k-th child node. However, the this approach is not sufficiently sensitive to node impurity for tree-growing, compared to the next two. Gini index: G = \\sum_k \\hat{p}_{mk}(1-\\hat{p}_{mk}) Note that the Gini index decreases as all \\hat{p}_{mk} get closer to 0 or 1. Therefore it is a measure of the node purity. Entropy: D = -\\sum_k \\hat{p}_{mk} log_2(\\hat{p}_{mk}) The Entropy is also a measure of the node purity and similar to the Gini index numerically. For a two-class decision tree, the impurity measures calculated from different methods for a given \\hat{p}_{mk} are simulated below with python. 12345678910111213import matplotlib.pyplot as pltimport numpy as npx = np.arange(0, 1, 0.001)y1 = x*np.log(x)/np.log(2)*-1 + (1-x)*np.log(1-x)/np.log(2)*-1y2 = x*(1-x) + (1-x)*(1-(1-x))y3 = 1-np.maximum(x, 1-x)fig = plt.figure(figsize=(6, 6))plt.plot(x, y1)plt.plot(x, y2)plt.plot(x, y3)plt.legend(['entropy', 'gini', 'class error'], loc='upper right') We can see that all three method are similar and consistent with each other. Entropy and the Gini are more sensitive to changes in the node probabilities, therefore preferrable when growing the trees. The classification error is more often used during complexity pruning. Information GainWhen the measure of node purity is calculated, we want to maximize the information gain after each split. We use P and C to denote the parent and child node, with entropy as the measure. N is the number of observations under the parent node: IG = Entropy(P) - \\sum_{k=1}^{K}\\dfrac{N_k}{N}Entropy(C_k)In R, we use the rpart library to create a classification tree to predict Survived in the Titanic data set.12345678910111213141516171819202122library(caret)library(rpart)library(rpart.plot)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]df &lt;- subset(df, select = -c(Freq))# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$Survived, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]# fit classification treet2 &lt;- rpart(Survived ~ ., train)t2.prob &lt;- predict(t2, test)t2.pred &lt;- rep(\"No\", nrow(t2.prob))t2.pred[t2.prob[, 2] &gt; .5] &lt;- \"Yes\"# pruningt2.prune &lt;- prune(t2, cp = 0.05)rpart.plot(t2.prune) The confusion matrix shows:123456789&gt; confusionMatrix(as.factor(t2.pred), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 368 105 Yes 4 72 Accuracy : 0.8015 Plotting the tree with rpart.plot(t2). Plotting the complexity parameter against the cross-validation relative error with plotcp(t2). Although the relative error is at the lowest at 5 nodes, comparable level of relative error was achieved at 2 nodes. Because decision tree is prone to overfitting, here we manually prune the tree back to 2 nodes. Plotting the tree with rpart.plot(t2.prune). The confusion matrix after pruning. We lose a small bit of out-of-sample accuracy due to manual pruning.123456789&gt; confusionMatrix(as.factor(t2.prune.pred), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 343 83 Yes 29 94 Accuracy : 0.796 Comparing the test error rate with naiveBayes, KNN and logistic regression.1234naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942classification tree: 0.1985classification tree (prune): 0.2040 Confusion MatrixThe confusion matrix is a convenient summary of the model prediction. In our previous example with the un-pruned tree: 12345678910111213&gt; confusionMatrix(as.factor(t2.pred), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 368 105 Yes 4 72# mapping the number to definition ReferencePrediction No Yes No TN FN Yes FP TP There are four types of prediction: True Positive (TP): 72 True Negative (TN): 368 False Positive (FP), or Type I Error: 4 False Negative (FN), or Type II Error: 105 Several metrics can be computed: Accuracy: (TP + TN) / N = (72 + 368) / 549 = 0.8015 Error Rate: (FP + FN) / N = 1 - Accuracy = 0.1985 Precision: TP / (Predicted Positive) = 72 / (72 + 4) = 0.9474 Sensitivity: TP / (Actually Positive) = 72 / (72 + 105) = 0.4068 Receiver Operator Characteristic CurveThe ROC curve can be used to evaluate the performacne of our model. The ROC curve plots the TPR (true positive rate) against FPR (false positive rate) over a range of cutoff values: TPR = TP / (Actually Positive) = 0.4068 FPR = FP / (Actually Negative) = 0.0107 In python, we can create a ROC curve from our previous prediction TPR and FPR:1234567891011121314import matplotlib.pyplot as pltimport numpy as npx = np.arange(0, 1, 0.0001)plt.plot(x, x)plt.xlabel('FPR')plt.ylabel('TPR')x2=0.0107y2=0.4680plt.scatter(x2, y2, s=20, color='red')plt.plot([0, x2], [0, y2], 'r-', color='red')plt.plot([x2, 1], [y2, 1], 'r-', color='red')plt.title('ROC Plot') The baseline line refers to a model/cutoff where all observations in the testing set are predicted to be positive (point x=1, y=1), or negative (point x=0, y=0). The area under the red lines and above the x-axis is an estimate of the model fit and is called AUC, or area under the ROC curve. An AUC of 1 means that the model has a TPR of 1 and FPR of 0. Ensemble MethodsBaggingThe decision tree method in general suffer from high model variance, compared to linear regression which shows low variance. Bootstrap aggregation, or bagging is a general-purpose procedure for reducing variance of a statistical model without affecting the bias. It is particular useful in the decision tree model context. For regression trees, construct B regression trees using B bootstrapped (repeatedly sampled) training sets. These trees grow deep and are not pruned, therefore having high variance. At the end, average the trees to reduce the variance. For classification trees, construct B classification trees. When predicting a test observation, take the majority classification resulted from the B trees. Note that the bagging results are more accruate but less visual. We can obtain the variable importances by computing the total RSS/Gini decreases by splits over each predictors, hence providing better interpretations of the results. Random ForestRandom forest improves upon bagged trees by de-correlating the trees. At each split, only m\\approx\\sqrt{p} predictors are considered, isolating effects on single feature with large influences. In R, use the randomForest package:123456789101112131415161718library(caret)library(randomForest)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]df &lt;- subset(df, select = -c(Freq))# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$Survived, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]# fit random forestrf &lt;- randomForest(formula = Survived ~ ., data = train, ntree = 100, importance = TRUE) In-sample confusion matrix:12345678910111213&gt; rfCall: randomForest(formula = Survived ~ ., data = train, ntree = 100, importance = TRUE) Type of random forest: classification Number of trees: 100No. of variables tried at each split: 1 OOB estimate of error rate: 23.43%Confusion matrix: No Yes class.errorNo 1063 55 0.04919499Yes 332 202 0.62172285 Out-of-sample confusino matrix:123456789&gt; confusionMatrix(as.factor(predict(rf, test)), test$Survived)Confusion Matrix and Statistics ReferencePrediction No Yes No 368 108 Yes 4 69 Accuracy : 0.796 Comparing the test error rate with naiveBayes, KNN and logistic regression.12345naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942classification tree: 0.1985classification tree (prune): 0.2040random forest: 0.2040 BoostingBoosting is also a general-purpose procedure for improving accuracy of the statistical model. In boosting, we repeatedly fit new trees to the residuals from the previous tree, and add the new trees to the main tree such that a loss function is minimized (subject to shrinkage parameter \\lambda, typical 0.01, to control overfitting). CV is often used to determine the total number of trees to be fitted. A gradient boosting machine is an algorithm that calculates the gradient of the loss function and update the paramters such that the model moves in the direction of the negative gradient, thus closer to a minimum point of the loss function. XGBoost is an open-source software library that provides a gradient boosting framework for R. XGBoost initially started as a research project by Tianqi Chen as part of the Distributed (Deep) Machine Learning Community (DMLC) group 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465llibrary(caret)library(xgboost)library(pROC)set.seed(9999)df &lt;- as.data.frame(Titanic)df &lt;- df[rep.int(seq_len(nrow(df)), df$Freq), ]df &lt;- subset(df, select = -c(Freq))# turn survival into 0 and 1Survived_Ind &lt;- rep(0, nrow(df))Survived_Ind[df$Survived == \"Yes\"] &lt;- 1df$Survived_Ind &lt;- Survived_Ind# create a 75/25 train/test splitpartition &lt;- createDataPartition(df$Survived_Ind, list=FALSE, p=0.75)train &lt;- df[partition, ]test &lt;- df[-partition, ]test_2 &lt;- testtrain &lt;- subset(train, select = -c(Survived))test &lt;- subset(test, select = -c(Survived))# create model frame for xgboost inputtrain.mf &lt;- model.frame(as.formula(\"Survived_Ind ~.\"), data = train)test.mf &lt;- model.frame(as.formula(\"Survived_Ind ~.\"), data = test)# create a model matrix only contains numerical values.train.mm &lt;- model.matrix(attr(train.mf, \"terms\"), data = train)test.mm &lt;- model.matrix(attr(test.mf, \"terms\"), data = test)# [optional] create A XGB dense matrix contains an R matrix and metadatatrain.dm &lt;- xgb.DMatrix(train.mm, label = train$Survived, missing = -1)test.dm &lt;- xgb.DMatrix(test.mm, label = test$Survived, missing = -1)# create xgboost parameter listparams &lt;- list(\"booster\" = \"gbtree\", # \"gblinear\" for glm \"objective\" = \"binary:logistic\", # the output here is a probability \"eval_metric\" = \"auc\", \"eta\" = 0.1, # lambda \"subsample\" = 0.6, # proportion of observations \"colsample_bytree\" = 0.6, # proportion of features \"max_depth\" = 5) # depth of the decision tree# train xgboost modelmodel.cv &lt;- xgb.cv(params = params, data = train.dm, nrounds = 1000, # the number of trees / iterations prediction = FALSE, # storage of prediction under each tree print_every_n = 25, early_stopping_rounds = 50, maximize = TRUE, # AUC metric -&gt; maximize nfold = 6) # cv# fit final modelmodel &lt;- xgb.train(params = params, data = train.dm, nrounds = model.cv$best_iteration, prediction = FALSE)# format predictionxgb.prob &lt;- predict(model, test.dm)xgb.pred &lt;- rep(\"No\", sum(lengths(xgb.prob)))xgb.pred[xgb.prob &gt; 0.5] &lt;- \"Yes\" Feature importance:1234&gt; xgb.importance(feature_names = dimnames(train.dm)[[2]], model = model) Feature Gain Cover Frequency1: Class3rd 0.8347889 0.5957944 0.52: Class2nd 0.1652111 0.4042056 0.5 AUC result:12&gt; auc(test$Survived_Ind, xgb.prob)Area under the curve: 0.6033 Confusion matrix:123456789&gt; confusionMatrix(as.factor(xgb.pred), as.factor(test_2$Survived))Confusion Matrix and Statistics ReferencePrediction No Yes No 370 180 Yes 0 0 Accuracy : 0.6727 Unfortunately xgboost predict everything to be “No” Comparing the test error rate with naiveBayes, KNN and logistic regression.123456naiveBayes (cv10): 0.2095KNN (cv10, K=1): 0.1942classification tree: 0.1985classification tree (prune): 0.2040random forest: 0.2040xgboost: 0.3273 Ensemble Model InterpretationFeature Importance ranks the contribution of each feature.Partial Dependence Plots visualizes the model’s average dependence on a specific feature (or a pair of features). Unsupervised Learning &#8634;In supervised learning, we are provided a set of n observations X, each containing p features, and a response variable Y. We are interested at predicting Y using the observations and features. In unsupervised learning, we are interested at exploring hidden relationships within the data themself without involving any response variables. It is “unsupervised” in the sense that the learning outcome is subjective, unlike supervised learning in which specific metrics such as error rates are used to evaluate learning outcomes. PCASee the PCA section below. K-Mean ClusteringClustering seek to partition data into homogeneous subgroups. The K-Mean clustering partitions data into K distinct and non-overlapping clusters C, by minimizing the objective function of total in-cluster variation W(C), which is the sum of all pair-wise squared Euclidean distances between the observations in the cluster, divided by the number of observations in the cluster. \\text{minimize } \\{\\ \\sum_{k=1}^K W(C_k)\\ \\}AlgorithmThe algorithm divides data into K initial cluster, and reassign observations to the cluster with the closest cluster centroid (mean of all previous observations in the cluster). The K-Mean clustering algorithm finds a local optimum, and therefore depend on the initial cluster. So it is important to repeat the process with different initial points, and then select the best result based on minimum total in-cluster variation. The nstart parameter in the kmean function in R specifies the number of random starting centers to try and the one ended with the optimal objective function value will be selected. It is also important to check for outliers, as the algorithm would let the outlier become its own cluster and stops improving. StandardizationIt is a must to standardize the variables before performing k-mean cluster analysis, as the objective function (Euclidean distance, etc.) is calculated from the actual value of the variable. Curse of DimensionalityThe curse of dimensionality describe the problems when performing clustering on three or more dimensional space, where: visualization becomes harder as the number of dimensions increases, the Euclidean distance between data points are the same on average. The solution is to reduce the dimensionality before using clustering technique. The Elbow MethodEach cluster replaces its data with its center. In other words, with a clustering model we try to predict which cluster a data point belongs to. A good model would explain more variance in the data with its cluster assignments. The elbow method looks at the F statistics defined as: F = \\dfrac{\\text{between-group variance}}{\\text{total variance}}As soon as the additional F statistics drops/stops increasing when adding a new cluster, we use that number of clusters. Hierarchical ClusteringThe hierarchical clustering provide flexibilities in terms of the number of clusters K. It results in a tree-based representation of the data called dendrogram, which is built either bottom-up/agglomerative or top-down/divisive. Hierarchical clustering assumes that there exists a hierarchical structure. In most feature generation cases, we prefer k-means clustering instead. AgglomerativeAn agglomerative hierarchical cluster starts off by assigning each data point in its own cluster. Each step in the clustering process two similar clusters with minimum distance among all are merged, where the distance is calculated between the elements within the cluster that are closest (single-linkage) or furthest (complete-linkage) PCA - A Deeper Dive &#8634;PCA finds low dimensional representation of a dataset that contains as much as possible of the variation. As each of the n observations lives on a p-dimensional space, and not all dimensions are equally interesting. Linear Algebra ReviewLet A be a n\\times n matrix. With n=2, \\ 3, the determinant of A can be calculated as follow. det(\\begin{bmatrix} a & b \\\\ c & d \\\\ \\end{bmatrix} ) = ad - bc\\begin{align} det ( \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\\\ \\end{bmatrix} ) &= aei + bfg + cdh - cdg - bdi - afh \\\\ &= a \\times det( \\begin{bmatrix} e & f \\\\ h & i \\\\ \\end{bmatrix} ) + b \\times det( \\begin{bmatrix} d & f \\\\ g & i \\\\ \\end{bmatrix} ) + c \\times det( \\begin{bmatrix} d & e \\\\ g & h \\\\ \\end{bmatrix} ) \\end{align}Properties of determinant: \\begin{align} det(A^T) &= det(A) \\\\ det(A^{-1}) &= det(A)^{-1} \\\\ det(AB) &= det(A)det(B) \\end{align}A real number \\lambda is an eigenvalue of A if there exists a non-zero vector x (eigenvector) in \\mathbb{R}^n such that: Ax = \\lambda xThe determinant of matrix A - \\lambda I is called the characteristic polynomial of A. The equation det(A - \\lambda I) is called the characteristic equation of A, where the eigenvalues \\lambda are the real roots of the equation. It can be shown that: \\prod_{i=1}^n \\lambda_i = det(A) \\\\ \\sum_{i=1}^n \\lambda_i = \\sum_{i=1}^n a_{i, \\ i} = trace(A)Matrix A is invertible if there exists a n\\times n matrix B such that AB = BA = I. A square matrix is invertible if and only if its determinant is non-zero. A non-square matrix do not have an inverse. Matrix A is called diagonalizable if and only if it has linearly independent eigenvectors. Let \\textbf{U} denote the eigen vectors of A and \\textbf{D} denote the diagonal \\lambda vector. Then: A = \\textbf{UDU}^{-1} \\rightarrow A^x = \\textbf{UD}^x\\textbf{U}^{-1}If matrix A is symmetric, then: all eigenvalues of A are real numbers all eigenvectors of A from distinct eigenvalues are orthogonal Matrix A is positive semi-definite if and only if any of the following: for any n\\times 1 matrix x, x^TAx \\geq 0 all eigenvalues of A are non-negative all the upper left submatrices A_K have non-negative determinants. Matrix A is positive definite if and only if any of the following: for any n\\times 1 matrix x, x^TAx > 0 all eigenvalues of A are positive all the upper left submatrices A_K have positive determinants. All covariance, correlation matrices must be symmetric and positive semi-definite. If there is no perfect linear dependence between random variables, then it must be positive definite. Let A be an invertible matrix, the LU decomposition breaks down A as the product of a lower triangle matrix L and upper triangle matrix U. Some applications are: solve Ax=b: LUx=b \\rightarrow Ly=b \\text{ ; } Ux=y solve det(A): det(A) = det(L)\\ det(U)=\\prod L_{i, \\ i}\\prod U_{j, \\ j} Let A be a symmetric positive definite matrix, the Cholesky decomponsition expand on the LU decomposition and breaks down A=U^TU, where U is a unique upper triangular matrix with positive diagonal entries. Cholesky decomposition can be used to generate correltaed random variables in Monte Carlo simulation Matrix InterpretationConsider a n\\times p matrix: \\begin{bmatrix} x_{11} & x_{12} & x_{13} & \\dots & x_{1p} \\\\ x_{21} & x_{22} & x_{23} & \\dots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & x_{n3} & \\dots & x_{np} \\end{bmatrix}To find the first principal component F^1, we define it as the normalized linear combination of X that has the largest variance, where its loading \\phi^1_j are normalized: \\sum^p_{j=1} (\\phi^1_j)^2 = 1 F^1 = \\phi^1_1X_1 + \\phi^1_2X_2 + \\dots + \\phi^1_pX_pOr equivalently, for each score: F^1_i = \\sum_{j=1}^{p} \\phi^1_jx_{ij} In matrix form: \\begin{bmatrix} x_{11} & x_{12} & x_{13} & \\dots & x_{1p} \\\\ x_{21} & x_{22} & x_{23} & \\dots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & x_{n3} & \\dots & x_{np} \\end{bmatrix}\\times \\begin{bmatrix} \\phi^1_1 \\\\ \\phi^1_2 \\\\ \\vdots \\\\ \\phi^1_p \\\\ \\end{bmatrix}= \\begin{bmatrix} f^1_1 \\\\ f^1_2 \\\\ \\vdots \\\\ f^1_n \\\\ \\end{bmatrix}Finally, the first principal component loading vector \\phi^1 solves the optimization problem that maximize the sample variance of the scores f^1. An objective function can be formulated as follow and solved via an eigen decomposition: \\text{maximize }\\{\\ \\dfrac{1}{n}\\sum_{i=1}^n(f^1_i)^2\\ \\} \\text{ subject to } \\sum^p_{j=1} (\\phi^1_j)^2 = 1To find the second principal component loading \\phi^2, use the same objective function with \\phi^2 replacement and include an additional constraint that \\phi^2 is orthogonal to \\phi^1. Geometric InterpretationThe p\\times k loading matrix L = [\\phi^1 \\dots \\phi^k] defines a linear transformation that projects the data from the feature space \\mathbb{R}^p into a subspace \\mathbb{R}^k, in which the data has the most variance. The result of the projection is the factor matrix F = [F^1 \\dots F^k], also known as the principal components. \\underbrace{ \\begin{bmatrix} x_{11} & x_{12} & x_{13} & \\dots & x_{1p} \\\\ x_{21} & x_{22} & x_{23} & \\dots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & x_{n3} & \\dots & x_{np} \\end{bmatrix} }_{data} \\times \\underbrace{ \\begin{bmatrix} \\phi^1_1 & \\dots & \\phi^k_1\\\\ \\phi^1_2 & \\dots & \\phi^k_2\\\\ \\vdots & \\ddots & \\vdots \\\\ \\phi^1_p & \\dots & \\phi^k_p\\\\ \\end{bmatrix} }_{loadings}= \\underbrace{ \\begin{bmatrix} f^1_1 & \\dots & f^k_1 \\\\ f^1_2 & \\dots & f^k_2 \\\\ \\vdots & \\ddots & \\vdots \\\\ f^1_n & \\dots & f^k_n \\\\ \\end{bmatrix} }_{\\text{principal components} \\\\ \\text{or, factor scores}}In other words, the principal components vectors F^1 ... F^k forms a low-dimensional linear subspace that are the closest (shortest average squared Euclidean distance) to the observations. Eigen DecompositionGiven n\\times p data matrix X, the objective of PCA is to find a lower dimension representation factor matrix F, from which a n\\times p matrix \\tilde{X} can be constructed where distance between the covariance matrices cov(X) and cov(\\tilde{X}) are minimized. The covariance matrix of X is a p\\times p symmetric positive semi-definite matrix, therefore we have the following decomposition where \\textbf{u}‘s’ are p\\times 1 eigenvectors of cov(X) and \\lambda‘s are the eigenvalues. Note that \\textbf{u} can be a zero vector if the columns of cov(X) are linearly dependent. \\begin{align} cov(X) &= \\dfrac{1}{n-1}X^TX \\\\ &=\\dfrac{1}{n-1} \\begin{bmatrix} \\textbf{u}_1 & \\dots & \\textbf{u}_p \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & \\dots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & \\lambda_p \\end{bmatrix} \\begin{bmatrix} \\textbf{u}_1 & \\dots & \\textbf{u}_p \\end{bmatrix}^T \\\\ &= \\dfrac{1}{n-1}\\sum_{i=1}^p \\lambda_i\\textbf{u}_i\\textbf{u}_i^T \\end{align}If we ignore the constant 1/(n-1), and define the p\\times p loading matrix L_0=[\\textbf{u}_1, \\dots, \\textbf{u}_p] and n\\times p factor matrix F_0 where F_0^TF_0=\\Lambda. Then: X = F_0L_0^TNow comes the PCA idea: Let’s rank the \\lambda_i‘s in descending order, pick k < p such that: \\dfrac{1}{n-1}\\sum_{i=1}^{k} \\lambda_i\\textbf{u}_i\\textbf{u}_i^T \\approx \\dfrac{1}{n-1}\\sum_{i=1}^p \\lambda_i\\textbf{u}_i\\textbf{u}_i^T = cov(X) \\\\ \\text{and we denote it } cov(\\tilde{X}) \\text{, i.e. } cov(\\tilde{X}) = \\dfrac{1}{n-1}\\sum_{i=1}^{k} \\lambda_i\\textbf{u}_i\\textbf{u}_i^TNow we observe that the matrix cov(\\tilde{X}) is also a p\\times p positive semi-definite matrix. Following similar decomposition, we obtain a p\\times k matrix L and n\\times k matrix F, where: \\tilde{X} = FL^THere we have it, a dimension-reduced n\\times k factor matrix F, where its projection back to n\\times p space, \\tilde{X}, has similar covariance as the original n\\times p dataset X. Practical ConsiderationsPCA excels at identifying latent variables from the measurable variables. PCA can only be applied to numeric data, while categorical variables need to be binarized beforehand. Centering: yes. Scaling: if the range and scale of the variables are different, correlation matrix is typically used to perform PCA, i.e. each variables are scaled to have standard deviation of 1 otherwise if the variables are in the same units of measure, using the covariance matrix (not standardizing) the variables could reveal interesting properties of the data Uniqueness: each loading vector \\phi^1 is unique up to a sign flip, as the it can take on opposite direction in the same subspace. Same applies to the score vector Z^1, as var(Z^1) = var(-Z^1) Propotional of Variance Explained: we can compute the total variance in a data set in the first formula below. The variance explained by the m-th principal component is: \\dfrac{1}{n} \\sum_{i=1}^n (z^m_i)^2. Therefore, the second formula can be computed for the PVE: \\sum_{j=1}^p Var(X_j) = \\sum_{j=1}^p [ \\dfrac{1}{n} \\sum_{i=1}^n x^2_{ij} ] \\\\ PVE^m = \\dfrac{\\sum_{i=1}^n (z^m_i)^2}{\\sum_{j=1}^p\\sum_{i=1}^n x^2_{ij}} A Side Note on Hypothesis Test and Confidence Interval &#8634;A formal statistical data analysis includes: hypothesis testings: seeing whether a structure is present confidence interval: setting error limits on estimates prediction interval: setting error limits on future outcomes We will discuss a typical setting as follow: Let \\beta be the unknown parameter, and \\hat{\\beta}_n is the parameter estimator based on n observations Let \\hat{\\sigma}^2_n be the estimator of \\sigma^2_n, equal to var(\\sqrt{n}(\\hat{\\beta}_n - \\beta)) In rare circumstances where \\sqrt{n}(\\hat{\\beta}_n - \\beta)/\\hat{\\sigma} is independent of the unknown \\beta, such as in regression settings where the error terms \\epsilon_i are i.i.d. normally distributed, we can show that it follows a t-distribution with n-1 degree of freedom. \\dfrac{\\sqrt{n}(\\hat{\\beta}_n - \\beta)}{\\hat{\\sigma}} \\sim t_{n-1} However, asymptotically as n\\rightarrow\\infty, the Central Limit Theorem applies which release us from the assumption restriction above. Under a variety of regular conditions: \\lim_{n\\rightarrow\\infty} \\dfrac{\\sqrt{n}(\\hat{\\beta}_n - \\beta)}{\\hat{\\sigma}} \\sim \\mathcal{N}(0, 1) The 1-\\alpha confidence interval can then be computed as: \\beta \\in CI = \\hat{\\beta}_n \\pm t^{\\alpha}_{n-1}\\hat{\\sigma}_n/\\sqrt{n} \\\\ \\text{or asymptotically, swapping } t^{\\alpha}_{n-1} \\text{ by } z^{\\alpha} The 1-\\alpha hypothesis test of \\beta=\\beta_0 would: accept H_0 that \\beta=\\beta_0 if \\beta \\in CI reject H_0 that \\beta=\\beta_0 if \\beta \\notin CI The p-value is \\alpha such that the hypothesis test is indifference between accept or reject. In practice, t-distribution is typically used for regression data as it is more conservative (t^{\\alpha}_{n-1} > z^{\\alpha}). For non-regression data, normal distribution is often used. Some background on LLN and CLT: Given n i.i.d. random variable X_i. Let \\bar{X} = \\sum X_i/n The Law of Large Numbers states that if \\mathbb{E}|X|","tags":"math"},{"title":"&#128214; Notes on C++","url":"/2019/04/cpp/","text":"C++ is a complied （vs interpreted: python), general-purpose (vs domain-specific: HTML) programming language created by Danish programmer Bjarne Stroustrup as an extension to C. BasicCompilerA compiler translate a high level language into a low level language and create an executable program. Pre-processor: read preprocessing lines #include &quot;foo.hpp&quot; Compiler: turn the above code it into assembly code (ASM). front end create IR (intermediate representation) with SSA (static singale assignment). The runtime is O(n). middle end optimize IR. remove unnecessary operations, O(n^2) or more. back end produce ASM Assembler: turn ASM into binary code Linker: link all relevant headers, libraries together Debugger: type checking Object Copy: generate .exe (for windows), and .bin (for mac) G++Compile with g++ at the command line:123$ g++ toto.cpp$ g++ toto.cpp -E (show c pre-processor)$ g++ toto.cpp --verbose (ask compile to give different steps) Running the complied result:1$ /a.exe HeaderThe C++ standard library is a collection of classes and functions, represented by different headers. For example, include the &lt;iostream&gt; header to handle input and outputs and other non-standard headers using double quoto.12#include &lt;iostream&gt;#include \"foo.h\" Macro12define N 4std::cout &lt;&lt; N + 2; // show 6 GuardsIn C++, function, class and variable can only be declared once. We use guards to make sure we do not duplicate declaration in multiple files.12#ifndef &quot;foo.h&quot;#define &quot;foo.h&quot; NamespaceSome classes and functions are grouped under the same name, which divides the global scope into sub-scopes, each with its own namespaces. Functions and classes in the C++ standard library are defined in the std namespace. For example, the cin (standard input), cout (standard output) and end (end line) objects.1234char c;std::cin &gt;&gt; c;std::cout &lt;&lt; c;std::endl; Alternatively, we can use using namespace std;. Data TypeEvery variable has to have a type in C++, and the type has to be declared and cannot be changed. There are fundamental types and user-defined types (classes) Characters In computer, each bit stores a binary (0/1) value. A byte is 8 bits. The computer stores characters in a byte using the ASCII format. Numbers The computer stores numbers in binary format with bits. The leftmost bit is used to store the sign of a number. (See twos-complement method). Real values are stored using a mantissa and an exponent: Value = Mantissa \\times 2^{Exponent}Note that very few values can be exactly represented, and how close we can get depends on the number of bits available. Type Size (Bytes) Value Range bool 1 true or false char 1 -128 to 127 short 2 -32,768 to 32,767 int 4 -2,147,483,648 to 2,147,483,647 float 4 3.4E +/- 38 double 8 1.7E +/- 308 C++ is a strongly typed language, which means type errors needs to be resolved for all variables at compile time. FunctionEvery console application has to have a main() function, which takes no argument and returns an integer value by default. A function that adds two numbers:12345678910111213#include &lt;iostream&gt;using namespace std;int Add(int a, int b)&#123; return a+b;&#125;int main()&#123; int result = Add(2, 3); cout &lt;&lt; \" Result: \" &lt;&lt; result &lt;&lt; endl;&#125; Overloading allows 2 or more functions to have the same name, but they must have different input argument types. Function ObjectFunction object, or functors, are objects that behave like functions, are functions with state. A regular function looks like this:12345int AddOne(int val)&#123; return val+1;&#125;int result = AddOne(2) A function object implementaion:123456789101112class AddOne&#123;public: int operator()(int&amp; val) &#123; return val+1; &#125;&#125;;AddOne addone;int val = 2;int result = addone(val) LambdaLambdas is a new feature introduced in C++11, which is an inline function that can be used as a parameter or local object.1234[] (string s) // [] is the lambda introducer/capture clause&#123; cout &lt;&lt; s &lt;&lt; endl;&#125; Example 1123vector&lt;int&gt; v&#123;1, 3, 2, 4, 6&#125;;for_each(v.cbegin(), v.cend(), //range [](int elem) &#123;cout &lt;&lt; elem &lt;&lt; endl;&#125;) //lambda Example 2123vector&lt;int&gt; v&#123;1, 3, 2, 4, 6&#125;;transform(v.begin(), v.end(), v.begin(), [] (int elem) &#123;return elem * elem&#125;); Example 31234567vector&lt;Person&gt; ppl;sort(ppl.begin(), ppl.end(), [](const Person&amp; p1, const Person&amp;p2) &#123; if (p1.GetAge() &lt; p2.GetAge()) return true; else return false; &#125;); ExternThe keyword extern means the function is declared in another file.12extern int foo(int a);int main() &#123; return foo(100); &#125; Inline FunctionC++ provides inline funcitons such that the overhead of a small function can be reduced. When inline function is called the entire code of the function is inserted at the point of the inline function call. TypedefUse typedef keyword to define a type alias.1234typedef double OptionPrice;typedef double StockPrice;typedef double Strike;OptionPrice BSPrice(StockPrice S, Strike K) OperatorsStandard operations:123456789Arithmetic: +, -, *, /Comparison: &lt;, &gt;, &lt;=, &gt;=Negate: !Equality, non Equality: ==, !=Logical and, or, &amp;&amp;, ||Assignment: =Modulo: %Increment, Decrement: i++, i--Multiple Operations: i += 1, i -= 1, i *= 1, i /= 1 Note the difference between i++ and ++i12i++; // return (old) i and increment i++i; // increment i and return new i ConstUse the const keyword to define a constant value. The compiler will stop any attempt to alter the constant values. Since C++ is a strongly typed language, it is preferred to use const int N = 4, instead of #define N 4, as the former defines a type. ReferenceExample 1 A reference is an alias for a variable and cannot rebind to a different variable. We can change val by changing ref:123int val = 10;int&amp; ref = val;ref = 20; // this will change val to 20 Example 2 We can also bind a const reference to a const object. An error will be raised if attempt to change the value or the reference.1234const int val = 10;const int&amp; ref = val;val = 20; // errorref = 20; // error Example 3 We can also bind a const reference to a non-const object, thereafter we can NOT change the object using the reference.1234int val = 10;const int&amp; ref = val;val = 20; // okref = 20; // error Pass By Value In a function, we can pass an argument by either value or reference. When passing by value, the variable x will NOT be changed. In this case, we waste time to both create a copy inside the function and memory to store the copy1234567891011void DoubleValue(int number)&#123; number = number * 2;&#125;int main()&#123; int x = 5; DoubleValue(x); cout&lt;&lt;\"x = \"&lt;&lt;x&lt;&lt;endl;&#125; 1x = 5 Pass By Reference When passing by reference (by adding &amp; in the function argument parameter), the variable x WILL be changed.1234567891011void DoubleValue(int&amp; number)&#123; number = number * 2;&#125;int main()&#123; int x = 5; DoubleValue(x); cout&lt;&lt;\"x = \"&lt;&lt;x&lt;&lt;endl;&#125; 1x = 10 Pass By Const Reference We add const when we do not want the specific function argument to be tempered when passed by reference. In this example, there will be a compiler error as we are trying to change the const reference number in the function.1234567891011void DoubleValue(const int&amp; number)&#123; number = number * 2; // error, cannot change const ref \"number\"&#125;int main()&#123; int x = 5; DoubleValue(x); cout&lt;&lt;\"x = \"&lt;&lt;x&lt;&lt;endl;&#125; PointerIn computer memory, each stored values has an address associated with it. We use a pointer object to store address of another object and access it indirectly. There are two pointer operator: &amp;: address of operator, used to get the address of an object *: de-reference operator, used to access the object Example 1123int* ptr = nullptr; // initiate an empty pointerint* ptr = &amp;val; // initiate ptr with the address of val*ptr = 20; // change val using the ptr pointer Example 2 If the object is const, a pointer cannot be used to change it.123const int val = 10;const int* ptr = &amp;val;*ptr = 20; // error Example 3 You can have a pointer that itself is const123456int val = 10;int* const ptr = &amp;val;*ptr = 20; // okint val2 = 20;ptr = &amp;val2 // error, as the pointer is const CastingC++ allows implicit and explicit conversions of types.1234short a = 1;int b;b = a; // implicit conversionb = (int) a; // explicit conversion However, the traditional explicit type-casting allows conversions between any types, and leads to run-time error. To control these conversions, we introduce four specific casting operators: dynamic_cast&lt;new_type&gt;( ): used only with pointers (and/or references to objects); can cast a derived class to its base class; base-to-derived conversions are allowed only with polymorphic base class 1234567891011121314151617181920class Base &#123;virtual void foo() &#123;&#125; &#125;;class Derived : public Base &#123; &#125;;int main() &#123; Derived* derived_ptr; Base* base_ptr = dynamic_cast&lt;Base*&gt; (derived_ptr); Base* base_ptr_2 = new Derived; Derived* derived_ptr_2 = dynamic_cast&lt;Derived*&gt; (base_ptr_2); // ok, base class polymorphic Base* base_ptr_3 = new Base; Derived* derived_ptr_3 = dynamic_cast&lt;Derived*&gt; (base_ptr_3); // will not work, derived_ptr_3 will be assigned a nullptr std::cout &lt;&lt; \"derived_ptr_2: \" &lt;&lt; derived_ptr_2 &lt;&lt; std::endl; std::cout &lt;&lt; \"derived_ptr_3: \" &lt;&lt; derived_ptr_3 &lt;&lt; std::endl; return 0;&#125; 12derived_ptr_2: 0x7fa5cec00630derived_ptr_3: 0x0 static_cast &lt; new_type&gt;( ): used only with pointers (and/or references to objects); can cast base-to-derived or derived-to-base, but no safety check at run-time; 123Base* base_ptr_3 = new Base;Derived* derived_ptr_3 = static_cast&lt;Derived*&gt; (base_ptr_3);// not nullptr this time, but lead to error when de-referencing derived_ptr_3 1derived_ptr_3: 0x7fc3d7400690 reinterpret_cast &lt;new_type&gt;( ): convert pointer to another unrelated class; often lead to unsafe de-referencing 12345class A &#123;&#125;;class B &#123;&#125;;A* a = new A;B* b = reinterpret_cast&lt;B*&gt; (a); const_cast &lt;new_type&gt;( ): remove/set the constant-ness of an object Array (C-Style)An array is a fixed collection of similar kinds of items that are stored in a contiguous block in memory. We define the size of the array at creation, and the array index starts a 0 in C++.12int a[10];int a[] &#123;1, 2, 3&#125; // uniform initializer syntax The address of the array is the same as the address of the first element of the array. Therefore, we can access an array using pointer increment - very efficient.1234int a[10];int* ptr = &amp;a[0]; // the same as int* ptr = aint a0 = a[0]; // the same as int a0 = *ptrint a3 = a[3]; // the same as int a3 = *(ptr+3) or *(a+3) Dynamic AllocationDynamic memory allocation is necessary when you do NOT know the size of the array at compile time. We use a new keyword paired with a delete keyword.123int* a = new int[10];delete[] = a; // correct. this tells the CPU that it needs to clean up multiple variables instead of a single variabledelete a; // incorrect. using this version will lead to a memory leak. Dynamic allocate a 4\\times4 matrix with cast.1234567891011121314151617181920212223#include &lt;iostream&gt;void func(double** a) &#123; * a = new double[16];&#125;int main() &#123; int (* a)[4]; func( (double**)&amp;a ); for (int i=0; i&lt;4; i++) &#123; for (int j=0; j&lt;4; j++) &#123; a[i][j] = 1; &#125; &#125; for (int i=0; i&lt;4; i++) &#123; for (int j=0; j&lt;4; j++) &#123; std::cout &lt;&lt; a[i][j] &lt;&lt; \" \" ; &#125; std::cout &lt;&lt; std::endl; &#125;&#125; 123451 1 1 11 1 1 11 1 1 11 1 1 1` LibraryA C++ library is a package of reusable code typically with these two components: header file precompiled binary containing the machine code for functionality implemntation There are two types of c++ libraries: static and dynamic libraries. a static library has a .a (.lib on Windows) extension and the library codes are complied as part of the executable - so that user only need to distribute the executable for other users to run the file with a static library. a dynamic library has a .so (.dll on Windows) extension and is loaded at run times. It saves space as many program can share a copy of dynamic library code, and it can be upgraded to new versions without replacing all the executables using it. ConditionIf/Else123456789101112if (condition_1)&#123; statement1;&#125;else if (condition_2)&#123; statement2;&#125;else&#123; statement2;&#125; SwitchA switch statement tests an integral or enum value against a set of constants. we can NOT use a string in the switch statement.12345678910111213141516int main()&#123; int value = 0; cin &gt;&gt; value; switch(value) &#123; case 0: cout &lt;&lt; \"value is zero\"; break; // if remove this break, it will also show case 1 even if value is 0 case 1: cout &lt;&lt; \"value is one\"; break; default: cout &lt;&lt; \"value is not 0 or 1\"; &#125;&#125; While / Do While / For LoopWhile loop:123456int n = 0;while (n &lt; 10)&#123; cout &lt;&lt; \" n: \" &lt;&lt; n &lt;&lt; endl; n = n + 1;&#125; Do while loop:123456do &#123; cout &lt;&lt; \"Enter number (0 to end): \"; cin &gt;&gt; n; cout &lt;&lt; \"You entered: \" &lt;&lt; n &lt;&lt; \"\\n\"; &#125; while (n != 0); For loop:1234for (unsigned int n = 0; n &lt; 10; ++n)&#123; cout &lt;&lt; \"n: \" &lt;&lt; n &lt;&lt; endl;&#125; For loop with two variables:1234for (unsigned int i = 0, j = 0; i &lt; 10 &amp;&amp; j &lt; 10; ++i, j+=2)&#123; cout &lt;&lt; \"i:\" &lt;&lt; i &lt;&lt; \", j:\" &lt;&lt; j &lt;&lt; endl;&#125; EnumThe enum (enumerated) type is used to define collections of named integar constants.1234567enum CurrencyType &#123;USD, EUR, GBP&#125;;cout &lt;&lt; USD &lt;&lt; \" \" &lt;&lt; EUR &lt;&lt; \" \" &lt;&lt; GBP;0 1 2enum CurrencyType &#123;USD, EUR=10, GBP&#125;;cout &lt;&lt; USD &lt;&lt; \" \" &lt;&lt; EUR &lt;&lt; \" \" &lt;&lt; GBP;0 10 11 ClassA class achieve data abstraction and encapsulation. abstraction refers to the separation of interface and implementation encapsulation refers to combining data and functions so that data is only accessible through functions. Member Variable &amp; FunctionDefine a customer class with member variable and function.123456789101112131415class Customer&#123;public: Customer(); // default constructor Customer(string name, string address); ~Customer(); // destructor, to free up resources string GetName(); string GetAddress(); void SetAddress(string address);private: string name_; string address_;&#125;; Instantiate Customer class instances to represent different customer.1234567Customer c1(\"Joe\", \"Hyde Park\");Customer c2(\"Jim\", \"Chicago\");Customer c3(\"John\", \"New York\");// Use `.` to access member function.c1.GetName()c2.SetAddress(\"Beijing\") Protection LevelThere are three protection levels to keep class data member internal to the class. public accessible to all. protected accessible in the class that defines them and in classes that inherit from that class. private only accessible within the class defining them. Constructor / DestructorA constructor is a special member functions used to initialize the data members when an object is created. This is an example to use initializer list to create more efficient constructors123456789101112131415Customer::Customer() : name_(\"\"), address_(\"\")&#123; // name_ = \"\"; // address_ = \"\";&#125;Customer::Customer(string name, string address) : name_(name), address_(address)&#123;&#125;Customer::~Customer()&#123;&#125; Free-StoreThere are several ways to create objects on a computer: Automatic/Stack int a; Dynamic Allocated Free Store int* ptr = new a[10]; Heap allocated/freed by malloc/free Summarized in a table from geeksforgeeks Parameter Stack Heap Basic Memory is allocated in a contiguous block Memory is allocated in any random order Allocated and de-allocation Automatic by compiler instructions Manual by programmer Cost Less More Access time Faster Slower Main issue Shortage of memory Memory leak/fragmentation We use -&gt; to access free-store object’s member functions:123Customer* c = new Customer(\"Joe\", \"Chicago\");c-&gt;GetName()c-&gt;SetAddress(\"New York\") Const Member FunctionsA const object can only invoke const member function on the class. A const member function is not allowed to modify any of the data members on the object on which it is invoked. However, if a data member is marked mutable, it then can be modified inside a const member function.12const Customer c1(\"Joe\", \"Hyde Park\");cout &lt;&lt; c1.GetName(); // ok if GetName() is a const member function. Static MemberWe use static keyword to associate a member with the class, as oppose to class instances. A static data member can NOT be accessed directly using a non-static member function. Static member variables can NOT be initialized through the class constructor, rather, they are initialized once outside the class body. However, a const static member variable can be initialized within the class body.123456789101112131415class Counter&#123;public: Counter(); static int GetCount(); static void Increment();private: static int count_; // non-const static need to be initialized outside const static int count_2_ = 0; // const static can be initialized within&#125;;int Counter::count_ = 0;Counter c;c.Increment(); // or Counter::Increment() ThisEvery non-static member function has access to a this pointer, which is initialized with the address of the object when the member function is invoked.123456double Currency::GetExchangeRate()&#123; return exchangeRate_; return this-&gt;exchangeRate_; // equivalent return (*this).exchangeRate_; // equivalent&#125; Copy ConstructorWe use the copy constructor to construct an object from another already constructed object of the same type.1234567891011class Customer&#123; Customer(const Customer&amp; other);&#125;;Customer::Customer(const Customer&amp; other) : name_(other.name_) address_(other.address_)&#123;&#125;Customer c2(c1); Assignment OperatorWe use the assignment operator to assign an object of the same type.123456789101112131415class Customer&#123; Customer&amp; operator=(const Customer&amp; other);&#125;;Customer&amp; Customer::operator=(const Customer&amp; other)&#123; if (this != &amp;other) //checking for self assignment &#123; name_ = other.name_; address_ = other.address_; &#125; //return the object on which the function was invoked return (*this);&#125; Shallow / Deep CopyThe default copy constructor and assignment operator provides shallow copy, which copies each member of the class individually. For pointer member, the shallow copying copies the address of the pointer, resulting in both members pointing to the same object on the free store. A deep copy, however, creates a new object on the free store and copy the contents of the object the original pointer is pointing to. Deep Copy copy constructor123456Customer::Customer(const Customer&amp; other) :name_(other.name_), address_(other.address_), account_(new Account(other.account_-&gt;GetAccountNumber(), other.account_-&gt;GetAccountBalance()))&#123;&#125; Deep Copy assignment operator123456789101112Customer&amp; Customer::operator=(const Customer&amp; other)&#123; if (this != &amp;other) &#123; name_ = other.name_; address_ = other.address_; delete account_; account_= new Account(other.account_-&gt;GetAccountNumber(), other.account_-&gt;GetAccountBalance()); &#125; return (*this);&#125; The Rule of 3There are 3 operations that control the copies of an object: copy constructor, assignment operator, and destructor. If you define one of them, you will most likely need to define the other two as well. Singleten ClassThe Singleton design pattern makes sure only one instance of an object of a given type is instantiated in a program, and provides a global point of access to it change the access level of the constructor to private add new public member function Instance() to create the object use static member variable to hold the object 12345678910class CurrencyFactory&#123;public: static CurrencyFactory* Instance(); Currency CreateCurrency(int currencyType);private: CurrencyFactory(); static CurrencyFactory* instance_;&#125;; 1234567891011121314151617181920212223CurrencyFactory* CurrencyFactory::Instance()&#123; if (!instance_) instance_ = new CurrencyFactory; return instance_; // no more than one CurrencyFactory object.&#125;Currency CurrencyFactory::CreateCurrency(int currencyType)&#123; switch(currencyType) &#123; case EUR: return Currency(\"EUR\", 0.7901); case GBP: return Currency(\"GBP\", 0.6201); case CAD: return Currency(\"CAD\", 1.1150); case AUD: return Currency(\"AUD\", 1.1378); default: return Currency(\"USD\", 1.0); &#125;&#125; 123456789101112131415#include \"CurrencyFactory.h\"int main()&#123; cout &lt;&lt; \"Enter amount in USD:\"; double amount; cin &gt;&gt; amount; cout &lt;&lt; \"Enter currency to convert to (ECU/GBP/CHF/JPY): \"; string symbol; cin &gt;&gt; symbol; double convertedAmount = 0.0; Currency currency = CurrencyFactory::Instance()-&gt;CreateCurrency(symbol); cout &lt;&lt; currency.ConvertFromUSD(amount) &lt;&lt; endl;&#125; InheritanceClasses related by inheritance form a hierachy consisting of base and derived classes. The derived class inherit some members from the base class subject to protection level restrictions, and may extend/override implementation of member functions in the base class.12345678910class Person&#123;protected: string name_; string address_;&#125;;class Student : public Person&#123; string school_;&#125;; VirtualDifferent derived classes may inplement member functions from the base class differently. The base class uses virtual keyword to indicate a member function that may be specialized by derived classes.12345678910111213class Base&#123;public: virtual void Method1(); virtual void Method2(); void Method3();&#125;;class Derived : public Base&#123; void Method1(); // specializes Method1() // uses default implementation of Method2() // can NOT specialize Method3()&#125;; Abstract ClassThe base class has to either provide a default implementation for that function or declare it pure virtual. If a class has one or more pure virtual function, it is called an abstract class or interface. An abstract class cannot be instantiated.123456789class Base&#123;public: virtual void Method1() = 0;&#125;;class Derived : public Base&#123; // this derived is also an abstract&#125;; Virtual DestructorWhen we delete a derived class we should execute both the derived class destructor and the base class destructor. A virtual base class destructor is needed to make sure the destructors are called properly when a derived class object is deleted through a pointer to a base class. If we delete a derived class object through a pointer to a base class when the base class destructor is non-virtual, the result is undefined. PolymorphismThe types related by inheritance are known as polymorphic. types. We can use polymorphic types interchangeably. We can use a pointer or a reference to a base class object to point to an object of a derived class – this is known as the Liskov Substitution Principle (LSP). This allows us to write code without needing to know the dynamic type of an object 12345BankAccount* acc1 = new Savings();acc1-&gt;ApplyInterest(); // ApplyInterest() on the Savings objectBankAccount* acc2 = new Checking();acc2-&gt;ApplyInterest(); // ApplyInterest() on the Checking object We can write one function which applies to all account types.12345void UpdateAccount(BankAccount* acc)&#123; acc-&gt;ApplyBankingFees(); acc-&gt;ApplyInterest();&#125; 12345void UpdateAccount(BankAccount&amp; acc)&#123; acc.ApplyBankingFees(); acc.ApplyInterest();&#125; Standard Template Library (STL)Sequential Containerstd::arrayThe STL array class from offers a more efficient and reliable alternative for C-style arrays, where size is known and we do not have to pass size of array as separate parameter.12345678#include &lt;array&gt;array &lt;int&gt; a1 = &#123;1, 2, 3&#125;;a1.front();a1.back();a1.size();a1.at(1);get&lt;1&gt;(a1); std::vectorVectors are the stored contiguously same as dynamic arrays with the ability to resize itself automatically when an element is inserted or deleted. Vector size is double whenever half is reached.12345678910111213#include &lt;vector&gt;vector&lt;int&gt; v1;v1.begin();v1.end();v1.size();v1.push_back(); // pushes the elements into a vector from the backv1.pop_back(); // removes the elements from a vector from the back.v1.insert(i); // inserts new elements before the element at the specified positionv1.assign(i); // assigns new value to the vector elements by replacing old onesv1.erase(i); // removes elements from a container from the specified position or range std::listDifferent from arrays and vectors, A list is a sequential container that allows non-contiguous memory allocation.123456789101112131415#include &lt;list&gt;list&lt;int&gt; l1;for (int i = 0; i &lt; 10; i++) &#123; l1.front(); // returns the value of the first element l1.back(); // returns the value of the last element l1.push_front(i); // adds a new element ‘i’ at the beginning of the list l1.push_back(i); // adds a new element ‘i’ at the back of the list l1.pop_front(); // removes the first element and reduces list size by 1 l1.pop_back(); // removes the last element and reduces list size by 1 l1.begin(); // returns an iterator pointing to the first element of the list l1.end(); // returns an iterator pointing to the last element of the list &#125; std::stringThe STL string class stores the characters as a sequence of bytes, allowing access to single byte character. Any string is terminated by a \\0, so the string foo actually stores four characters. size()The use sizeof() to return the size of an array in bytes. Use .size() member function to return the number of elements in a STL container.1234567891011121314#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;int main() &#123; int a[5] &#123;1, 2, 3, 4, 5&#125;; cout &lt;&lt; \"The size of a: \" &lt;&lt; sizeof(a) &lt;&lt; \" bytes\" &lt;&lt; endl; vector&lt;int&gt; b &#123;1, 2, 3, 4, 5&#125;; cout &lt;&lt; \"The size of b: \" &lt;&lt; sizeof(b) &lt;&lt; \" bytes\" &lt;&lt; endl; cout &lt;&lt; \"The size of b: \" &lt;&lt; b.size() &lt;&lt; \" elements\" &lt;&lt; endl;&#125; 123The size of a: 20 bytesThe size of b: 24 bytesThe size of b: 5 elements Associative Containerstd::setSets are an associative container where each element is unique. The value of the element cannot be modified once it is added to the set.1234567891011#include &lt;set&gt;set&lt;int&gt; s1;for (int i = 0; i &lt; 10; i++) &#123; s1.begin(); s1.end(); s1.size(); s1.insert(i); s1.erase(i); s1.find(i);&#125; std::mapA std::map sorts its elements by the keys. AlgorithmThe STL provides implementations of some widely used algorithms. &lt;algorithms&gt; header: sorting, searching, copying, modifying elements &lt;numeric&gt; header: numeric operation Sort123456int main()&#123; vector&lt;int&gt; values&#123;10, 1, 22, 12, 2, 7&#125;; //sort takes a range sort(values.begin(), values.end());&#125; Binary Search123456int main()&#123; vector&lt;int&gt; values&#123;10, 1, 22, 12, 2, 7&#125;; //binary_search takes a range and a value bool found = binary_search(values.begin(), values.end(), 12);&#125; Copy12345678int main()&#123; vector&lt;int&gt; values1&#123; 10, 1, 22, 12, 2, 7 &#125;; //destination vector &lt;int&gt; values2; copy(values1.begin(), values1.end(), //input range back_inserter(values2)); //output iterator&#125; Replace1234567int main()&#123; vector&lt;int&gt; values&#123; 10, 1, 22, 12, 2, 7 &#125;; replace(values.begin(), values.end(), //range 1, //old value 111); //new value&#125; Numeric123456789int main()&#123; vector&lt;int&gt; v2&#123; 5, 4, 3, 2, 1 &#125;; vector&lt;int&gt; v2&#123; 1, 2, 3, 4, 5 &#125;; int r1 = accumulate(v1.begin(), v1.end(), 0); //range int r2 = inner_product(v1.begin(), v1.end(), v2.begin(), 0);&#125; Complexity Comparison Smart Pointerstd::unique_ptrA unique pointer takes unique ownership in its pointed object. The unique pointer delete the object they managed either when the unique pointer is destroyed or when the object’s value changes. 12345678910111213#include &lt;memory&gt;std::unique_ptr&lt;Option&gt; sp(new Option());// initates a smart pointer (or through reset: sp.resert(Option()).)std::unique_ptr&lt;Option&gt; sp2(sp);// error: does not allow two reference (sp, sp2) to the same object (new Option());std::unique_ptr&lt;Option&gt; sp2(std::move(sp));// now sp is destroyed and sp2 takes ownership of the Option objectsp2-&gt;getPrice();// smart pointer can be used as regular pointer std::shared_ptrThe shared pointer counts the reference to its pointed object and can store and pass a reference beyond the scope of a function. In OOP, the share pointer is used to store a pointer as a member variable and can be used to reference value outside the scope of the class. 1234567std::share_ptr&lt;Option&gt; sp2;&#123; std::share_ptr&lt;Option&gt; sp(new Option()); sp2=sp;&#125;sp2-&gt;getPrice();// the Option object is not deleted after local scope ends Creating a vector of shared_ptr:12345#include &lt;vector&gt;std::vector&lt;std::shared_ptr&lt;Option&gt;&gt; option_list;for (int i=0; i&lt; 10; i++) &#123; option_list.push_back(std::shared_ptr&lt;Option&gt;(new Option(i)));&#125; std::weak_ptrA weak_ptr works the same as shared pointer, but will not increment the reference count.123456std::weak_ptr&lt;Option&gt; sp2;&#123; std::share_ptr&lt;Option&gt; sp(new Option()); sp2=sp;&#125;sp2-&gt;getPrice(); // error! the Option object does not exist beyond scope. Parallel ProcessingThreadingA thread is a small sequence of programmed instruction and is usually a component of a process. Multi-threading can exist within one process, executing concurrently and share resources such as memory, while processes do not share their resources. The std::thread class in c++ supports multi-threading, and can be initiated to represent a single thread. We need to pass a callable object (function pointer, function, or lambda) to the constructor of the std::thread class. We use the std::thread.join() method to wait for the copmletion of a thread. Here we initiate two threads. Both threads share memory and attempt to modify the balance variable at the same time which lead to concurrency issue.123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;thread&gt;using namespace std;int main() &#123; int balance = 0; // t1 starts thread t1([&amp;balance] &#123;for (int i=0; i&lt;1000000; i++) &#123;balance++;&#125;&#125;); // t2 starts thread t2([&amp;balance] &#123;for (int i=0; i&lt;1000000; i++) &#123;balance--;&#125;&#125;); t1.join(); // the main() waits here until t1 completes t2.join(); // the main() waits here until t2 completes cout &lt;&lt; balance &lt;&lt; endl; cout &lt;&lt; \"END OF CODE\" &lt;&lt; endl;&#125; 12153258END OF CODE We introduce the an mutex, or mutual exclusive, object, which contains a unique id for the resources allocated to the program. A thread can lock the resource by a std::mutex.lock() method, which prevent other thread from sharing the resource until the mutex becomes unlocked. 12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;mutex&gt;using namespace std;int main() &#123; int balance = 0; mutex m; // t1 starts thread t1([&amp;balance, &amp;m] &#123;for (int i=0; i&lt;1000000; i++) &#123; m.lock(); balance++; m.unlock(); &#125;&#125;); // t2 starts thread t2([&amp;balance, &amp;m] &#123;for (int i=0; i&lt;1000000; i++) &#123; m.lock(); balance--; m.unlock(); &#125;&#125;); t1.join(); // the main() waits here until t1 completes t2.join(); // the main() waits here until t2 completes cout &lt;&lt; balance &lt;&lt; endl; cout &lt;&lt; \"END OF CODE\" &lt;&lt; endl;&#125; 120END OF CODE Condition VariableA condition variable is an object that can block the calling thread until notified to resume. It uses a unique_lock (over a mutex) to lock the thread when one of its wait functions is called. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;iostream&gt;#include &lt;mutex&gt;#include &lt;thread&gt;#include &lt;vector&gt;using namespace std;mutex m;condition_variable cv;vector&lt;int&gt; v;bool ready = false;bool processed = false;void make_vector() &#123; unique_lock&lt;std::mutex&gt; lk(m); // own the mutex cv.wait(lk, []&#123;return ready;&#125;); // wait until main() sends data for (int k = 0; k &lt; 10; ++k) &#123; v.push_back(k); &#125; processed = true; lk.unlock(); // manual unlocking is done before notifying cv.notify_one(); // unblocks one of the threads currently waiting for this condition // if no threads are waiting, the function does nothing // if more than one threads are waiting, it is unspecified which will be selected&#125;int main() &#123; thread t(make_vector); ready = false; processed = false; &#123; cout &lt;&lt; \"main() signals ready for processing\\n\"; ready = true; &#125; cv.notify_one(); &#123; unique_lock&lt;std::mutex&gt; lk(m); // own the mutex cv.wait(lk, []&#123;return processed;&#125;); // wait for cv.notify_one cout &lt;&lt; \"back to main(), vector is processed\\n\"; &#125; for (auto i : v) &#123; cout &lt;&lt; i &lt;&lt; \" \"; &#125; t.join();&#125; 123main() signals ready for processingback to main(), vector is processed0 1 2 3 4 5 6 7 8 9 Reference: FINM 322 Lecture Notes, Chanaka Liyanaarachchi, the University of Chicago","tags":"tech"},{"title":"The St. Petersburg Paradox and Utility Function","url":"/2019/02/the-st-petersburg-paradox/","text":"The ParadoxThe St. Petersburg paradox describes a game of chance for a single player in which a fair coin is tossed at each stage. The initial stake starts at 2 dollars and is doubled every time heads appears. The first time tails appears, the game ends and the player wins whatever is in the pot. Let X_n denote the final winning amount after the nth toss. We know that X_0 = 2 and P(X_n = 2X_{n-1})=1/2 and P(X_n = X_{n-1})=1/2. We can see that X_{n} is a sub-martingale as \\mathbb{E}[X_{n}|\\mathcal{F}_{n-1}] \\geq X_{n-1}. Let T denote the minimum number of toss that shows a tail toss T := min\\{t: X_t=X_{t-1}\\}. T is a stopping time. \\mathbb{E}X_{T} = \\dfrac{1}{2} \\times 2 + \\dfrac{1}{4} \\times 4 + \\dfrac{1}{8} \\times 8 + ... \\rightarrow \\inftyThe variance: varX_{T} = \\mathbb{E}X^2 - (\\mathbb{E}X)^2 = \\lim_{n\\rightarrow \\infty} 2^{n+1} - 2 - n^2 \\rightarrow \\inftyConsidering nothing but the expected value of the net change in one’s monetary wealth, one should therefore play the game at any price if offered the opportunity. Yet the paradox exists between what people seem willing to pay to enter the game and the infinite expected value. The MartingaleWith slight modifications from above, the St. Petersburg martingale is the sequence of random variable X_n, where X_0=2 and P(X_n=2X_{n-1}) = 1/2 and P(X_n=0) = 1/2. It is now a martingale because \\mathbb{E}(X_n|\\mathcal{F}_{n-1}) = X_{n-1}. Note that now the expected payoff at inception is equal to initial stake based on Doob’s Idenfity, given stopping time T defined above: \\mathbb{E}X_{T} = X_{0} Expected Utility TheoryThe classic way to resolve the paradox involves the introduction of utility function and the presumption of diminishing marginal utility. Let x denote a person’s total wealth, then the logrithmic utility function states that: U(w) = \\log{w}Where the marginal utility is a decreasing function of x: \\dfrac{dU^{log}(w)}{dw} = \\dfrac{1}{w}Now we calculate the maximum cost c that a person with logrithmic utility function and wealth w would pay to enter the game. The maximum cost will make the expected utility change from playing the game be 0. \\Delta U(w) = 0 = \\sum_{k=1}^{\\infty} \\dfrac{1}{2^k}[\\log(w + 2^k - c) - \\log(w)]SimulationGiven w, c can be calculated numerically. In python, we first check the maximum power of two allowed in our simulation. It turns out to be 63:1234567import sysimport numpy as npx = sys.maxsizeprint('maximum integer: ', x)max_power = np.log(x)/np.log(2)print('maximum power of 2: ', max_power) Returns:12maximum integer: 9223372036854775807maximum power of 2: 63.0 Logrithmic Utility FunctionNow we use fsolve to calculate the maximum c to break even:123456789101112import numpy as npfrom scipy.optimize import fsolvedef func(c): w = 1000 p = 0.5`np.arange(1, 63) u = np.log(w + 2`np.arange(1, 63) - c) - np.log(w) return sum(p*u)maximum_cost = fsolve(func, 0)print(\"maximum cost to enter: \", round(maximum_cost[0], 2)) When w = 1000:1maximum cost to enter: 10.95 When w = 1000000:1maximum cost to enter: 20.87 Square-Root Utility Function123456789101112import numpy as npfrom scipy.optimize import fsolvedef func(c): w = 1000 p = 0.5`np.arange(1, 63) u = np.sqrt(w + 2`np.arange(1, 63) - c) - np.sqrt(w) return sum(p*u)maximum_cost = fsolve(func, 0)print(\"maximum cost to enter: \", round(maximum_cost[0], 2)) When w = 1000:1maximum cost to enter: 12.93 When w = 1000000:1maximum cost to enter: 22.87 Note that under the square-root utility function, the maximum cost to enter is higher, as utility diminished slower. \\dfrac{dU^{sqrt}(w)}{dw} = \\dfrac{1}{2\\sqrt{w}} > \\dfrac{1}{w}, \\;when \\;w\\;is\\;large Reference: Wikipedia: https://en.wikipedia.org/wiki/St._Petersburg_paradox","tags":"math"},{"title":"About","url":"/2019/01/about/","text":"Hi there, my name is Jack and I am an actuary in the United States. I share my thoughts and ramblings occasionally on this corner of the internet. Here is another (more interesting) blog: https://allenfrostline.com","tags":"travel"}]}